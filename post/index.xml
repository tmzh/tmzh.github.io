<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Ephemeral Dance Of Electrons</title><link>https://tmzh.github.io/post/</link><description>Recent content in Posts on Ephemeral Dance Of Electrons</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 16 May 2023 12:00:00 +0000</lastBuildDate><atom:link href="https://tmzh.github.io/post/index.xml" rel="self" type="application/rss+xml"/><item><title>Create beautiful paintings from rough sketches using Stable diffusion</title><link>https://tmzh.github.io/post/2023-05-16-paint-like-bob-ross-using-stable-diffusion/</link><pubDate>Tue, 16 May 2023 12:00:00 +0000</pubDate><guid>https://tmzh.github.io/post/2023-05-16-paint-like-bob-ross-using-stable-diffusion/</guid><description>Introduction When it comes to creating artwork, there are many Generative AI tools, but my favorite is Stable Diffusion. Since it is open source, a variety of tools and usecases have been built around Stable diffusion. With it, you can train your own model, fine-tune existing models, or use countless other models trained and hosted by others.
But one of my favorite use case is to render rough sketches into much prettier artwork.</description></item><item><title>GPT-4, Stable Diffusion, and Beyond: How Generative AI Will Shape Human Society</title><link>https://tmzh.github.io/post/2023-03-15-gpt-4-stable-diffusion-and-beyond/</link><pubDate>Wed, 15 Mar 2023 12:00:00 +0000</pubDate><guid>https://tmzh.github.io/post/2023-03-15-gpt-4-stable-diffusion-and-beyond/</guid><description>In 2020, I wrote about GPT-3 model. Late last year, OpenAI released ChatGPT which was based on GPT-3 but trained using Reinforcement Learning from Human Feedback (RLHF). And now GPT-4 has been released. It has only been out for a few days, but it is already seeing incredible applications such as creating office documents, turning sketches into functional apps, creating personal tutors, and more.
And not just GPT-based models, StableDiffusion and Dall-E are also pushing the boundaries of art, creating stunning visuals from mere textual descriptions.</description></item><item><title>Solvers for the Wordle game - Evaluation of strategies</title><link>https://tmzh.github.io/post/2022-03-07-solvers-for-the-wordle-game-evaluation-of-different-strategies/</link><pubDate>Mon, 07 Mar 2022 12:00:00 +0000</pubDate><guid>https://tmzh.github.io/post/2022-03-07-solvers-for-the-wordle-game-evaluation-of-different-strategies/</guid><description>&lt;p>Wordle is a web-based word game which has become incredibly popular during the pandemic. It became so popular over a while that it was even bought by New York times for a significant sum and is currently &lt;a href="https://www.nytimes.com/games/wordle/index.html">hosted&lt;/a> there. The game is a lot of fun to solve manually, but I am also interested in solving this computationally. This is my attempt at coming up with a solution strategy for the game.&lt;/p></description></item><item><title>Using Hugging Face Transformers on AWS Sagemaker</title><link>https://tmzh.github.io/post/2021-09-16-using-hugging-face-transformers-on-aws-sagemaker/</link><pubDate>Thu, 16 Sep 2021 12:00:00 +0000</pubDate><guid>https://tmzh.github.io/post/2021-09-16-using-hugging-face-transformers-on-aws-sagemaker/</guid><description>&lt;p>In July 2021, AWS and Hugging Face announced collaboration to make Hugging Face a first party framework within SageMaker. Earlier, you had to use PyTorch container and install packages manually to do this. With the new Hugging Face Deep Learning Containers (DLC) availabe in Amazon SageMaker, the process of training and deploying models is greatly simplified.&lt;/p>
&lt;p>In this post, we will go through a high level overview of Hugging Face Transformers library before looking at how to use the newly announced Hugging Face DLCs within Sagemaker.&lt;/p></description></item><item><title>Previewing command line JSON output using firefox</title><link>https://tmzh.github.io/post/2021-03-26-previewing-command-line-json-output-using-firefox/</link><pubDate>Fri, 26 Mar 2021 12:00:00 +0000</pubDate><guid>https://tmzh.github.io/post/2021-03-26-previewing-command-line-json-output-using-firefox/</guid><description>&lt;p>Firefox, like other modern browsers, has an excellent in-built JSON viewer. It also supports &lt;a href="https://en.wikipedia.org/wiki/Data_URI_scheme">Data URIs&lt;/a> which allows you to load HTML resource from text in URL as if they were external resources. We can make use of these two features to have a handy JSON previewer which can be invoked from command line.&lt;/p>
&lt;p>For example, when you enter the below link into your browser, it opens a &amp;ldquo;Hello world&amp;rdquo; text document.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">data:,Hello%2C%20World!
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>This content is not limited to plain text. It can even be an HTML document:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">data:text/html,%3Ch1%3EHello%2C%20World!%3C%2Fh1%3E
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>Writing BDD tests for Terraform Code Using Terratest</title><link>https://tmzh.github.io/post/2021-02-02-testing-terraform-code-using-terratest/</link><pubDate>Tue, 02 Feb 2021 12:00:00 +0000</pubDate><guid>https://tmzh.github.io/post/2021-02-02-testing-terraform-code-using-terratest/</guid><description>&lt;p>Terratest is a popular library for testing Terraform code. Testing Infrastructure As Code (IAC) is not as widespread as it should be. The reasons are multi-fold, ranging from developer&amp;rsquo;s attitude towards testing to the difficulty of writing unit tests because of inherent side effects of IAC. Nevertheless, testing is no less important, in particular under these scenarios:&lt;/p>
&lt;ol>
&lt;li>When your module gets complicated, with medium to complex behaviour logic&lt;/li>
&lt;li>When your module makes underlying assumptions of external dependencies (such as AWS SCPs at Organization level permitting certain actions)&lt;/li>
&lt;/ol>
&lt;p>In this post, we will take a look at using Terratest to test Terraform code. A typical Terratest testing pattern involves:&lt;/p>
&lt;ol>
&lt;li>Deploying real infrastructure in real environment&lt;/li>
&lt;li>Asserting that the deployed resources behaves as expected&lt;/li>
&lt;li>Undeploy everything at the end of the test.&lt;/li>
&lt;/ol>
&lt;p>Behavior Driven Test (BDD) uses examples to describe the behavior of a system. It serves the dual purpose of testing the code and documenting it at the same time. Terratest is not a BDD testing framework, however it is possible to write BDD tests that executes Terratest code. In a later section of this post, we will see how this can be achieved using Godog which is a Go BDD testing library.&lt;/p></description></item><item><title>GPT-3 and prospects of Artificial General Intelligence</title><link>https://tmzh.github.io/post/2020-09-20-gpt-3-and-prospects-of-artificial-general-intelligence/</link><pubDate>Sun, 20 Sep 2020 12:00:00 +0000</pubDate><guid>https://tmzh.github.io/post/2020-09-20-gpt-3-and-prospects-of-artificial-general-intelligence/</guid><description>&lt;p>Last year OpenAI released the Generative Pre-trained Transformer 2 (GPT-2) model. GPT-2 was a language model with 1.5 billion parameters, trained on 8 million web pages. It generated quite a buzz as it could generate coherent text, comprehend paragraphs, answer questions, and summarize text and do all sorts of smart stuff&amp;hellip; all without any task-specific learning. OpenAI even deemed the model too dangerous to release but eventually ended up releasing them.&lt;/p>
&lt;p>In May 2020, OpenAI released their follow-up GPT-3 model which took the game several notches higher. They trained it with 175 billion parameters, using close to half-a-trillion tokens. The model and its weights alone would take up 300GB VRAM. This is a drastic increase in scale and complexity, anyway you look at it. So what can a huge model like this achieve and why has it reinvigorated the talks ?&lt;/p>
&lt;p>&lt;img src="https://tmzh.github.io/images/2020-09-26-gpt-3-training-size.png" alt="GPT-3 Training Size">&lt;/p></description></item><item><title>Using OpenCV object detection to keep kids away from TV</title><link>https://tmzh.github.io/post/2019-03-02-using-opencv-object-detection-to-keep-kids-away-from-tv/</link><pubDate>Sat, 02 Mar 2019 12:00:00 +0000</pubDate><guid>https://tmzh.github.io/post/2019-03-02-using-opencv-object-detection-to-keep-kids-away-from-tv/</guid><description>&lt;blockquote>
&lt;p>Give me a dozen healthy infants, well formed, and my own specified world to bring them up in and I’ll guarantee to take any one at random and train him to become any type of specialist I might select—doctor, lawyer, artist, merchant-chief and yes, even beggar-man thief, regardless of his talents, penchants, tendencies, abilities, vocations, and race of his ancestors.&lt;/p>
&lt;/blockquote>
&lt;p>This was John Watson, one of the founders of &lt;a href="https://www.wikiwand.com/en/Behaviorism">Behaviorism&lt;/a>, writing around 1925. He believed that human behavior is completely malleable and that it can be shaped into anything given the right environment. While I don&amp;rsquo;t harbor any grand objectives or sinister experiments like Watson did, I do hope to be able to teach my kids good habits using controlled environments. For instance, my two year old kids started developing the habit of getting too close to the TV. I didn&amp;rsquo;t want to use force or impose restrictions on them, so I thought I could use technology to discourage them from getting too close to TV.&lt;/p></description></item><item><title>Visualizing air routes for major airports using Spark and Matplotlib</title><link>https://tmzh.github.io/post/2019-01-05-using-spark-and-matplotlib-to-visualize-air-routes-for-major-airports-around-the-world/</link><pubDate>Sat, 05 Jan 2019 12:00:00 +0000</pubDate><guid>https://tmzh.github.io/post/2019-01-05-using-spark-and-matplotlib-to-visualize-air-routes-for-major-airports-around-the-world/</guid><description>&lt;p>In 2nd Century AD, Hellenic Cartographer Ptolemy was beset with an arbitrary choice of whether his maps should have north on the top or any other direction. Based in Alexandria, he reasoned that all population centers and places of importance lie to the north and would be convenient for study if they were in the upper right corner of the map. This arbitrary choice had long, unintended repercussions for mankind such as Australia being considered &amp;ldquo;Down under&amp;rdquo; or even our solar system to be perceived as rotating in counter-clockwise direction. Who would have thought that the stroke of a cartographer carried celestial importance!&lt;/p></description></item><item><title>Book Review - Security Automation with Ansible 2</title><link>https://tmzh.github.io/post/2018-05-20-book-review-security-automation-with-ansible-2/</link><pubDate>Sun, 20 May 2018 12:00:00 +0000</pubDate><guid>https://tmzh.github.io/post/2018-05-20-book-review-security-automation-with-ansible-2/</guid><description>&lt;p>Security is a huge, complex, rapidly changing field. Advancements in infrastructue hosting, development methodologies has had the most impact on this domain. Thanks to automation, instances are spawned and deleted in a matter of second. Continuous development/Continuous Integration means that an average lifetime of a block of code is ever decreasing. Code review and vulnerability assessments based on static code and IP are hardly affordable at current rate of change. At the same time, the rate of proliferation of technology has seen comparable increase in risk vectors, vulnerabilities and attack methodologies. To keep up with this pace, automation in security operations has become more important than ever.&lt;/p></description></item><item><title>Predicting the playing role of a cricketer using Machine Learning (Part 2)</title><link>https://tmzh.github.io/post/2018-04-28-predicting-the-playing-role-of-a-cricketer-using-machine-learning-part-2/</link><pubDate>Sat, 28 Apr 2018 12:00:00 +0000</pubDate><guid>https://tmzh.github.io/post/2018-04-28-predicting-the-playing-role-of-a-cricketer-using-machine-learning-part-2/</guid><description>&lt;p>In the previous &lt;a href="https://tmzh.github.io/machine%20learning/2018/04/23/2018-04-23-predicting-the-playing-role-of-a-cricketer-using-machine-learning-part-1/">post&lt;/a> we saw how to scrape raw data from a content rich webpage. In this post, we will explore how to process that raw data and use Machine Learning tools to predict the playing role of a cricket player just based on his career statistics.&lt;/p></description></item><item><title>Predicting the playing role of a cricketer using Machine Learning (Part 1)</title><link>https://tmzh.github.io/post/2018-04-23-predicting-the-playing-role-of-a-cricketer-using-machine-learning-part-1/</link><pubDate>Mon, 23 Apr 2018 12:00:00 +0000</pubDate><guid>https://tmzh.github.io/post/2018-04-23-predicting-the-playing-role-of-a-cricketer-using-machine-learning-part-1/</guid><description>&lt;p>In this project, we will apply Machine Learning techniques to predict whether a particular cricket player is a batsman or bowler based on his career stats. First we will use Deep Neural Networks (DNN) model and later compare the results with a simpler classifier algorithm such as Random Forest Classifier.&lt;/p></description></item><item><title>Introduction to OpenStack Networking for Network Engineers</title><link>https://tmzh.github.io/post/2018-03-22-introduction-to-openstack-networking-for-network-engineers/</link><pubDate>Thu, 22 Mar 2018 12:00:00 +0000</pubDate><guid>https://tmzh.github.io/post/2018-03-22-introduction-to-openstack-networking-for-network-engineers/</guid><description>&lt;p>This post is a gentle introduction to networking with Openstack using the Neutron module. Being an introduction, we will not focus on setting up OpenStack from scratch. Instead we will familiarize ourselves with core concepts of Neutron and common administrative tasks. We will use the latest release of Openstack, &lt;strong>Queens&lt;/strong>.&lt;/p></description></item><item><title>Using Constraint Programming Tools to solve an ancient Chinese math puzzle</title><link>https://tmzh.github.io/post/2017-12-09-using-constraint-programming-to-solve-an-ancient-chinese-math-puzzle/</link><pubDate>Sat, 09 Dec 2017 12:00:00 +0000</pubDate><guid>https://tmzh.github.io/post/2017-12-09-using-constraint-programming-to-solve-an-ancient-chinese-math-puzzle/</guid><description>&lt;p>&lt;a href="https://www.wikiwand.com/en/Constraint_programming">Constraint programming (CP)&lt;/a> is a subset of Operations Research (OR) where our task is to identify all feasible solutions to a given problem that satisfies a set of constraints. This is different from an optimization problem, where an objective function is defined and we arrive at solutions that either maximizes or minimizes an objective function.&lt;/p>
&lt;p>CP is mostly well suited for solving logic puzzles, since most logic puzzles are based on constraints and enumerating feasible solutions. But apart from recreational maths, CP also has a lot of practical applications in Scheduling, Resource allocation, Manufacturing etc.,&lt;/p></description></item><item><title>Using Monte-Carlo Simulation to model ping test results</title><link>https://tmzh.github.io/post/2017-11-24-using-monte-carlo-simulation-to-model-ping-test-results/</link><pubDate>Fri, 24 Nov 2017 08:01:28 +0000</pubDate><guid>https://tmzh.github.io/post/2017-11-24-using-monte-carlo-simulation-to-model-ping-test-results/</guid><description>&lt;p>Recently we had a cabling issue in our core infrastructure which caused around 3 to 12% packet loss across few IP streams. One of my colleagues made an interesting observation that when he tried to ping with large packet size (5000 bytes) the packet loss rose up as high as 40%. In his opinion, that meant some applications were experiencing up to 40% packet loss. I seldom do large packet ping tests unless I am troubleshooting MTU issues, so to me this observation was interesting.&lt;/p>
&lt;p>At the outset, it may look like an aggravated problem. Yet you know that your network path MTU doesn&amp;rsquo;t support jumbo frames end-to-end. If so, why is there a difference in packet loss rate when you ping with large datagrams? The answer is not too obvious. The important thing to note is that a ping test result is not a measure of ethernet frame loss but ICMP datagram loss. In most cases (when the ICMP datagram is smaller than ethernet MTU) both are the same. But why do large ICMP datagrams have higher loss percentage than individual ethernet frames? Enter Math.&lt;/p></description></item><item><title>Emulating Angry IP Scanner with nmap scripting engine - A lua scripting primer</title><link>https://tmzh.github.io/post/2017-10-29-emulating-angryip-scanner-with-nmap-scripting-engine-a-lua-scripting-primer/</link><pubDate>Sun, 29 Oct 2017 12:08:28 +0000</pubDate><guid>https://tmzh.github.io/post/2017-10-29-emulating-angryip-scanner-with-nmap-scripting-engine-a-lua-scripting-primer/</guid><description>&lt;p>Often we have to discover the devices on a network. I use a very simple nmap command for performing a pingsweep.&lt;/p>
&lt;p>&lt;code>sudo nmap -sn &amp;lt;subnet or ip range&amp;gt;&lt;/code>&lt;/p>
&lt;p>On my Windows PC, I wrap it around in a batch script and place it in the search PATH. On Linux, it can be dropped in as an alias in bashrc.&lt;/p></description></item><item><title>Using a bluetooth serial console with linux</title><link>https://tmzh.github.io/post/2017-08-24-using-a-bluetooth-serial-console-with-linux/</link><pubDate>Thu, 24 Aug 2017 15:41:28 +0000</pubDate><guid>https://tmzh.github.io/post/2017-08-24-using-a-bluetooth-serial-console-with-linux/</guid><description>&lt;p>Recently I bought a &lt;a href="https://www.aliexpress.com/store/product/FREE-SHIPPING-Bt578-rs232-wireless-male-female-general-serial-port-bluetooth-adapter-bluetooth-module/719457_1271204185.html">bluetooth RS232 serial convertor&lt;/a>. I wasn&amp;rsquo;t sure whether it would work with my Linux laptop. But it turned out to be quite simple to setup.&lt;/p>
&lt;h2 id="pre-requisites">Pre-requisites&lt;/h2>
&lt;p>The following packages are required:&lt;/p>
&lt;ul>
&lt;li>bluez&lt;/li>
&lt;li>bluez-utils&lt;/li>
&lt;li>byobu (optional)&lt;/li>
&lt;/ul>
&lt;p>Bluez provides the bluetooth protocol stack (most likely shipped with the OS), bluez-utils provides the bluetoothctl utility and byobu is a wrapper around screen terminal emulator. You can also use &amp;lsquo;screen&amp;rsquo; directly. Install these using your distributions recommended procedure.&lt;/p></description></item><item><title>Dynamic registration of DNS for Linux devices in an Active Directory environment with Windows DNS server</title><link>https://tmzh.github.io/post/2017-07-19-dynamic-registration-of-dns-for-linux-devices-in-an-active-directory-environment-with-windows-dns-server/</link><pubDate>Wed, 19 Jul 2017 15:43:13 +0000</pubDate><guid>https://tmzh.github.io/post/2017-07-19-dynamic-registration-of-dns-for-linux-devices-in-an-active-directory-environment-with-windows-dns-server/</guid><description>&lt;p>While Linux has proliferated extensively in the server arena in the recent past, Windows still dominates client networks. Things that we take for granted in a client environment such as DDNS (Dynamic DNS) are not as matured as they are in Windows environment. One may wonder whether the recent surge in Linux based clients such as IoT devices has changed this equation.&lt;/p>
&lt;p>So, what does it take to make a Linux client register dynamically in a Windows environment?&lt;/p></description></item></channel></rss>