<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Using Monte-Carlo Simulation to model ping test results - Ephemeral Dance Of Electrons</title>
<meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=Cache-Control content="no-transform"><meta http-equiv=Cache-Control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="tmzh"><meta name=description content="Recently we had a cabling issue in our core infrastructure which caused around 3 to 12% packet loss across few IP streams. One of my colleagues made an interesting observation that when he tried to ping with large packet size (5000 bytes) the packet loss rose up as high as 40%. In his opinion, that meant some applications were experiencing up to 40% packet loss. I seldom do large packet ping tests unless I am troubleshooting MTU issues, so to me this observation was interesting.
At the outset, it may look like an aggravated problem. Yet you know that your network path MTU doesn&amp;rsquo;t support jumbo frames end-to-end. If so, why is there a difference in packet loss rate when you ping with large datagrams? The answer is not too obvious. The important thing to note is that a ping test result is not a measure of ethernet frame loss but ICMP datagram loss. In most cases (when the ICMP datagram is smaller than ethernet MTU) both are the same. But why do large ICMP datagrams have higher loss percentage than individual ethernet frames? Enter Math.
"><meta name=keywords content="blog,code,recreational"><meta name=generator content="Hugo 0.120.4 with theme even"><link rel=canonical href=https://tmzh.github.io/post/2017-11-24-using-monte-carlo-simulation-to-model-ping-test-results/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link href=/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css rel=stylesheet><meta property="og:title" content="Using Monte-Carlo Simulation to model ping test results"><meta property="og:description" content="Recently we had a cabling issue in our core infrastructure which caused around 3 to 12% packet loss across few IP streams. One of my colleagues made an interesting observation that when he tried to ping with large packet size (5000 bytes) the packet loss rose up as high as 40%. In his opinion, that meant some applications were experiencing up to 40% packet loss. I seldom do large packet ping tests unless I am troubleshooting MTU issues, so to me this observation was interesting.
At the outset, it may look like an aggravated problem. Yet you know that your network path MTU doesn&rsquo;t support jumbo frames end-to-end. If so, why is there a difference in packet loss rate when you ping with large datagrams? The answer is not too obvious. The important thing to note is that a ping test result is not a measure of ethernet frame loss but ICMP datagram loss. In most cases (when the ICMP datagram is smaller than ethernet MTU) both are the same. But why do large ICMP datagrams have higher loss percentage than individual ethernet frames? Enter Math."><meta property="og:type" content="article"><meta property="og:url" content="https://tmzh.github.io/post/2017-11-24-using-monte-carlo-simulation-to-model-ping-test-results/"><meta property="article:section" content="post"><meta property="article:published_time" content="2017-11-24T08:01:28+00:00"><meta property="article:modified_time" content="2017-11-24T08:01:28+00:00"><meta itemprop=name content="Using Monte-Carlo Simulation to model ping test results"><meta itemprop=description content="Recently we had a cabling issue in our core infrastructure which caused around 3 to 12% packet loss across few IP streams. One of my colleagues made an interesting observation that when he tried to ping with large packet size (5000 bytes) the packet loss rose up as high as 40%. In his opinion, that meant some applications were experiencing up to 40% packet loss. I seldom do large packet ping tests unless I am troubleshooting MTU issues, so to me this observation was interesting.
At the outset, it may look like an aggravated problem. Yet you know that your network path MTU doesn&rsquo;t support jumbo frames end-to-end. If so, why is there a difference in packet loss rate when you ping with large datagrams? The answer is not too obvious. The important thing to note is that a ping test result is not a measure of ethernet frame loss but ICMP datagram loss. In most cases (when the ICMP datagram is smaller than ethernet MTU) both are the same. But why do large ICMP datagrams have higher loss percentage than individual ethernet frames? Enter Math."><meta itemprop=datePublished content="2017-11-24T08:01:28+00:00"><meta itemprop=dateModified content="2017-11-24T08:01:28+00:00"><meta itemprop=wordCount content="953"><meta itemprop=keywords content="network,python,numpy,probability,scripting,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Using Monte-Carlo Simulation to model ping test results"><meta name=twitter:description content="Recently we had a cabling issue in our core infrastructure which caused around 3 to 12% packet loss across few IP streams. One of my colleagues made an interesting observation that when he tried to ping with large packet size (5000 bytes) the packet loss rose up as high as 40%. In his opinion, that meant some applications were experiencing up to 40% packet loss. I seldom do large packet ping tests unless I am troubleshooting MTU issues, so to me this observation was interesting.
At the outset, it may look like an aggravated problem. Yet you know that your network path MTU doesn&rsquo;t support jumbo frames end-to-end. If so, why is there a difference in packet loss rate when you ping with large datagrams? The answer is not too obvious. The important thing to note is that a ping test result is not a measure of ethernet frame loss but ICMP datagram loss. In most cases (when the ICMP datagram is smaller than ethernet MTU) both are the same. But why do large ICMP datagrams have higher loss percentage than individual ethernet frames? Enter Math."><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>Ephemeral Dance Of Electrons</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/><li class=mobile-menu-item>Home</li></a><a href=/post/><li class=mobile-menu-item>Archives</li></a><a href=/tags/><li class=mobile-menu-item>Tags</li></a><a href=/categories/><li class=mobile-menu-item>Categories</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>Ephemeral Dance Of Electrons</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/>Home</a></li><li class=menu-item><a class=menu-item-link href=/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=/categories/>Categories</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>Using Monte-Carlo Simulation to model ping test results</h1><div class=post-meta><span class=post-time>2017-11-24</span><div class=post-category><a href=/categories/modelling/>Modelling</a></div></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Contents</h2><div class="post-toc-content always-active"><nav id=TableOfContents><ul><li><ul><li><a href=#normal-ping-vs-large-ping>Normal ping vs Large ping</a></li><li><a href=#monte-carlo-simulation>Monte Carlo Simulation</a></li><li><a href=#using-probability-theory>Using Probability Theory</a></li><li><a href=#conclusion>Conclusion</a></li></ul></li></ul></nav></div></div><div class=post-content><p>Recently we had a cabling issue in our core infrastructure which caused around 3 to 12% packet loss across few IP streams. One of my colleagues made an interesting observation that when he tried to ping with large packet size (5000 bytes) the packet loss rose up as high as 40%. In his opinion, that meant some applications were experiencing up to 40% packet loss. I seldom do large packet ping tests unless I am troubleshooting MTU issues, so to me this observation was interesting.</p><p>At the outset, it may look like an aggravated problem. Yet you know that your network path MTU doesn&rsquo;t support jumbo frames end-to-end. If so, why is there a difference in packet loss rate when you ping with large datagrams? The answer is not too obvious. The important thing to note is that a ping test result is not a measure of ethernet frame loss but ICMP datagram loss. In most cases (when the ICMP datagram is smaller than ethernet MTU) both are the same. But why do large ICMP datagrams have higher loss percentage than individual ethernet frames? Enter Math.</p><h2 id=normal-ping-vs-large-ping>Normal ping vs Large ping</h2><p>In windows a normal ping packet size is 32 bytes and in most environments, the default MTU is 1500 bytes. So a single frame is sufficient to transmit a ping packet. Things get weirder when we ping with large packets. In windows, to simulate larger packets you can use the <code>-l</code> option to specify packet size. Note that this size doesn&rsquo;t include the packet header (20 bytes for IP header + 8 bytes for ICMP header). Which means that we can only fit 1472 bytes of ICMP payload inside a 1500 MTU ethernet frame. Any length above this must be fragmented.</p><p>We can test this easily. Below is the result when pinging with 1472 as the ping size (<code>ping 8.8.8.8 -n 2 -l 1472</code>)</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Capturing on &#39;Ethernet 2&#39;
</span></span><span class=line><span class=cl>    1   0.000000     10.1.1.1 → 8.8.8.8      ICMP 1514 Echo (ping) request  id=0x0001, seq=8/2048, ttl=128
</span></span><span class=line><span class=cl>    2   0.015698      8.8.8.8 → 10.1.1.1     ICMP 106 Echo (ping) reply    id=0x0001, seq=8/2048, ttl=45
</span></span><span class=line><span class=cl>2 packets captured
</span></span></code></pre></td></tr></table></div></div><p>When we ping with just one more byte, you can see that 2 packets are sent in place of 1 ((<code>ping 8.8.8.8 -n 2 -l 1473</code>)</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Capturing on &#39;Ethernet 2&#39;
</span></span><span class=line><span class=cl>    1   0.000000     10.1.1.1 → 8.8.8.8      IPv4 1514 Fragmented IP protocol (proto=ICMP 1, off=0, ID=4fab)
</span></span><span class=line><span class=cl>    2   0.000016     10.1.1.1 → 8.8.8.8      ICMP 35 Echo (ping) request  id=0x0001, seq=10/2560, ttl=128
</span></span><span class=line><span class=cl>2 packets captured
</span></span></code></pre></td></tr></table></div></div><p>So when we ping with 5000 bytes, 4 packets are sent. And ICMP protocol considers a datagram to be lost even when one of them fails. So the probability of the ICMP datagram loss is higher than the probability of single frame loss.</p><p>Is this what is happening in the ping test result? We can calculate the probability of datagram loss using probability theory but let us defer to it later on and do a numerical simulation first using Monte Carlo simulation.</p><h2 id=monte-carlo-simulation>Monte Carlo Simulation</h2><p><a href=https://www.wikiwand.com/en/Monte_Carlo_method>Monte carlo simulation</a> is a rather fancy title for a simple simulation using random event generator, but it is quite handy and widely used. Usually Monte Carlo simulation is useful for simulating events that are truly random in nature. In a chaotic backbone network, that handles traffic stream of different kinds, we can assume the frame loss to be random.</p><p>Let us write a short program to simulate random packet loss.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>random</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>sampleCount</span> <span class=o>=</span> <span class=mi>100000</span>                               <span class=c1># total events in our simulation</span>
</span></span><span class=line><span class=cl><span class=n>p</span> <span class=o>=</span> <span class=mf>0.03</span>                                           <span class=c1># ethernet frame loss probability</span>
</span></span><span class=line><span class=cl><span class=n>grpSize</span> <span class=o>=</span> <span class=mi>4</span>                                        <span class=c1># packet count per datagram, 5000 bytes = 4 packets</span>
</span></span><span class=line><span class=cl><span class=n>grpEventCount</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>sampleCount</span><span class=o>/</span><span class=n>grpSize</span><span class=p>)</span>           <span class=c1># datagram count</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># generate random packets with p% packet loss</span>
</span></span><span class=line><span class=cl><span class=n>events</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>([</span><span class=mi>0</span><span class=p>,</span><span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                          <span class=n>size</span><span class=o>=</span><span class=n>sampleCount</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                          <span class=n>p</span><span class=o>=</span><span class=p>[</span><span class=n>p</span><span class=p>,</span><span class=mi>1</span><span class=o>-</span><span class=n>p</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># group discrete packets into a datagram</span>
</span></span><span class=line><span class=cl><span class=n>grpEvents</span> <span class=o>=</span> <span class=n>events</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>grpEventCount</span><span class=p>,</span><span class=n>grpSize</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># function to determine datagram loss</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>checkFailure</span><span class=p>(</span><span class=n>grpEvent</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>count_nonzero</span><span class=p>(</span><span class=n>grpEvent</span><span class=p>)</span> <span class=o>&lt;</span> <span class=n>grpSize</span><span class=p>)</span>    <span class=c1># Return 1 if the success count is less than 3</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># count the result</span>
</span></span><span class=line><span class=cl><span class=n>failCount</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>grpEvent</span> <span class=ow>in</span> <span class=n>grpEvents</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>failCount</span> <span class=o>+=</span> <span class=n>checkFailure</span><span class=p>(</span><span class=n>grpEvent</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;The probability of a group failure is </span><span class=si>{:.2f}</span><span class=s2>%&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>failCount</span><span class=o>/</span><span class=nb>len</span><span class=p>(</span><span class=n>grpEvents</span><span class=p>)</span><span class=o>*</span><span class=mi>100</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><pre><code>The probability of a group failure is 11.78%
</code></pre><p>There you see! Even a 3% ethernet frame loss translates to 12% packet loss for jumbo ping test. This is same as what we observed. Now this is just a simulation with random input. Does the math agree?</p><h2 id=using-probability-theory>Using Probability Theory</h2><p>If <code>p</code> is the probability of a single frame loss, <code>(1-p)</code> is the probability of a successful transfer. And a datagram is successful only if all of its frames are successful. So a 4 frame long ICMP datagram transmission is successful only if 4 consecutive ethernet frame transmissions are successful. The probability is <code>(1-p)**n</code> where <code>n</code> is the number of frames. To calculate the failure rate, just take its inverse.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>n</span> <span class=o>=</span> <span class=mi>4</span>
</span></span><span class=line><span class=cl><span class=mi>1</span><span class=o>-</span> <span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>p</span><span class=p>)</span><span class=o>**</span><span class=n>n</span>
</span></span></code></pre></td></tr></table></div></div><pre><code>0.11470719000000007
</code></pre><p>As expected the simulation is slightly off from the calculated probability. But it will get closer to the real figure when we increase the simulation count.</p><h2 id=conclusion>Conclusion</h2><p>The exactness of our calculation hinges on the assumption of random nature of packet loss. While it happened to be close to true in my case, it need not be the case all the time. The link may have a bursty load and since our ping streams are evenly spaced over time, their chances of failure may not be truly random.</p><p>Nevertheless, we should be wary of the difference between a datagram loss and ethernet loss while interpreting results. Consider the MTU of the network path while testing with different packet sizes.</p></div><div class=post-copyright><p class=copyright-item><span class=item-title>Author</span>
<span class=item-content>tmzh</span></p><p class=copyright-item><span class=item-title>LastMod</span>
<span class=item-content>2017-11-24</span></p></div><footer class=post-footer><div class=post-tags><a href=/tags/network/>network</a>
<a href=/tags/python/>python</a>
<a href=/tags/numpy/>numpy</a>
<a href=/tags/probability/>probability</a>
<a href=/tags/scripting/>scripting</a></div><nav class=post-nav><a class=prev href=/post/2017-12-09-using-constraint-programming-to-solve-an-ancient-chinese-math-puzzle/><i class="iconfont icon-left"></i>
<span class="prev-text nav-default">Using Constraint Programming Tools to solve an ancient Chinese math puzzle</span>
<span class="prev-text nav-mobile">Prev</span>
</a><a class=next href=/post/2017-10-29-emulating-angryip-scanner-with-nmap-scripting-engine-a-lua-scripting-primer/><span class="next-text nav-default">Emulating Angry IP Scanner with nmap scripting engine - A lua scripting primer</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=social-links><a href=https://github.com/tmzh/tmzh.github.io class="iconfont icon-github" title=github></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a>
</span><span class=division>|</span>
<span class=theme-info>Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a>
</span><span class=copyright-year>&copy;
2017 -
2023<span class=heart><i class="iconfont icon-heart"></i></span><span>tmzh</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script><script type=text/javascript src=/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js></script></body></html>