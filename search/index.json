[{"content":"Introduction Large Language Models (LLMs) are good at generating coherent text, but they have few inherent limitations:\nHallucinations: They learn and generate information in terms of likelihood and may produce information that is not grounded in facts Knowledge Cutoff: LLMs are trained on a fixed dataset and do not have access to real-time information or the ability to perform complex tasks like web browsing or executing code. Abstraction and Reasoning: LLMs may struggle with abstract reasoning and complex tasks that require logical steps or mathematical operations. Their output is not precise enough for tasks with fixed rule-sets w/o interfacing with external tools There are two ways to address these limitations:\nRetrieval Augmented Generation (RAG) Function Calling This post focuses on the latter.\nWhat is Function Calling? Function Calling enables LLMs to interact with external tools or APIs, thereby supplementing their knowledge and capabilities. The process is illustrated in the diagram below:\nSource: AWS re:Invent 2024 This approach helps overcome the limitations of knowledge cutoff and abstract reasoning by allowing LLMs to leverage external knowledge sources or tools. Although the function specifications can be passed as part of the prompt, it\u0026rsquo;s more effective to use an internalized template known by the model.\nFunction Calling As A Natural Language Interface Function calling is not limited to simple tasks like calculator operations or weather API queries. It can be used to create an alternative, intuitive natural language interfaces for existing applications. This eliminates the need for complex UIs, as users can interact with the application using plain English.\nExample: TMDB Movie Explorer I have implemented a simple Flask application both with traditional UI and a Natural Language Interface that uses the function calling mechanism. The traditional UI of the application allows users to query movies by cast, genre, or title, whereas the Natural Language Interface allows users to ask in natural language\nChain of Thought Reasoning To handle user queries, we employ a chain of thought (CoT) reasoning approach. This involves:\nAnalyzing the User\u0026rsquo;s Request: Determine the intent behind the query. Generating a Reasoning Chain: Outline the logical steps required to gather the necessary information. Identifying Relevant Functions: Recognize which functions and the order in which they need to be called to fulfill the request 1 2 3 4 5 6 7 8 9 def generate_reasoning_chain(user_prompt: str) -\u0026gt; Any: messages = [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a movie search assistant bot who uses TMDB to help users find movies. Think step by step and identify the sequence of reasoning steps that will help to answer the user\u0026#39;s query.\u0026#34; ], {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_prompt}, ] return messages For example, for the query \u0026ldquo;List comedy movies with Tom Cruise in it,\u0026rdquo; the generated reasoning chain might be:\nSearch for the person ID of Tom Cruise using the search_person function. Search for the genre ID of comedy using the search_genre function. Call the discover_movie function with the person ID and genre ID to find comedy movies that Tom Cruise has been in. Tool Definitions Tools are described using JSON Schema and passed to the model in the prompt. Here\u0026rsquo;s an example of tool definitions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 tools = [ { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_movie_details\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get the top level details of a movie by ID\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;movie_id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The ID of the movie to get details for. Use discover_movie to find the ID of a movie.\u0026#34;, }, }, \u0026#34;required\u0026#34;: [\u0026#34;movie_id\u0026#34;], }, }, }, { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;search_person\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Search for people in the entertainment industry.\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The search query for the person\u0026#34; }, }, \u0026#34;required\u0026#34;: [\u0026#34;query\u0026#34;] } } } ] We pass this message structure to the LLM to generate a reasoning chain. Here we are using llama3-groq-70b-8192-tool-use-preview model from groq which is 70B parameter model that is fine-tuned for tool use.\n1 2 3 4 5 6 7 8 9 # Passing the tools to the function response = client.chat.completions.create( model=MODEL, messages=messages, tools=tools, tool_choice=tool_choice, temperature=0, max_tokens=4096, ) Here each tool is a python function like below which calls TMDB API to retrieve the data.\n1 2 3 4 5 6 7 8 9 def discover_movie(include_adult=False, include_video=False, language=\u0026#34;en-US\u0026#34;, page=1, sort_by=\u0026#34;popularity.desc\u0026#34;, **kwargs): endpoint = f\u0026#34;{BASE_URL}/discover/movie\u0026#34; params = {} for key, value in kwargs.items(): if value is not None: params[key] = value response = query_tmdb(endpoint, params=params) return response Here is an example of the reasoning chain generated for the query \u0026ldquo;List comedy movies with Tom Cruise in it\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a movie search assistant bot who uses TMDB to help users find movies. Think step by step and identify the sequence of function calls that will help to answer.\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;List comedy movies with tom cruise in it\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;To find comedy movies with Tom Cruise, I will first need to find the person ID for Tom Cruise using the search_person function. Once I have the person ID, I can use the discover_movie function to find comedy movies that he has been in.\u0026#34; } ] Handling Function Calls The model returns a response containing tool calls. We extract the function name and arguments from the response and execute the function. The function\u0026rsquo;s response is then added to the conversation history.\n1 2 3 4 5 6 7 8 9 10 11 12 13 if response.choices[0].finish_reason == \u0026#34;tool_calls\u0026#34;: tool_calls = response.choices[0].message.tool_calls new_messages = messages.copy() for tool_call in tool_calls: tool_result = execute_tool(tool_call) new_messages.append( { \u0026#34;tool_call_id\u0026#34;: tool_call.id, \u0026#34;role\u0026#34;: \u0026#34;tool\u0026#34;, \u0026#34;name\u0026#34;: tool_call.function.name, \u0026#34;content\u0026#34;: str(tool_result), } ) Now the conversation history would be like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 [{\u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;You are a movie search assistant bot who uses TMDB to help users find movies. Think step by step and identify the sequence of function calls that will help to answer.\\n Do not call multiple functions when they need to be executed in sequence. Only call multiple functions when they can be executed in parallel. Stop with a discover_movie function call that returns a list of movie ids\u0026#39;}, {\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;List some movies of Tom hanks\u0026#39;}, {\u0026#39;role\u0026#39;: \u0026#39;assistant\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;To find movies of Tom Hanks, I will first need to find the person ID of Tom Hanks using the search_person function. Once I have the person ID, I can use the discover_movie function to find movies that he has been in.\u0026#39;}, {\u0026#39;tool_call_id\u0026#39;: \u0026#39;call_p11f\u0026#39;, \u0026#39;role\u0026#39;: \u0026#39;tool\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;search_person\u0026#39;, \u0026#39;content\u0026#39;: ..., {\u0026#39;tool_call_id\u0026#39;: \u0026#39;call_0b5m\u0026#39;, \u0026#39;role\u0026#39;: \u0026#39;tool\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;discover_movie\u0026#39;, \u0026#39;content\u0026#39;: ...}] Recursive Function Calls We recursively call functions recommended by the LLM until we reach the final function call. The final discover_movie call consolidates all the gathered parameters and data into a single request that retrieves the ultimate list of movies relevant to the user\u0026rsquo;s inquiry\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def generate_domain_knowledge(messages, count=0): response = get_response(client, MODEL, messages, tool_choice=\u0026#34;required\u0026#34;) if response.choices[0].finish_reason == \u0026#34;tool_calls\u0026#34;: tool_calls = response.choices[0].message.tool_calls new_messages = messages.copy() for tool_call in tool_calls: tool_result = execute_tool(tool_call) if tool_call.function.name == \u0026#34;discover_movies\u0026#34;: return tool_result[\u0026#39;results\u0026#39;] else: new_messages.append( { \u0026#34;tool_call_id\u0026#34;: tool_call.id, \u0026#34;role\u0026#34;: \u0026#34;tool\u0026#34;, \u0026#34;name\u0026#34;: tool_call.function.name, \u0026#34;content\u0026#34;: str(tool_result), } ) if count \u0026lt; 2: generate_domain_knowledge(new_messages, count + 1) # stop at 2 recursive calls Lessons learnt and Caveats Keep in mind that the limitation and the perks of prompt engineering applies:\nSimplicity: Keep the function calls simple and concise. The more complex the function calls, the more likely it is that the model will make mistakes. Parameter Validation: Function parameter validation is not handled by the model. It is up to the developer to ensure that the parameters are valid. Guardrails: Establish Guardrails to prevent the model from calling functions that it should not call Remember that the model is just one component of the system. The effectiveness and safety of the model depend on how it is used and integrated into the broader context. It is the developer\u0026rsquo;s responsibility to tailor safety measures to their specific use case and ensure that the model\u0026rsquo;s outputs are safe and appropriate for the context.\nThe above code along with a working app is available in HF Spaces.\n","date":"2024-10-27T00:00:00Z","image":"https://example.com/images/2024-12-04-movies-app.gif","permalink":"https://example.com/p/building-natural-language-interfaces-with-llm-function-calling/","title":"Building Natural Language Interfaces with LLM Function Calling"},{"content":"Introduction Codenames is a word association game where two teams guess secret words based on one-word clues. The game involves a 25-word grid, with each team identifying their words while avoiding the opposing team\u0026rsquo;s words and the \u0026ldquo;assassin\u0026rdquo; word.\nI knew that word embeddings could be used to group words based on their semantic similarity. This seemed like a good way to cluster words on the board and generate clues. I was largely successful in getting this to work along with few surprises and learnings along the way.\nI have published a demo of this Gradio app along with its code in HF Spaces.\nInitial attempts My initial experiments with sentence embedding models did not yield satisfactory results. These models required a more context beyond individual words to deliver precise outcomes. Transitioning to word embedding models proved to be more effective; however, I still encountered challenges in filtering out undesired outputs such as foreign language terms and compound words. Additionally, employing similarity search techniques like kNN and Cosine similarity did not yield optimal results.\nI switched to word embedding models, which were better, but I still had to filter out unwanted outputs like foreign language words and compound words. Similarity search methods like kNN and Cosine similarity also didn\u0026rsquo;t give me the best results.\nExtracting the game words using OpenCV also turned out to be more involved than I expected. I had to deal with shadows, uneven exposure, grid detection, and draw bounding boxes. The upside-down clue words also caused problems for the character recognition model.\nMulti-Modal LLM Solution This is when I decided to try a small local Large Language Model (LLM) for this. I used microsoft/Phi-3-Mini-4K-Instruct which is a 3.8B parameters, lightweight LLM by Microsoft. Surprisingly this model consistently produced high-quality results with minimal effort. I split the task into 3 sections:\nIdentifying words in the game using OCR Grouping of words Generating clues for each groups OCR for text extraction The Phi-3 model family also has a multimodal version called microsoft/Phi-3-vision-128k-instruct with a focus on very high-quality, reasoning dense data both on text and vision. I used this model for OCR. It worked like a charm, eliminating the need for complex image processing techniques. Unfortunately the structured output generation library I used (Outlines, explained later) doesn\u0026rsquo;t yet support Vision models so I couldn\u0026rsquo;t use this as a single model for both OCR and text generation. So I leveraged Nvidia hosted LLM service (NIM) to perform the OCR task.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def process_image(img): # Resize the image max_size = (1024, 1024) img.thumbnail(max_size) image_byte_array = jpeg_with_target_size(img, 180_000) image_b64 = base64.b64encode(image_byte_array).decode() invoke_url = \u0026#34;https://ai.api.nvidia.com/v1/vlm/microsoft/phi-3-vision-128k-instruct\u0026#34; stream = False if os.environ.get(\u0026#34;NVIDIA_API_KEY\u0026#34;, \u0026#34;\u0026#34;).startswith(\u0026#34;nvapi-\u0026#34;): print(\u0026#34;Valid NVIDIA_API_KEY already in the environment. Delete to reset\u0026#34;) headers = { \u0026#34;Authorization\u0026#34;: f\u0026#34;Bearer {os.environ.get(\u0026#39;NVIDIA_API_KEY\u0026#39;, \u0026#39;\u0026#39;)}\u0026#34;, \u0026#34;Accept\u0026#34;: \u0026#34;text/event-stream\u0026#34; if stream else \u0026#34;application/json\u0026#34; } payload = { \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: f\u0026#39;Identify the words in this game of Codenames. Provide only a list of words. Provide the \u0026#39; f\u0026#39;words in capital letters only. \u0026lt;img src=\u0026#34;data:image/png;base64,{image_b64}\u0026#34; /\u0026gt;\u0026#39; } ], \u0026#34;max_tokens\u0026#34;: 512, \u0026#34;temperature\u0026#34;: 0.1, \u0026#34;top_p\u0026#34;: 0.70, \u0026#34;stream\u0026#34;: stream } response = requests.post(invoke_url, headers=headers, json=payload) if response.ok: print(response.json()) # Define the pattern to match uppercase words separated by commas pattern = r\u0026#39;[A-Z]+(?:\\s+[A-Z]+)?\u0026#39; words = re.findall(pattern, response.json()[\u0026#39;choices\u0026#39;][0][\u0026#39;message\u0026#39;][\u0026#39;content\u0026#39;]) return gr.update(choices=words, value=words) Grouping of words I employed few-shot prompting technique along with a custom system prompt to cluster words that share common characteristics. I observed that when we instruct the Language Model (LLM) to group words, the resulting clusters are often random, regardless of the initial instructions. However, prompting the LLM to group words and then explain the rationale behind the grouping led to more coherent and meaningful word groupings.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # Grouping the words def group_words(words): @outlines.prompt def chat_group_template(system_prompt, query, history=[]): \u0026#39;\u0026#39;\u0026#39;\u0026lt;s\u0026gt;\u0026lt;|system|\u0026gt; {{ system_prompt }} {% for example in history %} \u0026lt;|user|\u0026gt; {{ example[0] }}\u0026lt;|end|\u0026gt; \u0026lt;|assistant|\u0026gt; {{ example[1] }}\u0026lt;|end|\u0026gt; {% endfor %} \u0026lt;|user|\u0026gt; {{ query }}\u0026lt;|end|\u0026gt; \u0026lt;|assistant|\u0026gt; \u0026#39;\u0026#39;\u0026#39; grouping_system_prompt = (\u0026#34;You are an assistant for the game Codenames. Your task is to help players by grouping a \u0026#34; \u0026#34;given group of secrets into 3 to 4 groups. Each group should consist of secrets that \u0026#34; \u0026#34;share a common theme or other word connections such as homonym, hypernyms or synonyms\u0026#34;) prompt = chat_group_template(grouping_system_prompt, words, example_groupings) # Greedy sampling is sufficient since the objective is to generate grouping of existing words rather than # generating interesting new tokens sampler = samplers.greedy() generator = generate.json(model, Groups, sampler) print(\u0026#34;Grouping words:\u0026#34;, words) generations = generator( prompt, max_tokens=500 ) print(\u0026#34;Got groupings: \u0026#34;, generations) return [group.words for group in generations.groups] Generation of Clues I split the clue generation logic separately even though LLMs could generate them at the time of grouping the words together. This is because I wanted to be able to regenerate better clues for individual groups without having to regroup the entire word list every time.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def generate_clues(group): template = \u0026#39;\u0026#39;\u0026#39; {% for example in history %} INPUT: {{ example[0] }} OUTPUT: { \u0026#39;clue\u0026#39;:{{ example[1] }}, \u0026#39;explanation\u0026#39;:{{ example[2] }} } {% endfor %} INPUT: {{ query }} OUTPUT: {{ system }} Clue = {\u0026#39;clue\u0026#39;: str, \u0026#39;explanation\u0026#39;: str} Return: Clue \u0026#39;\u0026#39;\u0026#39; clue_system_prompt = (\u0026#34;You are a codenames game companion. Your task is to give a single word clue related to \u0026#34; \u0026#34;a given group of words. You will only respond with a single word clue. The clue can be a common theme or other word connections such as homonym, hypernyms or synonyms. Avoid clues that are not too generic or not unique enough to be guessed easily\u0026#34;) prompt = render_jinja2_template(template, clue_system_prompt, example_clues, group) raw_response = model.generate_content( prompt, generation_config={\u0026#39;top_k\u0026#39;: 3, \u0026#39;temperature\u0026#39;: 1.1}) response = json.loads(raw_response.text) print(\u0026#34;Generating clues for: \u0026#34;, group) print(\u0026#34;Got clue: \u0026#34;, json.dumps(response, indent=4)) return response Outlines One reason why I tried other options before LLMs because LLMs generations are stochastic and I needed parsable output for my web page i.e, I need guarantee that the output text will have a certain format. That\u0026rsquo;s when I came across the Outlines library which is one among the many popular guided generations libraries. It helped me generate structured text consistently, making it easy to integrate with my app\u0026rsquo;s interface.\nThe basic idea of Outlines is simple: in each state, it gets a list of symbols that correspond to completions that partially match the regular expression. It masks the other symbols in the logits returned by a large language model, so we derive a new FSM whose alphabet is the model\u0026rsquo;s vocabulary. We can do this in only one pass over the vocabulary.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Load LLM model using Outlines library model = models.transformers(\u0026#34;microsoft/Phi-3-mini-4k-instruct\u0026#34;, model_kwargs={\u0026#39;device_map\u0026#39;: \u0026#34;cuda\u0026#34;, \u0026#39;torch_dtype\u0026#39;: \u0026#34;auto\u0026#34;, \u0026#39;trust_remote_code\u0026#39;: True, \u0026#39;attn_implementation\u0026#39;: \u0026#34;flash_attention_2\u0026#34;}) # Generating structured output using Outlines class Clue(BaseModel): word: str explanation: str clue_system_prompt = (\u0026#34;You are a codenames game companion. Your task is to give a single word clue related to \u0026#34; \u0026#34;a given group of words. You will only respond with a single word clue. Compound words are \u0026#34; \u0026#34;allowed. Do not include the word \u0026#39;Clue\u0026#39;. Do not provide explanations or notes.\u0026#34;) prompt = chat_clue_template(clue_system_prompt, group, example_clues) generator = generate.json(model, Clue) generator(prompt, max_tokens=100) Reflections and the Road Ahead Initially, I underestimated the LLM approach, considering it excessive for what seemed like a straightforward issue. However, I was mistaken. Leveraging LLM significantly expedited the generation of usable outcomes, saving considerable time. Over time, despite its substantial size and computational demands, this ease of use aspect will boost its acceptance. As momentum builds, further research and optimization will ensue, resulting in more compact, effective, and intelligent models. Although optimized algorithms and specialized models will retain significance in particular domains, LLMs are positioned to transform numerous sectors with their adaptability and potency.\n","date":"2024-06-20T12:00:00Z","permalink":"https://example.com/p/building-a-codenames-ai-assistant-with-multi-modal-llms/","title":"Building a Codenames AI Assistant with Multi-Modal LLMs"},{"content":"Introduction By now, many of us may be familiar with text-to-image models like Midjourney, DALL·E 3, StableDiffusion etc., Recently, I came across an interesting project called Visual Anagrams that utilizes text-to-image model to generate picture illusions. This project enables us to input two different text prompts, and the model generates pictures that match the prompts under various transformations, such as flips, rotations, or pixel permutations. Growing up, I had a nerdy fascination with illusions and ambigrams, so I was thrilled to give this a try.\nDeepFloyd IF Model: Memory Requirements and Optimization Behind the scenes, Visual Anagrams utilizes the DeepFloyd IF model, which takes a unique approach to Stable diffusion. Unlike StableDiffusion which performs denoising in a latent space, DeepFloyd IF operates directly in the pixel space. This approach enables the model to better align with text and generate legible images, addressing a challenge faced by Stable Diffusion.\nHowever, these advantages come at a cost of significantly higher memory requirements. DeepFloyd IF is a modular model composed of a frozen text encoder and three cascaded pixel diffusion modules. Running the model in full float32 precision would require at least 37GB of memory.\nsource: DeepFloyd-IF model card Fortunately, it is possible to run this model on Google Colab or even on consumer hardware for free. The Diffusers API from HuggingFace allows us to load individual components modularly, reducing the memory requirements by loading components selectively.\nInference process Import and setup what we need First let us install the dependencies and a copy of visual anagrams repo.\n1 2 3 4 5 6 7 8 9 10 11 12 ! pip install -q diffusers transformers safetensors sentencepiece accelerate bitsandbytes einops mediapy accelerate !pip install -q git+https://github.com/dangeng/visual_anagrams.git Load TextEncoder Model The TextEncoder model used in DeepFloyd-IF is T5. To begin, we load this T5 model in half-precision (fp16) and utilize the device_map flag to enable transformers to offload model layers to either CPU or disk. This reduces the memory requirements by more than half. For more information on device_map, refer to the transformers documentation.\n1 2 3 4 5 6 7 8 9 from transformers import T5EncoderModel text_encoder = T5EncoderModel.from_pretrained( \u0026#34;DeepFloyd/IF-I-L-v1.0\u0026#34;, subfolder=\u0026#34;text_encoder\u0026#34;, device_map=\u0026#34;auto\u0026#34;, variant=\u0026#34;fp16\u0026#34;, torch_dtype=torch.float16, ) Addendum To further reduce memory utilization, we can also load the same T5 model using 8-bit quantization. Transformers directly supports bitsandbytes through the load_in_8bit flag. Set the variant=\u0026ldquo;8bit\u0026rdquo; flag to download pre-quantized weights. This allows loading the text encoders in as little as 8GB of memory.\nCreate text embeddings Next, we need to generate embeddings for the two prompts that describe the visual illusions. DiffusionPipeline from HuggingFace Diffusers library contains methods to load models necessary for running diffusion networks. We can override the individual models used by changing the keyword arguments to from_pretrained. In this case, we pass the previously instantiated text_encoder for the text_encoder argument and None for the unet argument to avoid loading the UNet into memory, enabling us to load only the necessary models to run the text embedding portion of the diffusion process.\n1 2 3 4 5 6 7 from diffusers import DiffusionPipeline pipe = DiffusionPipeline.from_pretrained( \u0026#34;DeepFloyd/IF-I-L-v1.0\u0026#34;, text_encoder=text_encoder, # pass the previously instantiated text encoder unet=None ) We can now use this pipeline to encode the two prompts. The prompts need to be concatenated for the illusion.\n1 2 3 4 5 6 7 8 9 10 11 # Feel free to change me: prompts = [ \u0026#39;an oil painting of a deer\u0026#39;, \u0026#39;an oil painting of a waterfall\u0026#39;, ] # Embed prompts using the T5 model prompt_embeds = [pipe.encode_prompt(prompt) for prompt in prompts] prompt_embeds, negative_prompt_embeds = zip(*prompt_embeds) prompt_embeds = torch.cat(prompt_embeds) negative_prompt_embeds = torch.cat(negative_prompt_embeds) # These are just null embeds Flush to free memory for the next stages.\n1 2 3 4 5 6 7 8 9 import gc def flush(): gc.collect() torch.cuda.empty_cache() del text_encoder del pipe flush() Main Diffusion Process With the available GPU memory, we can reload the DiffusionPipeline using only the UNet to execute the main diffusion process. Note that once again we are loading the weights in 16-bit floating point format using the variant and torch_dtype keyword arguments.\n1 2 3 4 5 6 7 8 9 10 11 from diffusers import DiffusionPipeline stage_1 = DiffusionPipeline.from_pretrained( \u0026#34;DeepFloyd/IF-I-L-v1.0\u0026#34;, text_encoder=None, variant=\u0026#34;fp16\u0026#34;, torch_dtype=torch.float16, ) stage_1.enable_model_cpu_offload() stage_1.to(\u0026#39;cuda\u0026#39;) 1 2 3 4 5 6 7 8 stage_2 = DiffusionPipeline.from_pretrained( \u0026#34;DeepFloyd/IF-II-L-v1.0\u0026#34;, text_encoder=None, variant=\u0026#34;fp16\u0026#34;, torch_dtype=torch.float16, ) stage_2.enable_model_cpu_offload() stage_2.to(\u0026#39;cuda\u0026#39;) Generate Image Choose one of the view transformations supported by the Visual Anagrams repository.\n1 2 3 4 5 6 7 8 9 10 11 12 from visual_anagrams.views import get_views # UNCOMMENT ONE OF THESE # views = get_views([\u0026#39;identity\u0026#39;, \u0026#39;rotate_180\u0026#39;]) # views = get_views([\u0026#39;identity\u0026#39;, \u0026#39;rotate_cw\u0026#39;]) # views = get_views([\u0026#39;identity\u0026#39;, \u0026#39;flip\u0026#39;]) # views = get_views([\u0026#39;identity\u0026#39;, \u0026#39;jigsaw\u0026#39;]) views = get_views([\u0026#39;identity\u0026#39;, \u0026#39;negate\u0026#39;]) # views = get_views([\u0026#39;identity\u0026#39;, \u0026#39;skew\u0026#39;]) # views = get_views([\u0026#39;identity\u0026#39;, \u0026#39;patch_permute\u0026#39;]) # views = get_views([\u0026#39;identity\u0026#39;, \u0026#39;pixel_permute\u0026#39;]) # views = get_views([\u0026#39;identity\u0026#39;, \u0026#39;inner_circle\u0026#39;]) Results Now, we are ready to generate the visual illusions. The sample_stage_1 function from visual anagrams repo accomplishes this and produces a $64 \\times 64$ image. Similarly, the sample_stage_2 function upsamples the resulting image while denoising all views, generating a $256 \\times 256$ image.\n1 2 3 4 5 6 7 8 9 10 11 from visual_anagrams.samplers import sample_stage_1, sample_stage_2 from visual_anagrams.utils import add_args, save_illusion, save_metadata image_64 = sample_stage_1(stage_1, prompt_embeds, negative_prompt_embeds, views, num_inference_steps=40, guidance_scale=10.0, reduction=\u0026#39;mean\u0026#39;, generator=None) mp.show_images([im_to_np(view.view(image_64[0])) for view in views]) 1 2 3 4 5 6 7 8 9 10 11 image = sample_stage_2(stage_2, image_64, prompt_embeds, negative_prompt_embeds, views, num_inference_steps=30, guidance_scale=10.0, reduction=\u0026#39;mean\u0026#39;, noise_level=50, generator=None) mp.show_images([im_to_np(view.view(image[0])) for view in views]) More Examples Here are few more examples of illusions generated using this model.\n|\nConclusion With this, we get a pretty impressive image of a waterfall which when inverted looks like a deer. I have a notebook version of the same code, you can give it a try in colab and try different transformation views. It is fascinating to observe how details from different objects and scenes can be embedded into a picture and how our visual apparatus end up seeing what we want to see.\nReferences https://huggingface.co/docs/diffusers/main/en/api/pipelines/deepfloyd_if https://huggingface.co/DeepFloyd/IF-I-XL-v1.0 https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/deepfloyd_if_free_tier_google_colab.ipynb ","date":"2024-01-22T12:00:00Z","permalink":"https://example.com/p/generating-visual-illusions-using-diffusion-models-on-low-vram/","title":"Generating Visual Illusions Using Diffusion Models On Low VRAM"},{"content":"Introduction While chatbots have grown common in applications like customer service, they have several shortcomings which disrupts user experience. Traditional chatbots rely on pattern matching and database lookups, which are ineffective when a user\u0026rsquo;s question deviates from what was expected. Responses may feel impersonal and fail to address the true intent when questions deviate slightly from pattern matching rules.\nThis is where large language models (LLMs) can provide value. LLMs are better equipped to handle out-of-scope questions due to their ability to understand context and previous exchanges. They can generate more personalized responses compared to typical rule-based chatbots. As such, chatbots represent a prime use case for generative AI in enterprises.\nWhen considering Generative AI use cases in an Enterprise context, it is hard to look past Chatbots utilizing Large Language Models (LLMs). Traditional chatbots can feel impersonal and inadequate due to their reliance on pattern matching and limited context understanding. In contrast, interacting with LLMs can feel natural since due to their improved understanding of context and personalized responses. This chatbot an ideal use case to explore Generate AI in enterprises.\nSource: Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond (arXiv:2304.13712) To help an LLM answer based on internal knowledge base, one approach utilizes prompting i.e., inserting knowledge corpora into the prompt along with user queries.\nHowever, using LLMs in this manner can lead to responses that lack constraints, resulting in potential factual inaccuracies and irrelevance, especially when dealing with topics beyond the scope of the training data. This phenomenon is referred to as \u0026ldquo;hallucination.\u0026rdquo;\nTo address this, knowledge retrieval can be incorporated to ground the LLM\u0026rsquo;s responses in factual information from curated sources. This is known as Retrieval Augmented Generation\nHow Retrieval Augmented Generation works Retrieval augmented generation (RAG) combines two main components: a retrieval model and a generation model.\nThe retrieval model searches external knowledge bases to extract facts relevant to a user\u0026rsquo;s query. It represents words and documents as embeddings, calculating similarity between query and document embeddings to retrieve top results.\nThe generation model—typically a large language model—accepts the retrieved information as input. It produces natural language responses guided by this context without direct access to the raw knowledge base. This anchoring to concrete facts mitigates generation of incorrect statements compared to relying solely on the LLM.\nsource: llm based chatbots to query your private knowledge base Advantages of RAG RAG offers several advantages over both LLM-based question answering (QA) systems and traditional rule-based chatbots.\nCompared to LLM QA, RAG can provide answers based on externally retrieved facts without the need for expensive fine-tuning. It also offers traceability to the cited information sources.\nIn contrast to traditional chatbots, RAG excels in handling ambiguous and out-of-scope queries through its contextual understanding. It goes beyond simple rule-based responses and generates more human-like and personalized answers, improving the user experience.\nThe hybrid approach also has computational benefits as well. By employing lighter models specifically designed for retrieval tasks that prioritize semantic similarity over language modeling, RAG enables quicker retrievals. This efficiency is achieved by operating in a lower dimensional space compared to LLM, resulting in faster response times.\nOverall, RAG leverages the strengths of both retrieval and language models. By anchoring flexible conversational abilities to verifiable facts, it offers a more robust framework for open-domain dialogue than previous paradigms alone.\nImplementation In this blog post, we will develop a retrieval augmented generation (RAG) based LLM application from scratch. We will be building a chatbot that answers questions based on a knowledge base. For the knowledge base, we will use E-commerce FAQ dataset.\nSetup Load documents The chat dataset used for this project, is in a JSON format as an array of key-value pairs. We will split it into chunks of n characters but to retain the information within each chunk, we will ensure that each QnA pair is loaded as an individual chunk.\n1 2 3 4 5 6 7 8 9 10 import json from pathlib import Path import uuid file_path=\u0026#39;./data/faq_dataset.json\u0026#39; data = json.loads(Path(file_path).read_text()) documents = [json.dumps(q) for q in data[\u0026#39;questions\u0026#39;]] # encode QnA as json strings for generating embeddings metadatas = data[\u0026#39;questions\u0026#39;] # retain QnA as dict in metadata ids = [str(uuid.uuid1()) for _ in documents] # unique identifier for the vectors Prepare embeddings Next we will use an embedding model to generate vector representations of the chunks. Here we are using BAAI/bge-small-en-v1.5 model. This is a tiny model, less than 150 MB in size and uses 384 dimensions to store semantic information, but it is sufficient for retrieval. Since embedding model needs to process a lot more tokens than answering model which only needs to process the prompt, it is better to keep it lightweight. If we have enough memory, we can use a larger model to generate embeddings.\n1 2 3 4 5 6 7 8 9 10 11 12 import chromadb from chromadb.utils import embedding_functions client = chromadb.Client() emb_fn = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\u0026#34;BAAI/bge-small-en-v1.5\u0026#34;) collection = client.create_collection( name=\u0026#34;retrieval_qa\u0026#34;, embedding_function=emb_fn, metadata={\u0026#34;hnsw:space\u0026#34;: \u0026#34;cosine\u0026#34;} # l2 is the default ) Since our FAQ dataset is very small, and we have a light embedding model, it is quite inexpensive to calculate the embeddings. So we will use in-memory non-persistent Chroma client as a Vector store. Vector stores is a data structure or database that specialize in storing and retrieving embeddings. They also provide methods for performing similarity search and Nearest neighbour search. Note that in ChromaDB, the default index type is Hierarchical Navigable Small Worlds (hnsw) and distance function is l2. However other distance functions are also available:\nDistance Parameter Equation Squared L2 l2 $$ d = \\sum\\left(A_i-B_i\\right)^2 $$ Inner product ip $$d = 1.0 - \\sum\\left(A_i \\times B_i\\right) $$ Cosine Similarity cosine $$d = 1.0 - \\frac{\\sum\\left(A_i \\times B_i\\right)}{\\sqrt{\\sum\\left(A_i^2\\right)} \\cdot \\sqrt{\\sum\\left(B_i^2\\right)}}$$ For more expensive embedding operations involving larger dataset or embedding models, we can use a persistent store such as the one offered by ChromaDB itself or other options such as pgVector, Pinecone, or Weaviate\nRetrieval Query Index We can now retrieve a set of documents closest to the query:\n1 2 3 query = \u0026#34;How can I open an account?\u0026#34; docs = collection.query(query_texts=[query], n_results=3) {'ids': [['d9b8bc80-7093-11ee-a189-00155d07b3f4', 'd9b8bee2-7093-11ee-a189-00155d07b3f4', 'd9b8bece-7093-11ee-a189-00155d07b3f4']], 'embeddings': None, 'documents': [['{\u0026quot;question\u0026quot;: \u0026quot;How can I create an account?\u0026quot;, \u0026quot;answer\u0026quot;: \u0026quot;To create an account, click on the \\'Sign Up\\' button on the top right corner of our website and follow the instructions to complete the registration process.\u0026quot;}', '{\u0026quot;question\u0026quot;: \u0026quot;Can I order without creating an account?\u0026quot;, \u0026quot;answer\u0026quot;: \u0026quot;Yes, you can place an order as a guest without creating an account. However, creating an account offers benefits such as order tracking and easier future purchases.\u0026quot;}', '{\u0026quot;question\u0026quot;: \u0026quot;Do you have a loyalty program?\u0026quot;, \u0026quot;answer\u0026quot;: \u0026quot;Yes, we have a loyalty program where you can earn points for every purchase. These points can be redeemed for discounts on future orders. Please visit our website to learn more and join the program.\u0026quot;}']], 'metadatas': [[{'question': 'How can I create an account?', 'answer': \u0026quot;To create an account, click on the 'Sign Up' button on the top right corner of our website and follow the instructions to complete the registration process.\u0026quot;}, {'question': 'Can I order without creating an account?', 'answer': 'Yes, you can place an order as a guest without creating an account. However, creating an account offers benefits such as order tracking and easier future purchases.'}, {'question': 'Do you have a loyalty program?', 'answer': 'Yes, we have a loyalty program where you can earn points for every purchase. These points can be redeemed for discounts on future orders. Please visit our website to learn more and join the program.'}]], 'distances': [[0.19405025243759155, 0.3536655902862549, 0.3666747808456421]]} For simple and straightforward user queries, it may be sufficient to return the top match. But consider a question like below:\n1 2 3 query = \u0026#34;What are the conditions for requesting a refund? Do I need to keep the receipt?\u0026#34; docs = collection.query(query_texts=[query], n_results=3) The top 3 responses are:\n[[{'question': \u0026quot;Can I return a product without a receipt?\u0026quot;, 'answer': \u0026quot;A receipt or proof of purchase is usually required for returns. Please refer to our return policy or contact our customer support team for assistance.\u0026quot;}, {'question': \u0026quot;Can I return a product if I no longer have the original receipt?\u0026quot;, 'answer': \u0026quot;While a receipt is preferred for returns, we may be able to assist you without it. Please contact our customer support team for further guidance.\u0026quot;}, {'question': \u0026quot;What is your return policy?\u0026quot;, 'answer': \u0026quot;Our return policy allows you to return products within 30 days of purchase for a full refund, provided they are in their original condition and packaging. Please refer to our Returns page for detailed instructions.\u0026quot;} ]] Clearly just returning the answer for the closest matched question will be incomplete and unsatisfactory for the user. The ideal answer needs to incorporate all facts from the relevant document chunks. This is where a generation model can help.\nGeneration Load a generative model LLMs are often trained and released as unaligned base models initially which simply take in text and predict the next token. Bloom, Llama2 and Mistral are examples of such base models. But for practical use, we often require models that are further fine-tuned for the task. For RAG and generally speaking for chat agents we need Instruct models that are further fine-tuned on instruction-response pairs.\nFor this demonstration, I used an instruction fine-tuned model Mistral-7B-Instruct-v0.1 from Mistral. The Mistral model is in particular impressive for the quality of its text generation given the relatively small model size (7B). This leads to quite performant prompt evaluation and response generation. I used the GPTQ quantized version which further reduces the model size and improves the prompt evaluation and token generation throughput significantly.\nGPTQ models are quantized versions that reduces memory requirements with a slight tradeoff of intelligence. Hugging Face transformers supports loading of GPTQ models since version 4.32.0 using AutoGPTQ library. You can learn more about this here\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import torch import transformers from transformers import AutoModelForCausalLM, AutoTokenizer models = { \u0026#34;wizardLM-7B-HF\u0026#34; : \u0026#34;TheBloke/wizardLM-7B-HF\u0026#34;, \u0026#34;wizard-vicuna-13B-GPTQ\u0026#34; : \u0026#34;TheBloke/wizard-vicuna-13B-GPTQ\u0026#34;, \u0026#34;WizardLM-13B\u0026#34; : \u0026#34;TheBloke/WizardLM-13B-V1.0-Uncensored-GPTQ\u0026#34;, \u0026#34;Llama-2-7B\u0026#34; : \u0026#34;TheBloke/Llama-2-7b-Chat-GPTQ\u0026#34;, \u0026#34;Vicuna-13B\u0026#34; : \u0026#34;TheBloke/vicuna-13B-v1.5-GPTQ\u0026#34;, \u0026#34;WizardLM-13B-V1.2\u0026#34; : \u0026#34;TheBloke/WizardLM-13B-V1.2-GPTQ\u0026#34;, \u0026#34;Mistral-7B\u0026#34; : \u0026#34;TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\u0026#34; } model_name = \u0026#34;Mistral-7B\u0026#34; tokenizer = AutoTokenizer.from_pretrained(models[model_name]) model = AutoModelForCausalLM.from_pretrained(models[model_name], torch_dtype=torch.float16, device_map=\u0026#34;auto\u0026#34;) Alternatively, you can use any of the other instruct models. I have had good results with WizardLM-13B as well. Note that the models we choose must fit the VRAM of your GPU. Often you can find the memory requirements of a model in their HuggingFace model card such as here.\nBuild a prompt Every instruct model works best when we provide it with prompts as per a specific template on which it was trained. Since this template can vary between models, to reliably apply model-specific chat template, we can use the Transformers chat template, which allows us to format a list of messages as per the model-specific chat template.\n1 2 3 4 5 6 7 8 chat = [ {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Hello, how are you?\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;I\u0026#39;m doing great. How can I help you today?\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;I\u0026#39;d like to show off how chat templating works!\u0026#34;}, ] tokenizer.use_default_system_prompt = True tokenizer.apply_chat_template(chat, tokenize=False) \u0026lt;s\u0026gt;[INST] \u0026lt;\u0026lt;SYS\u0026gt;\u0026gt;\\nYou are a helpful, respectful and honest support executive. Always answer as helpfully as possible, while being safe. While answering, use the information provided in the earlier conversations only. If the information is not present in the prior conversation, or If you don't know the answer to a question, please don't share false information. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. \\n\u0026lt;\u0026lt;/SYS\u0026gt;\u0026gt;\\n\\nHello, how are you? [/INST] I'm doing great. How can I help you today? \u0026lt;/s\u0026gt;\u0026lt;s\u0026gt;[INST] I'd like to show off how chat templating works! [/INST]\nIn our case, we want to customize the system prompt to pass the retrieved document chunks as a context for QnA. This is done by disabling the default system prompt and configuring the tokenizer to use default_chat_template. This allows us to override the message for the system role.\n1 2 3 4 5 6 7 8 9 10 11 chat = [] system_message = \u0026#34;You are a helpful, respectful and honest support executive. Always be as helpfully as possible, while being correct. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. Use the following piece of context to answer the questions. If the information is not present in the provided context, answer that you don\u0026#39;t know. Please don\u0026#39;t share false information.\u0026#34; for d in docs[\u0026#39;metadatas\u0026#39;][0]: # append context to system message system_message += f\u0026#34;\\n Question: {d[\u0026#39;question\u0026#39;]} \\n Answer: {d[\u0026#39;answer\u0026#39;]}\u0026#34; chat.append({\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: system_message}) chat.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: query}) prompt = tokenizer.apply_chat_template(chat, tokenize=False) This will insert a set of relevant questions and answers as additional context within the prompt so that the model can use this information to give an answer. For our example the constructed prompt looks like this:\n\u0026lt;s\u0026gt;[INST] \u0026lt;\u0026lt;SYS\u0026gt;\u0026gt; You are a helpful, respectful and honest support executive. Always be as helpfully as possible, while being correct. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. Use the following piece of context to answer the questions. If the information is not present in the provided context, answer that you don't know. Please don't share false information. Question: How can I create an account? Answer: To create an account, click on the 'Sign Up' button on the top right corner of our website and follow the instructions to complete the registration process. Question: Can I order without creating an account? Answer: Yes, you can place an order as a guest without creating an account. However, creating an account offers benefits such as order tracking and easier future purchases. Question: Do you have a loyalty program? Answer: Yes, we have a loyalty program where you can earn points for every purchase. These points can be redeemed for discounts on future orders. Please visit our website to learn more and join the program. \u0026lt;\u0026lt;/SYS\u0026gt;\u0026gt; How can I open an account? [/INST] Generate response Now we have everything needed to generate a user-friendly response from LLM.\n1 2 3 4 5 6 7 encodeds = tokenizer.apply_chat_template(chat, return_tensors=\u0026#34;pt\u0026#34;) model_inputs = encodeds.to(model.device) model.to(model.device) generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True) answer = tokenizer.batch_decode(generated_ids[:, model_inputs.shape[1]:])[0] \u0026lt;s\u0026gt;[INST] \u0026lt;\u0026lt;SYS\u0026gt;\u0026gt; You are a helpful, respectful and honest support executive. Always be as helpfully as possible, while being correct. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. Use the following piece of context to answer the questions. If the information is not present in the provided context, answer that you don't know. Please don't share false information. Question: How can I create an account? Answer: To create an account, click on the 'Sign Up' button on the top right corner of our website and follow the instructions to complete the registration process. Question: Can I order without creating an account? Answer: Yes, you can place an order as a guest without creating an account. However, creating an account offers benefits such as order tracking and easier future purchases. Question: Do you have a loyalty program? Answer: Yes, we have a loyalty program where you can earn points for every purchase. These points can be redeemed for discounts on future orders. Please visit our website to learn more and join the program. \u0026lt;\u0026lt;/SYS\u0026gt;\u0026gt; How can I open an account? [/INST] To open an account, click on the 'Sign Up' button on the top right corner of our website and follow the instructions to complete the registration process.\u0026lt;/s\u0026gt; Everything after the last token [/INST] is the response we seek. Keep in mind that, from an LLM perspective generating responses is merely continuing the text prompt that we passed to it. To retrieve the generated response we need to index from the input prompt length.\n1 answer = tokenizer.batch_decode(generated_ids[:, model_inputs.shape[1]:])[0] Build a Chat UI Now we have all the necessary ingredients to build a chatbot. Gradio library offers several ready-made components that simplify the process of building a Chat UI. We need to wrap our token generation process as below:\n1 2 3 4 5 6 7 8 9 10 import gradio as gr with gr.Blocks() as chatbot: with gr.Row(): answer_block = gr.Textbox(label=\u0026#34;Answers\u0026#34;, lines=2) question = gr.Textbox(label=\u0026#34;Question\u0026#34;) generate = gr.Button(value=\u0026#34;Ask\u0026#34;) generate.click(respond, inputs=question, outputs=[answer_block, global_state, exampleso]) chatbot.launch() Along with generating a response, we can also give a list of references to let the user know the source of truth for responses. We can also suggest other relevant questions that the users can click to follow. With these additions, the code for the chat component is as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 import gradio as gr import random samples = [ [\u0026#34;How can I return a product?\u0026#34;], [\u0026#34;What is the return policy?\u0026#34;], [\u0026#34;How can I contact customer support?\u0026#34;], ] def update_examples(): global samples samples = get_new_examples() return gr.Dataset.update(samples=samples) def respond(query): global samples docs = collection.query(query_texts=[query], n_results=3) chat = [] related_questions = [] references = \u0026#34;## References\\n\u0026#34; system_message = \u0026#34;You are a helpful, respectful and honest support executive. Always be as helpfully as possible, while being correct. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. Use the following piece of context to answer the questions. If the information is not present in the provided context, answer that you don\u0026#39;t know. Please don\u0026#39;t share false information.\u0026#34; for d in docs[\u0026#39;metadatas\u0026#39;][0]: # prepare chat template system_message += f\u0026#34;\\n Question: {d[\u0026#39;question\u0026#39;]} \\n Answer: {d[\u0026#39;answer\u0026#39;]}\u0026#34; # Update references references += f\u0026#34;**{d[\u0026#39;question\u0026#39;]}**\\n\\n\u0026#34; references += f\u0026#34;\u0026gt; {d[\u0026#39;answer\u0026#39;]}\\n\\n\u0026#34; # Update related questions related_questions.append([d[\u0026#39;question\u0026#39;]]) chat.append({\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: system_message}) chat.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: query}) encodeds = tokenizer.apply_chat_template(chat, return_tensors=\u0026#34;pt\u0026#34;) model_inputs = encodeds.to(model.device) streamer = TextStreamer(tokenizer) model.to(model.device) generated_ids = model.generate(model_inputs, streamer=streamer, temperature=0.01, max_new_tokens=100, do_sample=True) answer = tokenizer.batch_decode(generated_ids[:, model_inputs.shape[1]:])[0] answer = answer.replace(\u0026#39;\u0026lt;/s\u0026gt;\u0026#39;, \u0026#39;\u0026#39;) samples = related_questions related = gr.Dataset.update(samples=related_questions) yield [answer, references, related] def load_example(example_id): global samples return samples[example_id][0] with gr.Blocks() as chatbot: with gr.Row(): with gr.Column(): answer_block = gr.Textbox(label=\u0026#34;Answers\u0026#34;, lines=2) question = gr.Textbox(label=\u0026#34;Question\u0026#34;) examples = gr.Dataset(samples=samples, components=[question], label=\u0026#34;Similar questions\u0026#34;, type=\u0026#34;index\u0026#34;) generate = gr.Button(value=\u0026#34;Ask\u0026#34;) with gr.Column(): references_block = gr.Markdown(\u0026#34;## References\\n\u0026#34;, label=\u0026#34;global variable\u0026#34;) examples.click(load_example, inputs=[examples], outputs=[question]) generate.click(respond, inputs=question, outputs=[answer_block, references_block, examples]) chatbot.queue() chatbot.launch() Conclusion With the above setup, we can build a chatbot that can provide truthful responses based on an organization\u0026rsquo;s knowledge base. Since the model possesses the language understanding typical of all LLMs, it is able to respond to questions phrased in different manners. And since the responses are tailored to follow a question-and-answer format, the user experience is not disruptive and they don\u0026rsquo;t feel like talking to an impersonal bot.\nReference https://arxiv.org/abs/2005.11401 https://scriv.ai/guides/retrieval-augmented-generation-overview/ https://research.ibm.com/blog/retrieval-augmented-generation-RAG ","date":"2023-06-24T12:00:00Z","permalink":"https://example.com/p/2023-06-24-llm-powered-faq-chat-bot/","title":"Exploring Retrieval-Augmentated Generation with Open Source Large Language Models"},{"content":"Introduction When it comes to creating artwork, there are many Generative AI tools, but my favorite one is the vanilla Stable Diffusion. Since it is open source, an ecosystem of tools and techniques have sprouted around it. With it, you can train your own model, fine-tune existing models, or use countless other models trained and hosted by others.\nBut one of my favorite use case is to render rough sketches into much prettier artwork. In this post we will see how to setup real-time rendering so that we have an interactive drawing experience. See below to see how quickly we can come up with a decent painting.\nThis was just a rough draft done in 2 minutes, with a bit more skill and persistence it is possible to extract a more beautiful artwork as per your want.\nWhat I like about this approach is that it is interactive - you don\u0026rsquo;t go in with a pre-conceived notion. You take your artwork to where the canvas (Stable Diffusion in this case) leads you. Next we will see how to set it up on your own.\nInstructions For this, I made use of the excellent Stable Diffusion web UI project by AUTOMATIC1111. The Web UI also supports an API mode which we will use to generate images using img2img feature of Stable Diffusion. img2img uses the weights from Stable Diffusion to generate new images from an input image using StableDiffusionImg2ImgPipeline.\nIdeally you would require a GPU with more than 8GB VRAM. There are workarounds to run the model on lower end GPUs. Refer to AUTOMATIC1111 docs or wiki for more details.\nSetup Install Stable Diffusion Web UI by following the instructions in the project page. If you are on Windows, I would recommend running this directly on Windows rather than on WSL2. Navigating CUDA runtime dependencies across Windows + linux is not worth the time. We would also need Jupyter notebook and webuiapi packages to call Stable Diffusion Web UI API. At launch, AUTOMATIC1111 always sets up a VirtualEnv and pip installs the packages from requirements.txt. So add the packages notebook and webuiapi to the bottom of requirements.txt present at the project root. Jupyter notebook package will get installed at the next launch. Next we need to enable API support. For example, if you\u0026rsquo;re using Windows, edit the webui-user.bat file and add \u0026ndash;api \u0026ndash;api-log to the COMMANDLINE_ARGS line: 1 set COMMANDLINE_ARGS=--api Run the modified execution script. For example, on Windows, run webui-user.bat. It should launch Stable diffusion web UI In order to run Jupyter notebook, open a separate CMD shell, activate the venv and then launch jupyter notebook 1 2 3 4 5 ## Activate venv venv\\Scripts\\activate.bat ## Run Jupyter notebook start jupyter notebook Verify that Stable Diffusion Web UI is running by visiting http://localhost:7860/ in your browser. When you are done, remember to close both the CMD windows. Drawing Open a new notebook and add the following code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import webuiapi from IPython.display import clear_output import os import time from pathlib import Path from PIL import Image # create API client api = webuiapi.WebUIApi(sampler=\u0026#39;Euler a\u0026#39;, steps=20) file_base = \u0026#34;base.png\u0026#34; file_prompt = \u0026#34;prompt.txt\u0026#34; f_base_new = 0 f_prompt_new = 0 Add your desired prompt text to the file prompt.txt. In my example above, I used a very generic prompt like below: 1 an oil painting of a scenery by bob ross Create a PNG file called base.png using MS Paint or any of your favorite image editing tool and save it at the base path.\nRun this code from a new cell block in your notebook. It will monitor you files for changes periodically andcall Stable Diffusion web UI API to generate new images based on your updated drawings.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 while True: f_base = os.path.getmtime(file_base) f_prompt = os.path.getmtime(file_prompt) if f_base == f_base_new and f_prompt==f_prompt_new: time.sleep(0.5) else: f_base_new = f_base f_prompt_new = f_prompt prompt_txt = Path(file_prompt).read_text() with Image.open(file_base) as im: result2 = api.img2img( images=[im], prompt=prompt_txt, negative_prompt = \u0026#34;poorly drawn, photorealistic, watermark, logo, text, bad anatomy, missing fingers,missing body part,mangled hands, NSFW\u0026#34;, steps=40, seed=-1, styles=[], cfg_scale=7, width=512, height=724, denoising_strength=0.6) clear_output(wait=True) display(result2.image) Draw some rough strokes and watch stable-diffusion interpret and render it as a painting. Each rendering may take few seconds depending on the GPU configuration. You will have to save the file periodically to trigger the img2img inference. If you are using a tool like Photoshop, you can turn on autosave. ","date":"2023-05-16T12:00:00Z","image":"https://example.com/images/2023-05-16-stable-diffusion-art.png","permalink":"https://example.com/p/2023-05-16-paint-like-bob-ross-using-stable-diffusion/","title":"Create Beautiful Paintings From Rough Sketches Using Stable diffusion"},{"content":"In 2020, I wrote about GPT-3 model. Late last year, OpenAI released ChatGPT which was based on GPT-3 but trained using Reinforcement Learning from Human Feedback (RLHF). And now GPT-4 has been released. It has only been out for a few days, but it is already seeing incredible applications such as creating office documents, turning sketches into functional apps, creating personal tutors, and more.\nAnd not just GPT-based models, StableDiffusion and Dall-E are also pushing the boundaries of art, creating stunning visuals from mere textual descriptions. Professional ad agencies, too, are exploring how to use AI, as seen in this Coca-Cola ad and this Crocs ad which apparently took only 28 minutes to create from scratch.\nThe newly released GPT-4 exhibits human-level performance on a variety of common and professional academic exams. Source: OpenAI GPT-4 Technical Report Suddenly, the pace of advancements in AI has accelerated from a sluggish seep to a torrent, some might say it is not something we can control anymore. The dread of hyper-intelligence has long been a staple of our fears, for if an AI were to improve itself recursively, it would have no perceivable ceiling to the extent of its intelligence. Yet, such concerns remain speculations, given our limited understanding of AI\u0026rsquo;s internal workings (more on this below). We tend to project our apprehensions and anthropocentric tendencies onto AI and assume that it would act as a sentient being would. Rather than fretting over future possibilities, we should focus on the present: AI capabilities are already advanced enough to disrupt society.\nChanging Realities: The Societal Impact of Generative AI Human society has always evolved in a dynamic equilibrium with the technology it wields. Every major technological revolution has been accompanied by a sociological inflection point, as society adjusts not just to the novelty of new inventions but revises its implicit assumptions, norms and behaviors.\nThe cognitive revolution (language \u0026amp; arts) that laid the foundation for civilization by bringing people together, also gave us myths, hero worship, and kings. It took us millennia for society at large to loosen the shackles of such institutions. Similarly, the invention of the printing press brought about a new era of mass communication, but also led to the spread of propaganda and the manipulation of public opinion. With the rise of radio and television came new forms of propaganda and disinformation. With the internet and social media, we still haven\u0026rsquo;t figured out coping mechanisms for fake news and large-scale manipulation of public opinion.\nFew models to rule them all Generative AI brings with it all of the above risks, but on a scale and reach never imagined before. Almost all large scale models are derived from very few foundational models, further centralizing the risks and power. When I wrote about GPT-3, three things caught my attention:\nEffectiveness of scale Languages can be a model of the physical world Emergent behaviors leading to Zero to few shot learning However, I only thought of GPT-3 as a precursor for things to come. But recent products like ChatGPT that are built on GPT-3 have shown that GPT-3 and similar Large Language Models (LLM) are already good enough.\nAnd good enough models are easy to use, so their adoption will only continue to grow. As people become more familiar with these tools, they will begin to trust its judgment automatically. In place of a circumspect, potentially biased human, we will have an infallible, impartial arbitrator telling us what is true. But how can we know how it made its decision?\n\u0026ldquo;In place of a circumspect, potentially biased human, we will have an infallible, impartial arbitrator telling us what is true\u0026rdquo;\nEpistemology of Large Language Models: Defying Human Comprehension The performance of an LLM comes from their emergent behaviors rather than explicit instruction. For example, when OpenAI scaled GPT-3 to 175 billion parameters from 1.5 billion in GPT-2, they observed few-shot learning, an emergent property that was neither specifically trained for nor anticipated to arise. Even in the past researchers have warned that \u0026ldquo;despite their deployment into the real world, these models are very much research prototypes that are poorly understood\u0026rdquo;. It is just now more of the same.\nFurthermore, the resources required for training such large-scale models keep it out of reach for many; commercial incentives lend very few reasons for companies to make their models transparent or to attend to any of these social externalities arising from their obscurity. In essence, our attempts to grasp the nature of these models resemble those of blind men attempting to describe an elephant.\nCultural Norms Redefined As mentioned before, large-scale models are largely homogeneous, derived from very few foundation models; training data is often lopsided and only represents a tiny percentage of languages. The embedded social and political factors in these models lead to the entrenchment of a pre-dominant value system and undermine plurality. The prevalence of homogeneous LLMs in society can lead to cultural conformity where individual expression and differences are lost on several levels ranging from the individual to society.\nIn the past, LLMs have also produced inaccurate or harmful outputs. While these have been somewhat mitigated through fine-tuning, the notion that a mere handful of individuals could define the parameters of morality with any confidence is problematic. The obscure and hermetic nature of these large-scale models only amplifies our concerns. Chomsky in his NY Times piece argues that fine-tuning their ability to be original and opinionated. Thus, engineers \u0026ldquo;sacrificed creativity for a kind of amorality\u0026rdquo;\nThe Future of Work in an AI Dominated World There is always a concern that AI systems will replace human workers in a variety of industries, leading to job losses and economic disruption. Fears of job losses due to technology changes are not new. In the early 19th century, textile workers in England formed the Luddite movement to protest against the introduction of new textile technologies that threatened their jobs. Their paranoia turned out to be misguided as new technologies led to industry growth, creating new jobs and increasing productivity.\nThe increasing ability of AI to outperform humans in certain tasks has raised concerns about AI replacing human workforce. However, it is more likely that AI will operate in a more symbiotic relationship with the human workforce by enhancing and empowering human expertise rather than replacing them. For example, chess engines have long possessed the capabilities to defeat human players but now they are used to assist Grandmasters in their preparations. However, it is also important to acknowledge that while new technologies can bring in new avenues of employment there will be displacements in existing employment patterns. We as individuals and society, need to be prepared for these changes and adapt accordingly.\nThe Intersection Of AI and Art With the advent of DALL-E and Stable Diffusion, AI-generated art has now been thrust into the mainstream, and as such, concerns have arisen over the potential for plagiarism and disrespect for intellectual property rights. This is like the ancient paradox of \u0026ldquo;Ship of Theseus\u0026rdquo; - when does art transcend beyond its origins to become something entirely new? How do you distinguish and attribute ownership to your own contributions.\nSurrealist art generated using Stable Diffusion (MidJourney v4) Irrespective of how we address the plagiarism problem, I don\u0026rsquo;t see AI-generated art diminishing the need or impact of artists. Art in its purest form serves as a counterpoint to the cultural norms of contemporary society. If everyone is producing StableDiffusion art, certain predictable patterns begin to emerge that can feel soulless. The same can be said of ChatGPT or any AI-derived work. Folks have already noted that in StableDiffusion \u0026ldquo;the default mode of these images is to shine and sparkle, as if illuminated from within\u0026rdquo;. True art will find voice to shine through these mimics. On the other hand, mainstream artists who adopt derivative, orthodox forms of expression may become obsolete and be replaced by a new generation of artists who integrate technology into their art.\nArt in its purest form serves as a counterpoint to the cultural norms of contemporary society.\nBalancing Progress with Needs of Society and Safety In order to tackle the potential hazards of AI, we have to channel our innate ability to adapt. As much as we\u0026rsquo;d like to think we\u0026rsquo;re in charge of the progress of AI, we can\u0026rsquo;t possibly anticipate all the potential consequences and risks that come with AI and expect to control it with a set of rules and regulations. The genie is out of the bottle, and companies large and small will embrace AI in one way or another to keep up with the growing competition. Companies that embrace AI will have a distinct advantage over those that are slow to adopt it. Therefore, it is imperative that we approach the development and integration of AI with humility, acknowledging the complexity and unpredictability of the technology, and actively seeking to examine, understand and manage its potential impact on society and individual lives. Companies should also build AI systems to express doubts and train users regarding the error margins\n","date":"2023-03-15T12:00:00Z","image":"https://example.com/images/2020-09-26-meta-learning.png","permalink":"https://example.com/p/2023-03-15-gpt-4-stable-diffusion-and-beyond/","title":"GPT-4, Stable Diffusion, and Beyond: How Generative AI Will Shape Human Society"},{"content":"Wordle is a web-based word game which has become incredibly popular during the pandemic. It became so popular over a while that it was even bought by New York times for a significant sum and is currently hosted there. The game is a lot of fun to solve manually, but I am also interested in solving this computationally. This is my attempt at coming up with a solution strategy for the game.\nThe game The game is about guessing a five-letter word which changes every day. You get six tries to get it right. After every guess, the game tells you whether any of your letters are in the secret word and whether they are in the correct place.\nMy initial attempts involved getting 5-letter words from a well-known corpus like NLTK. But it turns out that wordle uses a two smaller dictionaries which can be extracted from Javascript. The challenges are from the first dictionary which is a smaller one consisting of more familiar words. The second word list is a larger one, which consists of words that are accepted as guesses. Using a dictionary may sound like cheating, but I just wanted explore the algorithm and math behind solver for the game.\nBase solution The game is very similar to Master Mind game (which in turn is similar to even older game Bulls and Cow). As a base solution we can use Donald Knuth\u0026rsquo;s Master Mind algorithm. The algorithm works as follows:\nCreate a set of candidates Play an initial guess from this list and get the response If the response is all green (ggggg) game is won Otherwise, filter the candidate list to contain only those words that would give the response that we got. For example, if we guessed ether and got a response rgggg then we can reduce our candidate space to [other, ither] Use a scoring strategy to choose the next best guess and repeat Here is an abstract python implementation of parts of this algorithm:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def color_remaining(c, index, pattern, count): if count.get(c, 0): pattern[index] = \u0026#39;y\u0026#39; count[c] -= 1 else: pattern[index] = \u0026#39;r\u0026#39; def compare(this, other): this_count, other_count = Counter(this), Counter(other) this_matches, other_matches = [None] * len(this), [None] * len(other) for index, this_char, other_char in zip(range(5), other, this): if this_char == other_char: other_matches[index] = this_matches[index] = \u0026#39;g\u0026#39; this_count[other_char] -= 1 other_count[this_char] -= 1 for index, this_char, other_char in zip(range(5), this, other): if not this_matches[index]: color_remaining(this_char, index, this_matches, other_count) color_remaining(other_char, index, other_matches, this_count) return \u0026#39;\u0026#39;.join(this_matches), \u0026#39;\u0026#39;.join(other_matches) def word_matches_pattern(word, guess, pattern): return compare(guess, word)[0] == pattern class Solver(ABC): def __init__(self, word_list): self.word_list = word_list self.first_word = self.top_word(word_list) def top_word(self, words): pass def next_guess(self, words, prev_guess=None, prev_result=None, explore=False): if not prev_guess: return self.first_word, words words = list(filter(lambda word: word_matches_pattern(word, prev_guess, prev_result), words)) return self.top_word(words), words @abstractmethod def score(self, word, words, char_counts): pass Now we need to choose an ideal scoring strategy that would allow us to make first guess as well as choose the best candidates among the remaining after every guess. This is where we can trial a few approaches and see which one gives the best result.\nCharacter frequency As a first pass, we can prioritize guessing the words containing most common characters. This should increase our odds of landing on the correct word. For example, here is the frequency of characters appearing in all 5 letter words in the dictionary:\nBy this criteria, soare is the ideal first guess as it is made up of the most frequent characters. Python implementation of this strategy would look like this:\n1 2 3 4 5 6 7 8 9 10 11 from collections import Counter def top_word(words): char_counts = Counter() for w in words: char_counts.update(w) scores = [(score(word, words, char_counts), word) for word in words] scores.sort(reverse=True) return scores[0][1] def score(self, word, counter): return sum(counter[c] for c in set(word)) True enough, it works well most of the time. Almost half the time, it only takes 3 attempts to guess the word correctly. And 9 out of 10 times we are able to guess within 6 attempts.\nBut we can do better. If we look at the words that took ong to solve, there are multiple candidates which are too similar to them. For example, it takes 13 attempts to predict the word wares\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 time_solve(\u0026#39;wares\u0026#39;, word_list) # Next guess, remaining word list soare 12972 aesir 61 rales 32 tares 12 nares 11 dares 10 cares 9 pares 8 mares 7 hares 6 gares 5 bares 4 fares 3 wares 2 The problem here is that once we reach the 3rd guess rales, there are 13 more possibilities for the first char. The only way to find the right answer is to try all 13 possibilities.\nRandom Explore One way to mitigate this scenario is if we can sacrifice first few attempts in trying to \u0026ldquo;explore\u0026rdquo; the solution space to learn more about valid and invalid characters. This way we are able to zero in on the right answer much quicker. In a way this is like exploration-exploitation strategies seen in reinforcement learning. After exploration, we can revert to frequency based scoring for exploitation.\n1 2 3 4 5 6 7 8 9 10 11 import random class RandomExploreExploit(Solver): def next_guess(self, words, prev_guess=None, prev_result=None, explore=False): words = list(filter(lambda word: word_matches_pattern(word, prev_guess, prev_result), words)) if explore: random.shuffle(words) return words[0], words[1:] return self.top_word(words), words def score(self, word, words, char_counts): return sum(char_counts[c] for c in word) Surprisingly this stochastic approach works better than the first method at a lower cost (we don\u0026rsquo;t do any scoring for the first 3 attempts). We are able to win the game 93% of the time. More specifically, my run failed 154 out of 2135 challenges. Can we do better?\nMaximum Entropy In the previous strategy, we kind of adopted an exploration-exploitation strategy which is normally used for problems whose probability distribution is not known apriori. But in this case, we can do better. Since the word list is already known, we can calculate the probability distribution. So how can we use this probability information to make a good choice?\nOne thing to note from Master Mind algorithm is that at every turn, we also learn more information about the target word. We can choose words that gives us more information about the target there by reducing our solution space at each turn.\nAs we are talking about expected information available from a probability distribution, we can use Claude Shannon\u0026rsquo;s definition of information entropy to assign a score. Here is how it is calculated:\nLet n be the number of words in the solution space Group the words into k partitions based on the pattern it generates when compared with our guess word i.e, if our guess word is east group ['fast', 'last'] into same partition as they would output a pattern rggg with our guess word. Let ki be the size of the ith partition after the current guess. Then the Shannon entropy score of the guess word is sum{pi * -log(pi)} where pi = i / n Here is the Python code to calculate entropy:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def generate_comparison_dict(word_list): df = pd.DataFrame(None, columns=word_list, index=word_list) for i, w1 in tqdm(enumerate(word_list)): df[w1][w1] = \u0026#39;g\u0026#39; * len(w1) for w2 in word_list[i + 1:]: this_pattern, other_pattern = compare(w1, w2) df[w1][w2] = other_pattern df[w2][w1] = this_pattern with pd.HDFStore(COMPARE_PATTERNS) as hdf: hdf.put(key=\u0026#34;df\u0026#34;, value=df) def entropy_score(guess, word_list): global compare_results_df if not isinstance(compare_results_df, pd.DataFrame): if not os.path.exists(COMPARE_PATTERNS): generate_comparison_dict() compare_results_df = pd.read_hdf(COMPARE_PATTERNS) probs = compare_results_df.loc[guess][word_list].value_counts() return stats.entropy(probs) By this criteria, tares is the best starting word which will give us most information. And it performs way better than the above 2 strategies. Out of 2315 target words, it only fails to guess around 35 words within the first 6 attempts leading to a success rate of 98.4%. In fact 90% of the words can be guessed within the first 4 attempts.\nConclusion Here is how the three strategies stack up next to each other. Entropy based solution is the clear winner by a mile. On the flip side, it is quite expensive to compute.\nYou can find the source code for the solver in the repo https://github.com/tmzh/wordle.git\nFurther references The best explanation I have seen for the Information entropy forumula is from Aurélien Géron (https://youtu.be/ErfnhcEV1O8). The concept of information entropy leads to cross entropy which is heavily used as loss function in classification methods. This video is quite short but incredible informative. Grant Sanderson (from 3Blue1Brown) has also released an excellent video (as usual) on solving Worlde using Information theory. He goes much more in detail into the intuition behind the entropy formula. ","date":"2022-03-07T12:00:00Z","image":"https://tmzh.github.io/images/2022-03-07-frequency_count.png","permalink":"https://example.com/p/2022-03-07-solvers-for-the-wordle-game-evaluation-of-different-strategies/","title":"Solvers for the Wordle game - Evaluation of strategies"},{"content":"In July 2021, AWS and Hugging Face announced collaboration to make Hugging Face a first party framework within SageMaker. Earlier, you had to use PyTorch container and install packages manually to do this. With the new Hugging Face Deep Learning Containers (DLC) availabe in Amazon SageMaker, the process of training and deploying models is greatly simplified.\nIn this post, we will go through a high level overview of Hugging Face Transformers library before looking at how to use the newly announced Hugging Face DLCs within Sagemaker.\nIntroduction to Hugging Face Transformers The Hugging Face Transformers is a library that makes it easy to use NLP models. It allows developers to leverage hundreds of pretrained models for Natural Language Understanding (NLU) tasks as well as making it simple to train new transformer models. The API of this library is based around 3 broad classes:\nModel - PyTorch or Keras models that we can use in training loop or for prediction Configuration - Stores all the configuration required to build a model Tokenizer - Stores the vocabulary and methods for encoding and decoding between strings and tokens The transformers library offers a simple abstraction over the above 3 models using the pipeline method. This is the simplest way to get started using the pre-trained models from model hub.\n1 2 classifier = pipeline(\u0026#39;sentiment-analysis\u0026#39;, model=\u0026#34;distilbert-base-uncased-finetuned-sst-2-english\u0026#34;) classifier(\u0026#39;We are very happy to show you the 🤗 Transformers library.\u0026#39;) The first argument is a Hugging Face NLP task, in this case it is sentiment analysis. Some of the supported tasks are:\nSequence Classification Sentiment Analysis Question Answering Language Modelling Text Generation Named Entity Recognition (NER) Summarization Translation See here for an overview of the tasks supported by the library.\nUnder the hood, calling the pipeline method roughly covers the following steps:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from transformers import AutoTokenizer, AutoModelForSequenceClassification from torch import nn # download the model and tokenizer model_name = \u0026#34;distilbert-base-uncased-finetuned-sst-2-english\u0026#34; model = AutoModelForSequenceClassification.from_pretrained(model_name) tokenizer = AutoTokenizer.from_pretrained(model_name) # tokenize the input batch = tokenizer([\u0026#34;The Hugging Face Transformers library is amazing\u0026#34;], padding=True, truncation=True, max_length=512, return_tensors=\u0026#34;pt\u0026#34;) # run predictions outputs = model(**batch) # returns logits for classification task # get predictions predictions = nn.functional.softmax(outputs.logits, dim=-1) print(predictions) The downloaded models are stored in ~/.cache/huggingface/transformers.\nHere is the process:\nInstantiate AutoTokenizer to download the tokenizer associated to the model we picked and instantiate it. Use AutoModelForSequenceClassification to download the model itself. Build a sequence from the input sentence, using the correct model-specific separators token type ids and attention masks Pass this sequence through the model to get the logits Compute the softmax of the result to get probabilities over the classes Tokenizer Tokenizer\u0026rsquo;s job is to preprocess your text into tokens suitable for training or inference. Tokens can be a word (predict) or a subword (##ly). For example, a tokenizer may split the word Transformers into (transform, ##ers) so that the model\u0026rsquo;s vocabulary doesn\u0026rsquo;t explode. The tokenizer can also take care of other pre-processing tasks such as normalizing cases and punctuations.\nThe tokenization logic is tied to the model we use. That is why in our example we derived the model and tokenizer from the same model name. The AutoTokenizer and AutoModelForxxx classes ensures that the tokenizers and models are paired correctly.\nWhen we apply a tokenizer to an input text, it returns a dictionary containing ids of the tokens and attention mask. ids are the numerical representation of tokens. To learn about attention mask and other details related to Tokenizers refer here.\n1 2 \u0026gt;\u0026gt;\u0026gt; tokenizer(\u0026#34;We are very happy to show you the 🤗 Transformers library.\u0026#34;) {\u0026#39;input_ids\u0026#39;: [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], \u0026#39;attention_mask\u0026#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} Note that the tokens also consists of some special tokens which encodes special meaning in the sentences. They differ from model to model. In our model, they are:\n1 2 3 4 5 6 7 8 \u0026gt;\u0026gt;\u0026gt; tokenizer.special_tokens_map { \u0026#39;cls_token\u0026#39;: \u0026#39;[CLS]\u0026#39;, \u0026#39;mask_token\u0026#39;: \u0026#39;[MASK]\u0026#39;, \u0026#39;pad_token\u0026#39;: \u0026#39;[PAD]\u0026#39;, \u0026#39;sep_token\u0026#39;: \u0026#39;[SEP]\u0026#39;, \u0026#39;unk_token\u0026#39;: \u0026#39;[UNK]\u0026#39; } Hugging Face Model Once the input text has been preprocessed by the tokenizer, we can pass it directly to the model\n1 outputs = model(**batch) The contents of the model output depends on the task. For SequenceClassification we get back logit, an optional loss, hidden_states and attentions attributes.\nThe model class can also be used to do transfer learning for custom NLP tasks. The Transformers library provides a Trainer API that takes this model as input, extracts the pre-trained weights and fine tunes it.\n1 2 3 4 5 6 7 8 9 10 11 12 from transformers import Trainer # Build the trainer class trainer = Trainer( model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset ) # Fine-tune the model trainer.train() This covers a high level overview of the Hugging Face Transformers library. Next we will see how to use the library along with Sagemaker.\nUsing Hugging Face on Sagemaker Hugging Face in collaboration with AWS released Sagemaker Hugging Face Deep Learning Containers (DLCs) that makes it easy to train and deploy Hugging Face models using AWS platform. In the following section, we will see how to use these DLCs to train and deploy Hugging Face Transformer models in AWS.\nRunning a Training job Preparing a training script First we need to prepare the training script. This would be similar to any Transformers training script. A minimal training script would look like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 %%writefile train.py from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer from sklearn.metrics import accuracy_score, precision_recall_fscore_support from datasets import load_from_disk import sys import argparse if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser() # Data, model, and output directories parser.add_argument(\u0026#34;--model_dir\u0026#34;, type=str, default=os.environ[\u0026#34;SM_MODEL_DIR\u0026#34;]) parser.add_argument(\u0026#34;--training_dir\u0026#34;, type=str, default=os.environ[\u0026#34;SM_CHANNEL_TRAIN\u0026#34;]) parser.add_argument(\u0026#34;--test_dir\u0026#34;, type=str, default=os.environ[\u0026#34;SM_CHANNEL_TEST\u0026#34;]) # load datasets train_dataset = load_from_disk(args.training_dir) test_dataset = load_from_disk(args.test_dir) # download model from model hub model_name = \u0026#34;distilbert-base-uncased-finetuned-sst-2-english\u0026#34; model = AutoModelForSequenceClassification.from_pretrained(model_name) tokenizer = AutoTokenizer.from_pretrained(model_name) # define training args training_args = TrainingArguments( output_dir=args.model_dir ) # create Trainer instance trainer = Trainer( model=model, args=training_args, train_dataset=train_dataset, eval_dataset=test_dataset, tokenizer=tokenizer, ) # train model trainer.train() # save the model trainer.save_model(args.model_dir) For a more complete version of the script covering model evaluation, logging and additional training arguments, refer to this sample script.\nAs with any Sagemaker training job, we need to ensure that this script reads data from DLC\u0026rsquo;s data input directory and saves the model to model directory. The following lines of codes takes care of this.\n1 2 3 4 5 6 7 8 9 # loading datasets parser.add_argument(\u0026#34;--training_dir\u0026#34;, type=str, default=os.environ[\u0026#34;SM_CHANNEL_TRAIN\u0026#34;]) parser.add_argument(\u0026#34;--test_dir\u0026#34;, type=str, default=os.environ[\u0026#34;SM_CHANNEL_TEST\u0026#34;]) train_dataset = load_from_disk(args.training_dir) test_dataset = load_from_disk(args.test_dir) # storing models trainer.save_model(args.model_dir) Run training using Hugging Face estimator First we create a Hugging Face estimator which exposes methods similar to other Sagemaker Estimator. Note that the entry_point attribute matches the file name of our training script.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from sagemaker.huggingface import HuggingFace # hyperparameters, which are passed into the training job hyperparameters={\u0026#39;epochs\u0026#39;: 1, \u0026#39;per_device_train_batch_size\u0026#39;: 32, \u0026#39;model_name_or_path\u0026#39;: \u0026#39;distilbert-base-uncased\u0026#39; } # create the Estimator huggingface_estimator = HuggingFace( entry_point=\u0026#39;train.py\u0026#39;, instance_type=\u0026#39;ml.p3.2xlarge\u0026#39;, instance_count=1, role=role, transformers_version=\u0026#39;4.4\u0026#39;, pytorch_version=\u0026#39;1.6\u0026#39;, py_version=\u0026#39;py36\u0026#39;, hyperparameters = hyperparameters ) Training is invoked by calling the fit method on Hugging Face Estimator.\n1 2 3 4 huggingface_estimator.fit( {\u0026#39;train\u0026#39;: \u0026#39;s3://sagemaker-us-east-1-558105141721/samples/datasets/imdb/train\u0026#39;, \u0026#39;test\u0026#39;: \u0026#39;s3://sagemaker-us-east-1-558105141721/samples/datasets/imdb/test\u0026#39;} ) The trained model is a tarball with all the resources needed for inference.\n1 2 3 4 5 6 model.tar.gz/ |- pytroch_model.bin |- vocab.txt |- tokenizer_config.json |- config.json |- special_tokens_map.json Deploying the model for inference Once the training is completed, we can deploy a Hugging Face model directly from the Estimator object.\n1 2 # deploy model to SageMaker Inference predictor = huggingface_estimator.deploy(initial_instance_count=1, instance_type=\u0026#34;ml.m5.xlarge\u0026#34;) Alternatively, if we already have a completed training job, we can used its output model to deploy a new Hugging Face model and deploy it.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from sagemaker.huggingface.model import HuggingFaceModel # create Hugging Face Model Class huggingface_model = HuggingFaceModel( model_data=\u0026#34;s3://models/my-bert-model/model.tar.gz\u0026#34;, # path to your trained sagemaker model role=role, # iam role with permissions to create an Endpoint transformers_version=\u0026#34;4.6\u0026#34;, # transformers version used pytorch_version=\u0026#34;1.7\u0026#34;, # pytorch version used py_version=\u0026#39;py36\u0026#39;, # python version used ) # deploy model to SageMaker Inference predictor = huggingface_model.deploy( initial_instance_count=1, instance_type=\u0026#34;ml.m5.xlarge\u0026#34; ) We can use this deployed model to make predictions on input text. The default inference script in Hugging Face DLC expects a dictionary with inputs as key. For details on default input formats for various tasks refer to this.\n1 2 3 4 5 6 # example request. data = { \u0026#34;inputs\u0026#34;: \u0026#34;Sagemaker SDK is easy to use\u0026#34; } # request predictor.predict(data) The above method makes use of Sagemaker SDK to invoke the model. Often in a production ML application, invocation is handled by calling InvokeEndpoint API via boto3 or other SDK. A sample boto3 based invocation would look like below:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import os import io import boto3 import json import csv # grab environment variables ENDPOINT_NAME = os.environ[\u0026#39;ENDPOINT_NAME\u0026#39;] runtime= boto3.client(\u0026#39;runtime.sagemaker\u0026#39;) def lambda_handler(event, context): print(\u0026#34;Received event: \u0026#34; + json.dumps(event, indent=2)) data = json.loads(json.dumps(event)) payload = data[\u0026#39;data\u0026#39;] print(payload) data = { \u0026#34;inputs\u0026#34;: \u0026#34;Sagemaker SDK is easy to use\u0026#34; } payload = json.dumps(data).encode(\u0026#39;utf-8\u0026#39;) response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType=\u0026#39;application/json\u0026#39;, Body=payload) Remember to delete the endpoint at the end of experiments.\n1 2 # delete endpoint predictor.delete_endpoint() Advanced features of the Inference toolkit We can also pass additional environment variables to the inference model that simplifies deployment.\n1 2 3 4 5 6 7 8 9 10 11 hub = { \u0026#39;HF_MODEL_ID\u0026#39;:\u0026#39;distilbert-base-uncased-distilled-squad\u0026#39;, \u0026#39;HF_TASK\u0026#39;:\u0026#39;question-answering\u0026#39; } # create Hugging Face Model Class huggingface_model = HuggingFaceModel( transformers_version=\u0026#39;4.6\u0026#39;, pytorch_version=\u0026#39;1.7\u0026#39;, py_version=\u0026#39;py36\u0026#39;, env=hub, Here, HF_TASK variable defines the task for the Transformers pipeline and HF_MODEL_ID defines the model id to load from huggingface.co/models. For the full list of supported environment variables refer to here.\nCustomizing Inference script When creating an inference model, we can specify use defined code/modules that allows us to customize the inference process.\nFor example, here is a barebones inference script which we will call inference.py:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from sagemaker_huggingface_inference_toolkit.transformers_utils import ( _is_gpu_available, get_pipeline, ) from sagemaker_huggingface_inference_toolkit import decoder_encoder if _is_gpu_available(): device = int(self.context.system_properties.get(\u0026#34;gpu_id\u0026#34;)) else: device = -1 def load_fn(model_dir): # gets pipeline from task tag hf_pipeline = get_pipeline(task=\u0026#39;sentiment-analysis\u0026#39;, model_dir=model_dir, device=device) return hf_pipeline def transform_fn(self, model, input_data, content_type, accept): processed_data = decoder_encoder.decode(input_data, content_type) predictions = model(processed_data[\u0026#39;my_custom_input\u0026#39;]) # Our custom input format response = decoder_encoder.encode(predictions, accept) return response To use this script, we need to place it under a source directory along with any additional files required.\n1 2 3 |- source/ |- inference.py |- requirements.txt Next when we create the Hugging FaceModel we need to set the source_dir and entry_point attribute. These attributes are derived from the Sagemaker Estimator Framework so they are available under all Frameworks.\n1 2 3 4 5 6 7 8 9 huggingface_model = HuggingFaceModel( model_data=\u0026#34;s3://models/my-bert-model/model.tar.gz\u0026#34;, source_dir=\u0026#39;source\u0026#39;, #relative path to current directory of calling script entry_point=\u0026#39;inference.py\u0026#39; #name of the inference script under the source dir role=role, transformers_version=\u0026#34;4.6\u0026#34;, pytorch_version=\u0026#34;1.7\u0026#34;, py_version=\u0026#39;py36\u0026#39;, ) This has the effect of setting the environment variables SAGEMAKER_SUBMIT_DIRECTORY to source and SAGEMAKER_PROGRAM to inference.py on the inference model. The inference model also has the files packaged with the following directory structure:\n1 2 3 4 5 6 model.tar.gz/ |- pytroch_model.bin |- .... |- code/ |- inference.py |- requirements.txt Now when we deploy the model, we can pass custom inputs to it.\n1 2 3 4 5 6 7 8 9 10 11 12 13 # deploy model to SageMaker Inference predictor = huggingface_model.deploy( initial_instance_count=1, instance_type=\u0026#34;ml.m5.xlarge\u0026#34; ) # example request. data = { \u0026#34;my_custom_input\u0026#34;: \u0026#34;The Hugging Face Transformers library is amazing\u0026#34; } # request predictor.predict(data) For further instructions on how to customize inference, refer to this\nAdditional resources To learn more, you can refer to:\nPhilosophy of Hugging Face transformers library Sample Hugging Face Transformers Notebooks Hugging Face on Amazon Sagemaker ","date":"2021-09-16T12:00:00Z","image":"https://huggingface.co/blog/assets/17_the_partnership_amazon_sagemaker_and_hugging_face/cover.png","permalink":"https://example.com/p/2021-09-16-using-hugging-face-transformers-on-aws-sagemaker/","title":"Using Hugging Face Transformers on AWS Sagemaker"},{"content":"Firefox, like other modern browsers, has an excellent in-built JSON viewer. It also supports Data URIs which allows you to load HTML resource from text in URL as if they were external resources. We can make use of these two features to have a handy JSON previewer which can be invoked from command line.\nFor example, when you enter the below link into your browser, it opens a \u0026ldquo;Hello world\u0026rdquo; text document.\n1 data:,Hello%2C%20World! This content is not limited to plain text. It can even be an HTML document:\n1 data:text/html,%3Ch1%3EHello%2C%20World!%3C%2Fh1%3E Or any other supported MIME type:\n1 data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGQAAABkCAQAAADa613fAAAAIGNIUk0AAHomAACAhAAA+gAAAIDoAAB1MAAA6mAAADqYAAAXcJy6UTwAAAACYktHRAD/h4/MvwAAAAd0SU1FB+UDGxInJl2PylkAAABqdEVYdFJhdyBwcm9maWxlIHR5cGUgYXBwMQAKYXBwMQogICAgICAzNAo0OTQ5MmEwMDA4MDAwMDAwMDEwMDMxMDEwMjAwMDcwMDAwMDAxYTAwMDAwMDAwMDAwMDAwNDc2ZjZmNjc2YzY1MDAwMAqnX4qZAAAC5UlEQVR42u3ZX2iVZRwH8M/Z2dqa0zP/tNxATc2wYDJZDmnOiJoURRBWGiLRfRcVdFURQdFNeBGjLiqoQIhuizBvzCiFWMayJllghK6kUteYW27zdLOoi9jynIf33anf5+pcnOf3/L7nfZ/3fd73EEIIIYQQQgghhBBCCCGEEEKeChnP12i1laZ855e0heszDFHnNg+bdNSwy6mLFzMM8ohnfOBT7TZaYcxobQbp8qL3bXSfBuc1Welnv6Urn92pdbcWPQ56yogy6tRlNve/VucBT1g2xzcK9vvC1rwbnc8Gp5Tt0zBHkJ268m5zfve6pOycvvxaSHOeltRjqZ21HmRi9r6wbc51UgNBRlwEq7XXdpDvnQEt2qqsdJ2OPIP85MhstWrvS2u8anvmO8C/6Teq7IItVdYpGvCjRzXnFeQq+5R9kmCxb3bWJW9am1eUVnvcnKBOkwPKyobsuJJh6TaNk44bSVBnWp9uxwwp+tx0sv4yV7DfQR2yf+hLbLkjHrzyYQtvK73JlMP/hSA7HHI27yaq1+49myoZuNCOyB2+9VXeTVRvibf0VDZ0YR2RO502mHcT1WvzmhsrHbyQjsgeg07UfpDt1nkn+2kbXJt0C7HG63m8ZWn0nLc1Jqu3zCt2ZR+Du4w7aVWiaku97Mlq9+GVrZFtmq23N8kK6/CScQNm8ghSwgm32G1RlTG2eMOvXjCR4CepwLMOWWWJXg/pU6qwSqvHHPe0q/MJAet1zn5a7HaP2+uGOd77/pMl7nfYsN2p/g9IcQkt6XWrJkM+c8r4PN9utFa/XW7yoed9nSZGusfJJpvdo1vBSV/6xogLLpoyo6ygqEGL5dbp0qvbYkcNOJByZaS8qRW02apfj3aMOWfUhGlFzUpWuEYJZ3zkXR8bSzhz4iB/Vmx1vU6dNujQqknRjEnn/WDYoGNOV3upzSbIX+ot0qxR0WW/Gzdeyy93QgghhBBCCCGEEEII/wd/APMIkSFxtxhoAAAAJXRFWHRkYXRlOmNyZWF0ZQAyMDIxLTAzLTI3VDEwOjQwOjA3KzA4OjAwvOJovAAAACV0RVh0ZGF0ZTptb2RpZnkAMjAyMS0wMy0yN1QxMDozOTozOCswODowMGBWmUMAAAAASUVORK5CYII= Here we are passing in base64-encoded version of the png image.\nIf you are on linux, it is easy to generate base64 encoded url:\n1 cat smiley.png | base64 | xargs -I % firefox \u0026#34;data:application/png;base64,%\u0026#34; Firefox also ships with an inbuilt JSON viewer. The document rendered should be of MIME type application/json. We can use the same trick to pipe any JSON output to firefox:\n1 cat person.json | base64 | xargs -I % firefox \u0026#34;data:application/json;base64,%\u0026#34; This is quite handy when trying to navigate long JSON responses from awscli. In awscli just disable the cli pager and pipe the base64 encoded result firefox.\n1 aws ec2-describe-subnets --no-cli-pager | base64 | xargs -I % \u0026#34;data:application/json;base64,%\u0026#34; This gives us a nice visual interface to inspect the json document.\nWe can even drop it into a handy bash function and pipe json output to this function\n1 2 3 4 5 json_viewer(){ cat /dev/stdin | base64 -w 0 | xargs -I % firefox \u0026#34;data:application/json;base64,%\u0026#34; } aws ec2 describe-subnets --no-cli-pager | json_viewer Now when firefox starts supporting intelligent filtering, using either JMESPath or jq syntax, this can become even more useful.\n","date":"2021-03-26T12:00:00Z","image":"https://raw.githubusercontent.com/cucumber/godog/master/logo.png","permalink":"https://example.com/p/2021-03-26-previewing-command-line-json-output-using-firefox/","title":"Previewing command line JSON output using firefox"},{"content":"Terratest is a popular library for testing Terraform code. Testing Infrastructure As Code (IAC) is not as widespread as it should be. The reasons are multi-fold, ranging from developer\u0026rsquo;s attitude towards testing to the difficulty of writing unit tests because of inherent side effects of IAC. Nevertheless, testing is no less important, in particular under these scenarios:\nWhen your module gets complicated, with medium to complex behaviour logic When your module makes underlying assumptions of external dependencies (such as AWS SCPs at Organization level permitting certain actions) In this post, we will take a look at using Terratest to test Terraform code. A typical Terratest testing pattern involves:\nDeploying real infrastructure in real environment Asserting that the deployed resources behaves as expected Undeploy everything at the end of the test. Behavior Driven Test (BDD) uses examples to describe the behavior of a system. It serves the dual purpose of testing the code and documenting it at the same time. Terratest is not a BDD testing framework, however it is possible to write BDD tests that executes Terratest code. In a later section of this post, we will see how this can be achieved using Godog which is a Go BDD testing library.\nA Basic Test Scenario Terraform code Let us start with a simple terraform module that deploys a Hello world lambda function.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 terraform { required_version = \u0026#34;\u0026gt;= 0.14.6\u0026#34; required_providers { archive = { source = \u0026#34;hashicorp/archive\u0026#34; version = \u0026#34;1.3\u0026#34; } } } data \u0026#34;archive_file\u0026#34; \u0026#34;zip\u0026#34; { type = \u0026#34;zip\u0026#34; source_dir = \u0026#34;${path.module}/src\u0026#34; output_path = \u0026#34;${path.module}/${var.function_name}.zip\u0026#34; } variable \u0026#34;function_name\u0026#34; { description = \u0026#34;The name of the function to provision\u0026#34; default = \u0026#34;test_lambda_function\u0026#34; } resource \u0026#34;aws_lambda_function\u0026#34; \u0026#34;lambda\u0026#34; { filename = data.archive_file.zip.output_path source_code_hash = data.archive_file.zip.output_base64sha256 function_name = var.function_name role = aws_iam_role.lambda.arn handler = \u0026#34;handler\u0026#34; runtime = \u0026#34;go1.x\u0026#34; } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;lambda\u0026#34; { name = var.function_name assume_role_policy = data.aws_iam_policy_document.policy.json } data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;policy\u0026#34; { statement { actions = [\u0026#34;sts:AssumeRole\u0026#34;] principals { type = \u0026#34;Service\u0026#34; identifiers = [\u0026#34;lambda.amazonaws.com\u0026#34;] } } } output \u0026#34;lambda_function\u0026#34; { value = aws_lambda_function.lambda.id } Here is the lambda script that we plan to deploy. It is a slightly modified version taken from terratest examples repo.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/aws/aws-lambda-go/lambda\u0026#34; ) type Event struct { Name string `json:\u0026#34;Name\u0026#34;` } func HandleRequest(ctx context.Context, evnt Event) (string, error) { return fmt.Sprintf(\u0026#34;Hello %s!\u0026#34;, evnt.Name), nil } func main() { lambda.Start(HandleRequest) } Terratest code To test this using Terratest, we need to write tests using Go\u0026rsquo;s built-in package testing. This means that we create a file ending with _test.go which implements test cases in a function with name TestXxxx. In our case, the test script is called main_test.go and test function is called TestTerraformAwsLambdaFunction. Here is the folder structure:\n1 2 3 4 5 6 📁 lambda_basic ├ 📁 src │ ├ 📄 handler.go ├ 📁 test │ ├ 📄 main_test.go ├ 📄 main.tf The content of the test case is:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 package test import ( \u0026#34;github.com/gruntwork-io/terratest/modules/aws\u0026#34; \u0026#34;github.com/gruntwork-io/terratest/modules/terraform\u0026#34; \u0026#34;github.com/stretchr/testify/assert\u0026#34; \u0026#34;testing\u0026#34; ) type Payload struct { Name string } func TestBasicLambdaFunction(t *testing.T) { t.Parallel() awsRegion := \u0026#34;us-east-1\u0026#34; terraformOptions := terraform.WithDefaultRetryableErrors(t, \u0026amp;terraform.Options{ TerraformDir: \u0026#34;..\u0026#34;, EnvVars: map[string]string{ \u0026#34;AWS_DEFAULT_REGION\u0026#34;: awsRegion, }, }) defer terraform.Destroy(t, terraformOptions) terraform.InitAndApply(t, terraformOptions) functionName := terraform.Output(t, terraformOptions, \u0026#34;lambda_function\u0026#34;) response := aws.InvokeFunction(t, awsRegion, functionName, Payload{Name: \u0026#34;World\u0026#34;}) assert.Equal(t, `\u0026#34;Hello World!\u0026#34;`, string(response)) } To test the module, simply run go test command under the test directory.\n1 2 cd test go test -v This performs the following steps:\nSetup the root directory of the terraform code. This is specified using TerraformDir option. Deploy Lambda function using terraform init and terraform apply code. This is done by calling terraform.InitAndApply function. Retrieve the resources that have been deployed using terraform.Output method. This is handy when we need to use generated attributes such as resource arn in the subsequent test statements. These attributes have to be exported using terraform output resource for this to work. Terratest provisions real resources and it will cost money. To avoid incurring cost, we always follow up the test cases by a deferred call to terraform.Destroy method. This statement executes last after all test cases and it cleans up the test resources. If you have multiple test functions, you can run a specific test as well.\n1 go test -run TestBasicLambdaFunction You can get the complete code for this scenario here\nTesting multiple behaviours Now, it is possible to test more than one test scenarios simply by adding more lines or functions for test cases. For example, we can send an erraneous input to lambda function and expect that it fails with a particular error message.\n1 2 3 4 5 6 // Invoke the function, this time causing it to error and capturing the error response, err := aws.InvokeFunctionE(t, awsRegion, functionName, ExampleFunctionPayload{ShouldFail: true, Echo: \u0026#34;hi!\u0026#34;}) // Function-specific errors have their own special return functionError, ok := err.(*aws.FunctionError) require.True(t, ok) At some point of time, adding more test cases like this is going to become unwieldy. Later we will see how to make the test cases more readable and self-documenting by writing BDD style test cases.\nPassing other terraform options We can also pass custom options to the test code. For example, if we want to override the function_name variable, we can pass it as a Vars parameter to terraform options.\n1 2 3 4 terraformOptions := terraform.WithDefaultRetryableErrors(t, \u0026amp;terraform.Options{ Vars: map[string]interface{}{ \u0026#34;function_name\u0026#34;: \u0026#34;test_lambda_function_v2\u0026#34;, }, This would be same as calling terraform command with -var options. This would take precedence over any variable set in terraform code. These are the available options that can be used in terratest as of the time of writing:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 type Options struct { TerraformBinary string // Name of the binary that will be used TerraformDir string // The path to the folder where the Terraform code is defined. // The vars to pass to Terraform commands using the -var option. Note that terraform does not support passing `null` // as a variable value through the command line. That is, if you use `map[string]interface{}{\u0026#34;foo\u0026#34;: nil}` as `Vars`, // this will translate to the string literal `\u0026#34;null\u0026#34;` being assigned to the variable `foo`. However, nulls in // lists and maps/objects are supported. E.g., the following var will be set as expected (`{ bar = null }`: // map[string]interface{}{ // \u0026#34;foo\u0026#34;: map[string]interface{}{\u0026#34;bar\u0026#34;: nil}, // } Vars map[string]interface{} VarFiles []string // The var file paths to pass to Terraform commands using -var-file option. Targets []string // The target resources to pass to the terraform command with -target Lock bool // The lock option to pass to the terraform command with -lock LockTimeout string // The lock timeout option to pass to the terraform command with -lock-timeout EnvVars map[string]string // Environment variables to set when running Terraform BackendConfig map[string]interface{} // The vars to pass to the terraform init command for extra configuration for the backend RetryableTerraformErrors map[string]string // If Terraform apply fails with one of these (transient) errors, retry. The keys are a regexp to match against the error and the message is what to display to a user if that error is matched. MaxRetries int // Maximum number of times to retry errors matching RetryableTerraformErrors TimeBetweenRetries time.Duration // The amount of time to wait between retries Upgrade bool // Whether the -upgrade flag of the terraform init command should be set to true or not NoColor bool // Whether the -no-color flag will be set for any Terraform command or not SshAgent *ssh.SshAgent // Overrides local SSH agent with the given in-process agent NoStderr bool // Disable stderr redirection OutputMaxLineSize int // The max size of one line in stdout and stderr (in bytes) Logger *logger.Logger // Set a non-default logger that should be used. See the logger package for more info. Parallelism int // Set the parallelism setting for Terraform PlanFilePath string // The path to output a plan file to (for the plan command) or read one from (for the apply command) } BDD Testing using GoDog Let us add more behaviors to our Terraform code. For instance, suppose we want to want to assign an IAM role to the lambda function that grants permission to log to Cloudwatch.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 resource \u0026#34;aws_lambda_function\u0026#34; \u0026#34;lambda\u0026#34; { filename = data.archive_file.zip.output_path source_code_hash = data.archive_file.zip.output_base64sha256 function_name = var.function_name role = aws_iam_role.lambda.arn handler = \u0026#34;handler\u0026#34; runtime = \u0026#34;go1.x\u0026#34; } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;lambda\u0026#34; { name = var.function_name assume_role_policy = data.aws_iam_policy_document.policy.json } data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;policy\u0026#34; { statement { actions = [\u0026#34;sts:AssumeRole\u0026#34;] principals { type = \u0026#34;Service\u0026#34; identifiers = [\u0026#34;lambda.amazonaws.com\u0026#34;] } } } # See also the following AWS managed policy: AWSLambdaBasicExecutionRole resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;lambda_logging\u0026#34; { name = \u0026#34;lambda_logging\u0026#34; path = \u0026#34;/\u0026#34; description = \u0026#34;IAM policy for logging from a lambda\u0026#34; policy = \u0026lt;\u0026lt;EOF { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34; } ] } EOF } resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;lambda_logs\u0026#34; { role = aws_iam_role.lambda.name policy_arn = aws_iam_policy.lambda_logging.arn } Now, when we deploy this, not only do we need to assert that all resources are deployed properly (Lambda function, IAM role etc.,) but we also need to assert that the Lambda function can send logs to Cloudwatch. This is where BDD tests can be useful.\nFirst we need to specify our expected behavior using a gherkin Feature file. Create a file called features/Smoke.feature\n1 2 3 4 5 6 7 8 9 10 Feature: Simple test to confirm lambda function behavior Confirms that given a valid terraform variable Lambda resources are deployed The Lambda function executes as intended Scenario: Deploy a Lambda function Given Terraform code is deployed with these variables: |function_name | random_name| Then For given inputs Lambda function output is as expected: |world | \u0026#34;Hello world!\u0026#34;| Then Cloudwatch log stream is generated To test this, we will use godog BDD framework for Golang. Let us create a Godog test function and call it bdd_test.go.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 type godogFeaturesScenario struct { testing *testing.T terraformOptions *terraform.Options stepValues map[string]string } func TestLambdaFunctionBDD(t *testing.T) { t.Parallel() opts := godog.Options{ Format: \u0026#34;progress\u0026#34;, Paths: []string{\u0026#34;features\u0026#34;}, Randomize: time.Now().UTC().UnixNano(), } o := \u0026amp;godogFeaturesScenario{} o.testing = t godog.TestSuite{ Name: \u0026#34;LambdaTest\u0026#34;, TestSuiteInitializer: InitializeTestSuite, ScenarioInitializer: o.InitializeScenario, Options: \u0026amp;opts, }.Run() } Here we pass our Feature file location in the option godog.Options{Paths: []string{\u0026quot;features\u0026quot;}}. We also need to pass TestSuiteInitializer and ScenarioInitializer as part of the TestSuite specs. These functions allows us to hook to events such as BeforeScenario, AfterScenario.\nThose coming from a BDD framework like Behave will notice that Godog doesn\u0026rsquo;t support a mutable context object. So it cannot be used to pass values between each step. Instead we have to create a struct called godogFeaturesScenario on which we implement a ScenarioInitializer function.\nThis allows us to pass objects like Testing.T, terraform.Options which are shared across multiple steps. We have also added a stepValues parameter which can be used to capture values from intermediate steps (like getting resource ARN from terraform.Output)\nNext step would be to map the Step definitions to go functions.\n1 2 3 4 5 6 7 8 func (o *godogFeaturesScenario) InitializeScenario(ctx *godog.ScenarioContext) { o.stepValues = make(map[string]string) ctx.Step(`^Terraform code is deployed with these variables:$`, o.terraformIsDeployedWithVariables) ctx.Step(`^For given inputs Lambda function output is as expected:$`, o.givenInputsLambdaReturnsValuesAsExpected) ctx.Step(`^Cloudwatch log stream is generated$`, o.cloudwatchLogIsGenerated) ctx.AfterScenario(o.destroyTerraform) } Here we also add an AfterScenario event hook which always makes sure that we destroy Terraform resources at the end of the test.\nNext we implement the functions. Note that Terratest also provides us some helper functions like github.com/gruntwork-io/terratest/modules/aws which makes it simpler to perform actions like aws.InvokeFunction. In other case, we need to use AWS Go SDK to make calls such as cloudwatch.DescribeLogGroups\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 func (o *godogFeaturesScenario) terraformIsDeployedWithVariables(tbl *godog.Table) error { tfVars := make(map[string]interface{}) for _, row := range tbl.Rows { tfVars[row.Cells[0].Value] = row.Cells[1].Value } o.stepValues[\u0026#34;awsRegion\u0026#34;] = \u0026#34;us-east-1\u0026#34; terraformOptions := terraform.WithDefaultRetryableErrors(o.testing, \u0026amp;terraform.Options{ TerraformDir: \u0026#34;..\u0026#34;, EnvVars: map[string]string{ \u0026#34;AWS_DEFAULT_REGION\u0026#34;: o.stepValues[\u0026#34;awsRegion\u0026#34;], }, }) o.terraformOptions = terraformOptions terraform.InitAndApply(o.testing, terraformOptions) return nil } func (o *godogFeaturesScenario) givenInputsLambdaReturnsValuesAsExpected(tbl *godog.Table) error { o.stepValues[\u0026#34;functionName\u0026#34;] = terraform.Output(o.testing, o.terraformOptions, \u0026#34;lambda_function\u0026#34;) for _, row := range tbl.Rows[1:] { input := row.Cells[0].Value expected := row.Cells[1].Value response := aws.InvokeFunction(o.testing, o.stepValues[\u0026#34;awsRegion\u0026#34;], o.stepValues[\u0026#34;functionName\u0026#34;], Payload{Name: input}) actual := string(response) if expected != actual { return fmt.Errorf(\u0026#34;Not equal: \\n\u0026#34;+ \u0026#34;expected: %s\\n\u0026#34;+ \u0026#34;actual : %s\u0026#34;, expected, actual) } } return nil } func (o *godogFeaturesScenario) cloudwatchLogIsGenerated() error { logGroupName := fmt.Sprintf(\u0026#34;/aws/lambda/%s\u0026#34;, o.stepValues[\u0026#34;functionName\u0026#34;]) client := aws.NewCloudWatchLogsClient(o.testing, o.stepValues[\u0026#34;awsRegion\u0026#34;]) output, _ := client.DescribeLogGroups(\u0026amp;cloudwatchlogs.DescribeLogGroupsInput{ LogGroupNamePrefix: \u0026amp;logGroupName, }) if len(output.LogGroups) \u0026lt; 1 { return fmt.Errorf(\u0026#34;Expected at least one log group. Found %d log groups\u0026#34;, len(output.LogGroups)) } return nil } func (o *godogFeaturesScenario) destroyTerraform(sc *godog.Scenario, err error) { terraform.Destroy(o.testing, o.terraformOptions) } Now we are ready to run the test. You should see the below output indicating that 1 scenario was executed with 3 succesful steps.\n1 2 3 4 5 1 scenarios (1 passed) 3 steps (3 passed) 48.024014744s Randomized with seed: 1614398736564936647 You can get the complete code for this scenario here\nTips for testing with terratest Testing in random folder Terraform init and apply steps leaves behind a bunch of artifacts like state file and .terraform directory, even after performing a terraform destory. Sometimes it is a mild inconvenience, sometimes it can causes artifacts from past test runs leak into the current run.\nTo avoid this scenario, terratest can copy the terraform files to a random temp directory and execute the test cases from there. This ensures that each run of terratest test cases are independent of each other.\n1 2 3 4 5 exampleFolder := test_structure.CopyTerraformFolderToTemp(t, \u0026#34;../\u0026#34;, \u0026#34;temp_terraform_test_dir\u0026#34;) terraformOptions := terraform.WithDefaultRetryableErrors(t, \u0026amp;terraform.Options{ TerraformDir: exampleFolder, }) WARNING: If terratest fails abruptly during execution, either through uncaught exceptions or through errors lower down in the stack (os, network etc.,) this can leave behind resources. Executing test cases in random directory makes it trickier to hunt down these orphaned resourrces and clean them up.\nTesting in random regions Sometimes it is better to run your test cases in random AWS regions to ensure that the test scenarios doesn\u0026rsquo;t make any unknown assumptions about the pre-existing resources.\n1 2 3 4 5 6 7 awsRegion := aws.GetRandomStableRegion(t, nil, nil) terraformOptions := terraform.WithDefaultRetryableErrors(t, \u0026amp;terraform.Options{ EnvVars: map[string]string{ \u0026#34;AWS_DEFAULT_REGION\u0026#34;: awsRegion, }, }) ","date":"2021-02-02T12:00:00Z","image":"https://raw.githubusercontent.com/cucumber/godog/master/logo.png","permalink":"https://example.com/p/2021-02-02-testing-terraform-code-using-terratest/","title":"Writing BDD tests for Terraform Code Using Terratest"},{"content":"Last year OpenAI released the Generative Pre-trained Transformer 2 (GPT-2) model. GPT-2 was a language model with 1.5 billion parameters, trained on 8 million web pages. It generated quite a buzz as it could generate coherent text, comprehend paragraphs, answer questions, and summarize text and do all sorts of smart stuff\u0026hellip; all without any task-specific learning. OpenAI even deemed the model too dangerous to release but eventually ended up releasing them.\nIn May 2020, OpenAI released their follow-up GPT-3 model which took the game several notches higher. They trained it with 175 billion parameters, using close to half-a-trillion tokens. The model and its weights alone would take up 300GB VRAM. This is a drastic increase in scale and complexity, anyway you look at it. So what can a huge model like this achieve and why has it reinvigorated the talks ?\nGPT-3 can write poetry, mimic the writing style of personalities. It also performs better than an average college applicant in SAT analogy problems, generates cohesive stories, writes news articles that are hard to distinguish from a human-written article. You can even ask it to do system admin jobs in natural language and it will come up with shell commands and execute them- like a seasoned sysadmin.\nLearning how to learn One of the amazing aspects of the GPT-3 model is that it doesn\u0026rsquo;t need any task-specific fine-tuning and yet achieves decent results on many of the Natural Language Processing (NLP) benchmarks and tasks. OpenAI\u0026rsquo;s claim is that if an NLP model is sufficiently complex and if it has been trained on a large volume of data, it can learn to do new tasks only by looking at few examples prompts i.e, the model has learned the ability to learn new tasks on the go. They call it \u0026ldquo;Few Shot Learning\u0026rdquo;. You can see that in action in the chart below.\nIn the above chart, you can see when the GPT-3 175B model is given with few examples, the accuracy rate shoots up compared to a poor learner (1.3B or even the 13B parameter model). The ability to learn is one of the defining characteristics of Artificial General Intelligence (AGI) and now you can understand why GPT-3 is generating the buzz.\nIs Language enough to model reality? GPT-3 has no direct exposure to reality, except via a large corpus of text. Yet the knowledge imparted from this large volume of text allows GPT-3 to reason (arguably) about the physical world as seen in the example below\nIn fact, GPT-3 also demonstrates surprising ability outside the traditional NLP domain. It can do certain non-linguistic tasks like simple arithmetics. This begs the question, whether modeling language alone is sufficient to form an understanding of reality? We use language to describe our world. So a relation between words should correspond to a relation between the real world entities represented by those words. Some reason that ML models are transitive (i.e, if X models Y and Y models Z, then X models Z) and since language itself is an approximate model of the world, a language model should be an approximate representation of the world.\nThe upper-bound of the effectiveness of scale Another surprising outcome of GPT-3 is that the performance has not tapered as the model got more complex. When GPT-2 got released most people speculated that we will soon hit the upper bound on the returns from further scaling up the model. But that hasn\u0026rsquo;t happened with GPT-3. With a more complex model, GPT-3 has shown big improvements in its performance and seems to get better at learning new tasks (see the widening gap between zero-shot and few-shot benchmarks).\nAs recently as last year, training a model with 175B seemed far off into the future. The Human brain is estimated to contain around 100 trillion neurons, how long before we reach that figure and what would AI look like as it approaches the figure?\nNow I agree that I am playing to the gallery here and being sensational. Biological neurons and ML neurons are quite different. A CNN model can do object detection (in fact better than us) but it doesn\u0026rsquo;t share the same structure with our visual cortex. This is why CNN outperforms humans in certain tasks like detecting anomalies in medical images, yet they can get confused between a ball and a referee\u0026rsquo;s head.\nNot everyone buys the scaling hypothesis. Detractors like Marcus claim that just because we know how to stack ladders doesn\u0026rsquo;t mean that we can build ladders to the moon. The argument is loaded with an assumption. In the case of ladders to the moon, we know the governing rules and the limits of possibility. But to extend the metaphor to AGI, we don\u0026rsquo;t understand our own intelligence well enough and we cannot say for sure that the chasm between a statistical model and causal reasoning is impervious. Like the expression Turtles all the way down, perhaps our intelligence is also made of statistical models stacked all the way down.\nWhat do the critics say? It is not as good as SOTA To be correct, GPT-3 is not the best model out there. On most NLP tasks \u0026amp; benchmarks, other State-of-the-art (SOTA) algorithms perform better than GPT-3. But those criticisms are missing the point.\nMost of the SOTA algorithms are fine-tuned for specific tasks whereas GPT-3 is not. But GPT-3 has still learned to do these tasks by understanding (in a specific sense) the English language, somewhat similar to how humans learn language tasks. And this, not the benchmark performance, is the most talked-about aspect of GPT-3. SOTA also performs better than humans on many specific tasks like object recognition, mastering games like Chess and Go. But that doesn\u0026rsquo;t weigh against the ability of our general intelligence. General intelligence, in a sense, always has to underfit to have wider applicability. One should also note that, in spite of being a general-purpose model, GPT-3 betters fine-tuned SOTA in some benchmarks (PhysicalQA, LAMBADA, Penn Tree Bank to name a few)\nIt is just an autocomplete Another common criticism is that GPT-3 is just a sophisticated text prediction engine. It doesn\u0026rsquo;t understand what those words mean. But this is a hasty criticism based on an intuitive definition understanding. Our intuitions about intelligence, understanding, and meaning are not precise enough for such claims.\nLet us say our brain is a sophisticated lookup dictionary populated by past experiences and culturally accumulated knowledge. Suppose a non-human entity possesses a similar dictionary but populated using a different process (by training on text corpus for example). Can it claim to have an understanding? This was the crux of the argument laid down by Searle in his Chinese room thought experiment.\nSearle claims that looking up a dictionary doesn\u0026rsquo;t represent understanding, but not everyone agrees with that take. On a similar note, if we dismiss GPT-3\u0026rsquo;s output as a mere statistical calculation, there is no telling that our brain\u0026rsquo;s neuro-biological process is any different.\nIt has already seen the data it is predicting on This is a more serious allegation than the ones above. Since GPT-3 was trained on a huge volume of data, there is a chance that it has already seen the inputs and it is simply recalling them from memory. For example, Yannic Kilcher in his video suspects that it can do arithmetic predictions because the model is likely to have seen the same data in the training dataset. OpenAI team claims to have done sufficient deduplication to remove the testing dataset from the training data. But given the volume of data, their deduplication is only optimistic.\nBut there is evidence that transformer models can indeed generate answers that it has not seen before. Karpathy trained a mini GPT model exclusively on synthetic data and it was already able to do 2 digit arithmetic. In this case, there is a clear separation between the training set and the validation set. So to conclude, the results we see from GPT-3 is not impossible and need not necessarily come out of a training data set corruption.\nConclusion All these comparisons with AGI don\u0026rsquo;t mean we should haphazardly claim that GPT-3 is an AGI or that it definitively proves that AGI is possible. However, there are few interesting observations from the results of GPT-3:\nIt shows that NLP models can be a shortcut to AGI as they have the ability to indirectly model the physical world. The performance of the GPT model continues to scale with the model size and hasn\u0026rsquo;t tapered off yet. So there is the promise that bigger models can be more intelligent. At higher model complexities, interesting behaviors seem to emerge, such as the ability to learn new tasks, perform non-linguistic tasks, etc., The year 2018 has been called the ImageNet moment for NLP and we can see why. The amount of progress made in NLP during the last couple of years is staggering and there is continued interest and investments in the domain. And mastering the NLP domain is closely linked with mastering General Intelligence.\nThere are many who believe that intelligence is somewhat innate to biological processes and cannot be reduced to mathematical models. There are others who believe that it is possible. In a way, this is just another incarnation of the nature vs nurture debate raging in the philosophical world for centuries. Except for this time, we are not relying on thought experiments. We get to do real experiments and see the answers in reality. Whichever side of the fence one sits on, the world waits with bated breath as the story unfolds.\n","date":"2020-09-20T12:00:00Z","image":"https://example.com/images/2020-09-26-meta-learning.png","permalink":"https://example.com/p/2020-09-20-gpt-3-and-prospects-of-artificial-general-intelligence/","title":"GPT-3 and prospects of Artificial General Intelligence"},{"content":" Give me a dozen healthy infants, well formed, and my own specified world to bring them up in and I’ll guarantee to take any one at random and train him to become any type of specialist I might select—doctor, lawyer, artist, merchant-chief and yes, even beggar-man thief, regardless of his talents, penchants, tendencies, abilities, vocations, and race of his ancestors.\nThis was John Watson, one of the founders of Behaviorism, writing around 1925. He believed that human behavior is completely malleable and that it can be shaped into anything given the right environment. While I don\u0026rsquo;t harbor any grand objectives or sinister experiments like Watson did, I do hope to be able to teach my kids good habits using controlled environments. For instance, my two year old kids started developing the habit of getting too close to the TV. I didn\u0026rsquo;t want to use force or impose restrictions on them, so I thought I could use technology to discourage them from getting too close to TV.\nProblem statement I play YouTube rhymes on my HTPC which is connected to our living room TV. While watching they sometimes get down from the couch and walk to the TV. When they do, I want to turn off the video automatically to let them know that they have gone too close and not resume the playback until they get back to the couch. For my setup, that means pausing the youtube player and minimzing the browser.\nFortunately this is easy to do with a camera and OpenCV. Let us see how.\nTools and solutions Object detection is among the fundamental features of OpenCV and there are a few ways to do it. I explored three of them:\nUsing HAAR Cascades: This is a popular object detection method in OpenCV. It uses models that are trained on a lot of positive and negative images and extracting features from those images. The training step is optional here as there are lot of pre-trained models available to detect face and human bodies. However for my case, those models didn\u0026rsquo;t do well due to the awkward angle at which my camera was placed. Training my own model based on an image corpus is also not an option since asking two year olds to hold postures for model training is a non-trivial endeavour.\nBackground Subtraction: Among all the methods this is the one which impressed me! All we need is a single call to opencv2.createBackgroundSubtractorMOG() to identify moving objects in the foreground and to eliminate static background. The method works by subtracting pixels from subsequent frames of a video and extracting those that change. However this method left a relatively larger footprint on the CPU (still not significant). I was also put off by the false shadow objects created under changing light conditions. But the biggest problem for me was that initial state of the frame was out of my control, so a person moving out of the frame would trigger object detection in the same way as a new person moving in. Asking my family to stay still as the computer boots up is a tricky request\u0026hellip; even by my standards.\nContour detection: This is among the cheaper method in terms of computational requirements and it was just the method I needed for my simple requirement. It works by identifying contours in a picture. If the picture is black and white, the contour detection works better. Fortunately, since I only needed to detect a head in the visual frame and the hair in the head helped to distinguish the contours clearly. So this seemed to be the best fit for my requirement and I went with it.\nUsing Contour detection With the default OpenCV video capture drivers, my camera returned a frame with resolution 640 x 480 pixels. Here you can see my son lost in the colorful world of baby rhymes, hoping to dive into the TV to enjoy the rhymes in full splendor.\nFirst I cropped the image to focus only on the area that I want to monitor- a small rectange close to TV. Blurring was done to smoothen the contours.\n1 2 3 4 cropped = frame[100:300, 0:630] # Blur the image blurred = cv2.blur(cropped, (3, 3)) Next I needed to convert color channels. Contour detection is not efficient when the color intensities are in 3 dimensions (RGB). The job is greatly simplified if we can restict the colors to a single dimension. And it works even better with binary images where the pixels are either pure black or white (even for us black and white are easier to tell apart than a million shades of other colors in between).\nSo we apply a threshold using cv2.inRange function which converts the image to black and white (not grey scale). This is done by specifying a range of interesting colors and every pixel that fall in the range will be converted to white and rest to black.\n1 2 3 4 5 6 7 8 9 # Convert to HSV color space hsv = cv2.cvtColor(blurred, cv2.COLOR_BGR2HSV) # define range of blue color in HSV lower_black = np.array([0,0,0]) upper_black = np.array([180, 255, 50]) # Create a binary image with where the head will be white while the rest is black thresh = cv2.inRange(hsv, lower_black, upper_black) Finally we turn to the cv2.findContours method. The cv2.findContours method returns an array of contours. I looked for the largest contour by area and triggered helper methods when the area was bigger than a threshold. We can even plot the contours to see how our object detection works, as I have done in the picture below.\n1 2 3 4 5 6 7 _, contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE) if contours: area = max([cv2.contourArea(c) for c in contours]) if area \u0026gt; 1500: cv2.drawContours(cropped, contours, -1 , (0,255,0), 3) # Object detected. Do something cv2.drawContours(cropped, contours, -1, (0,255,0), 3) And there you go! The naughty infiltrator is caught.\nControlling the playback For controlling the playback, I used a combination of pyautogui and win32con libraries. Since I didn\u0026rsquo;t want to interrupt playbacks on normal times, the object detection is activated only when Youtube is one among the foreground windows.\nNow without much adieu, here is the full code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 #!C:\\Python import time import numpy as np from win32con import SW_RESTORE, SW_SHOW import cv2 import pyautogui import win32gui TRIGGER_THRESHOLD = 30 COOLDOWN_THRESHOLD = 5 def get_windows_placement(window_id): return win32gui.GetWindowPlacement(window_id)[1] def set_active_window(window_id): if get_windows_placement(window_id) == 2: win32gui.ShowWindow(window_id, SW_RESTORE) else: win32gui.ShowWindow(window_id, SW_SHOW) win32gui.SetForegroundWindow(window_id) win32gui.SetActiveWindow(window_id) def search_and_restore_youtube(window_id, hwnd): if \u0026#34;YouTube\u0026#34; in win32gui.GetWindowText(window_id): set_active_window(window_id) def check_for_youtube(window_id, hwnd): if \u0026#34;YouTube\u0026#34; in win32gui.GetWindowText(window_id): hwnd.append(window_id) return True class FrameCounter(): def __init__(self): self.counter = 0 self.minimized = False self.state = \u0026#34;nf_max\u0026#34; def __str__(self): return(\u0026#34;Current count: {}\u0026#34;, self.counter) def found(self): if self.state == \u0026#34;nf_max\u0026#34;: self.counter += 1 self.state = \u0026#34;f_max\u0026#34; elif self.state == \u0026#34;f_max\u0026#34;: if self.counter \u0026gt; TRIGGER_THRESHOLD: self.minimize_all_windows() self.state = \u0026#34;f_min\u0026#34; else: self.counter += 1 else: next def not_found(self): if self.state == \u0026#34;f_min\u0026#34;: self.counter -= 1 self.state = \u0026#34;nf_min\u0026#34; elif (self.state == \u0026#34;nf_min\u0026#34;): if self.counter \u0026lt; (TRIGGER_THRESHOLD - COOLDOWN_THRESHOLD): self.restore_firefox() self.state = \u0026#34;nf_max\u0026#34; self.counter = 0 #forget and forgive else: self.counter -= 1 elif (self.state ==\u0026#34;f_max\u0026#34;): self.counter = max(0, self.counter - 1) else: next def minimize_all_windows(self): print(\u0026#34;Minimizing firefox at {0}\u0026#34;.format(self.counter)) pyautogui.hotkey(\u0026#39;k\u0026#39;) pyautogui.hotkey(\u0026#39;win\u0026#39;, \u0026#39;d\u0026#39;) self.minimized = True def restore_firefox(self): print(\u0026#34;Restoring firefox at {0}\u0026#34;.format(self.counter)) win32gui.EnumWindows(search_and_restore_youtube, None) pyautogui.hotkey(\u0026#39;k\u0026#39;) self.minimized = False def find_kids(frame, frame_counter): cropped = frame[100:300, 0:630] # Blur the image blur = cv2.blur(cropped, (3, 3)) # Convert to HSV color space hsv = cv2.cvtColor(blur, cv2.COLOR_BGR2HSV) # define range of blue color in HSV lower_black = np.array([0,0,0]) upper_black = np.array([180, 255, 50]) # Create a binary image with where the head will be white while the rest is black thresh = cv2.inRange(hsv, lower_black, upper_black) _, contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE) if contours: area = max([cv2.contourArea(c) for c in contours]) if area \u0026gt; 1500: cv2.drawContours(cropped, contours, -1 , (0,255,0), 3) frame_counter.found() else: frame_counter.not_found() else: frame_counter.not_found() cv2.imshow(\u0026#39;filtered\u0026#39;,cropped) if __name__ == \u0026#39;__main__\u0026#39;: cap = cv2.VideoCapture(0) cap.set(cv2.CAP_PROP_POS_MSEC,1000) frame_counter = FrameCounter() hwnd=[] while(cap.isOpened()): # Run only when Youtube is one of the running window titles win32gui.EnumWindows(check_for_youtube, hwnd) if (hwnd): ret, frame = cap.read() find_kids(frame,frame_counter) k = cv2.waitKey(500) \u0026amp; 0xff if k == 27: break cap.release() cv2.destroyAllWindows() Results Overall, I would say that the whole exercise was quite effective. The kids have learnt stay away from TV- well, most of the times. Sometimes they do succumb to the temptation and get closer. And when the playback stops, they move away from the TV and wait for the playback to automatically resume. It seems they have figured out their environment on their own.\nBut the kids cognition process is not without its faults, there were some unexpected results. For instance, my son developed a superstition that everytime the playback stops he needs to run all the way to kitchen for it to resume (exactly the time taken for the resume timer to cooldown) and so far he hasn\u0026rsquo;t observed any counter-examples to break the correlation between trip to kitchen and resumption of youtube videos. But I expect that over the next few days he will refine his model of environment. My daughter has developed no such belief, she climbs back in to the couch and patiently waits for the playback to resume.\nNow, I think I have solved my initial problem satisfactorily using technology, further strengthening my conviction that when used properly, technology can be a solution and not a problem. Now all I need to do is find some technical solutions to get rid of those earworm rhymes that I keep awkwardly humming in public.\n","date":"2019-03-02T12:00:00Z","permalink":"https://example.com/p/2019-03-02-using-opencv-object-detection-to-keep-kids-away-from-tv/","title":"Using OpenCV object detection to keep kids away from TV"},{"content":"In 2nd Century AD, Hellenic Cartographer Ptolemy was beset with an arbitrary choice of whether his maps should have north on the top or any other direction. Based in Alexandria, he reasoned that all population centers and places of importance lie to the north and would be convenient for study if they were in the upper right corner of the map. This arbitrary choice had long, unintended repercussions for mankind such as Australia being considered \u0026ldquo;Down under\u0026rdquo; or even our solar system to be perceived as rotating in counter-clockwise direction. Who would have thought that the stroke of a cartographer carried celestial importance!\nSimilarly, I am always fascinated about the distribution of population masses and economic activities acrosss the globe and what sort of external signals gets influenced by this distribution. Flight routes in major airports would be one such signal. For instance, closer you get to north pole , most of your businesses will be facing south and vice versa. Or across the Atlantic, it would be interesting to note whether east-west trans-atlantic traffic volume dominates over domestic air traffic in America. I tried to visualize this flight path orientation across important airports in the globe. This post is partly inspired by another beautiful piece of visualization of street orientation of major cities across the world by Goef Boeing.\nThe Data and tools The source data is from openflights.org. It contains over 67,000 routes between 3321 airports across the globe. It is not the most recent (their routes data source stopped providing updates since 2014), however it is enough to satiate our curiousity. For pre-processing, I chose to use Spark which is an overkill for such a small dataset. Spark is normally used for Big Data workloads where the data is too big to fit inside a single host. I thought this example would be a nice segue to using Spark for data transformations. Pandas and numpy were also used to bin the data before visualizing them on matplotlib.\n1 2 wget -O /tmp/airports.dat https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat wget -O /tmp/routes.dat https://raw.githubusercontent.com/jpatokal/openflights/master/data/routes.dat Pre-processing data in Spark Spark has two fundamental sets of APIs: the low-level unstructured RDD APIs, and the higher-level structured APIs which includes DataFrames and Spark SQL. Unlike the basic Spark RDD API, the structured APIs provide Spark with more information about the structure of the data and transformations. This allows it to take advantage of Spark\u0026rsquo;s optimizers such as Catalyst and Tungsten. For this reason, it is better to use the modern DataFrames API.\nThe Spark DataFrame API provides a convenient read function which can parse csv files and infer the schema on its own. However the source data doesn\u0026rsquo;t carry the header row and it is too tedious to rename columns once the dataframe has been created. It is far simpler to insert the header row before letting spark infer the schema.\n1 2 sed -i \u0026#39;1s/^/airport_id,name,city,country,iata,icao,latitude,longitude,altitude,tz,dst,tz_olson,airport_type,data_source\\n/\u0026#39; /tmp/airports.dat sed -i \u0026#39;1s/^/airline,airline_id,src_airport,src_airport_id,dst_airport,dst_airport_id,codeshare,stops,equipment\\n/\u0026#39; /tmp/routes.dat 1 2 3 4 5 6 7 8 9 10 11 12 table_names = [\u0026#34;routes\u0026#34;,\u0026#34;airports\u0026#34;] for table_name in table_names: df = spark.read.load(\u0026#34;file:///tmp/\u0026#34; + table_name +\u0026#34;.dat\u0026#34;, format=\u0026#34;csv\u0026#34;, inferSchema=\u0026#34;true\u0026#34;, header=\u0026#34;true\u0026#34;) df.write.format(\u0026#34;parquet\u0026#34;).saveAsTable(table_name) # Ignore broken rows and consider only direct flights route_df = spark.read.table(\u0026#39;routes\u0026#39;)\\ .filter(col(\u0026#39;stops\u0026#39;) == 0)\\ .filter(col(\u0026#39;dst_airport_id\u0026#39;) != \u0026#39;\\\\N\u0026#39;)\\ .filter(col(\u0026#39;src_airport_id\u0026#39;) != \u0026#39;\\\\N\u0026#39;) The next step to join the routes and airports table is seemingly trivial. However when we join airport table twice in Spark (once each for source airport and destination airport), the query planner gets confused on the join conditions (See SPARK-25150). To mitigate this, we create separate copies of dataframes for src_airport and dst_airport (although just one additional copy is sufficient to sort out the ambiguity).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 src_airport_df = airport_df.select( \u0026#39;airport_id\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;country\u0026#39;, \u0026#39;latitude\u0026#39;, \u0026#39;longitude\u0026#39; ).toDF( \u0026#39;src_airport_id\u0026#39;, \u0026#39;src_airport_name\u0026#39;, \u0026#39;src_city\u0026#39;, \u0026#39;src_cntry\u0026#39;, \u0026#39;src_lat\u0026#39;, \u0026#39;src_lon\u0026#39;) dst_airport_df = airport_df.select( \u0026#39;airport_id\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;country\u0026#39;, \u0026#39;latitude\u0026#39;, \u0026#39;longitude\u0026#39; ).toDF( \u0026#39;dst_airport_id\u0026#39;, \u0026#39;dst_city\u0026#39;, \u0026#39;dst_cntry\u0026#39;, \u0026#39;dst_lat\u0026#39;, \u0026#39;dst_lon\u0026#39;) src_cond = [route_df.src_airport_id == src_airport_df.src_airport_id] dst_cond = [route_df.dst_airport_id == dst_airport_df.dst_airport_id] routes_airport_df = route_df.join(src_airport_df, src_cond, how=\u0026#39;left\u0026#39;)\\ .join(dst_airport_df, dst_cond, how=\u0026#39;left\u0026#39;)\\ .select(\u0026#39;airline\u0026#39;,\u0026#39;src_airport\u0026#39;,\u0026#39;src_airport_name\u0026#39;,\u0026#39;dst_airport\u0026#39;,\u0026#39;src_city\u0026#39;,\u0026#39;src_cntry\u0026#39;, \u0026#39;src_lon\u0026#39;, \u0026#39;src_lat\u0026#39;, \u0026#39;dst_city\u0026#39;,\u0026#39;dst_cntry\u0026#39;,\u0026#39;dst_lon\u0026#39;, \u0026#39;dst_lat\u0026#39;) result_df = routes_airport_df\\ .withColumn(\u0026#34;delta_lon\u0026#34;, routes_airport_df.dst_lon - routes_airport_df.src_lon - 360*(routes_airport_df.dst_lon - routes_airport_df.src_lon \u0026gt; 180).cast(\u0026#34;integer\u0026#34;) + 360*(routes_airport_df.src_lon - routes_airport_df.dst_lon \u0026gt; 180).cast(\u0026#34;integer\u0026#34;))\\ .withColumn(\u0026#34;delta_lat\u0026#34;,routes_airport_df.dst_lat - routes_airport_df.src_lat)\\ .select(\u0026#39;airline\u0026#39;,\u0026#39;src_airport\u0026#39;,\u0026#39;src_airport_name\u0026#39;,\u0026#39;dst_airport\u0026#39;,\u0026#39;src_city\u0026#39;,\u0026#39;src_cntry\u0026#39;,\u0026#39;dst_city\u0026#39;,\u0026#39;dst_cntry\u0026#39;,\u0026#39;src_lon\u0026#39;,\u0026#39;dst_lon\u0026#39;,\u0026#39;src_lat\u0026#39;,\u0026#39;dst_lat\u0026#39;,\u0026#39;delta_lon\u0026#39;, \u0026#39;delta_lat\u0026#39;) The additional arithmetic juggling in calculating the longitudinal distance is to ensure that we take the shortest distance between the points. If two points are more than 180 degrees apart, we can get the shorter path by adding or removing a full circle (360 degrees).\nNote that for these calculations, we treat geographic co-ordinates as cartesian co-ordinates in Euclidean plane, with longitudes representing x-ticks and latitudes representing y-ticks- all of them equally spaced. This may not accurately represent the ground reality as earth\u0026rsquo;s surface is almost spherical and longitudes converges as we go towards the poles. The net effect is that the map gets stretched out at the poles. However this kind of representation(called Mercator projection) is not too uncommon and we will proceed to use it.\nNext we calculate the relative bearing between the source and destination. In navigation, bearing is the direction to a destination point from a source point. We treated our geo co-ordinates as cartesian co-ordinates and x-axis is represented by the equator. So a bearing of zero would indicate east-bound direction and the angle grows counter-clockwise from east to west. Since we assumed Euclidean plane from the beginning, we can use atan2 function from popular computing to calculate the angle. Fortunately, spark also has a native implementation of atan2 function, or else we migt have had to create a UDF (User Defined Function).\nThe angle calculated using atan2 still doesn\u0026rsquo;t give the exact bearing between two points on earth\u0026rsquo;s surface. In fact, the actual bearing along a geodesic continuously changes further apart we are from the equator. Since we are visualizing earth\u0026rsquo;s surface as flat map, this difference is acceptable.\nVisualizing Visualization happens in the python process which means the data has to be succesfully transferred from the JVM universe of Spark to Python. In real world workcases, when we output to python we have to make sure that the transformed data from Spark can fit inside the memory of the Spark driver process. Since we aren\u0026rsquo;t encumbered by any such problems we can safely move the data to Python process (a pandas Dataframe). If the memory is really a bottleneck, we have to move some of the steps upstream (such as histogram binning) to spark UDF\u0026rsquo;s so that Pandas only works with aggregates.\n1 2 3 4 import pandas as pd import numpy as np import matplotlib.pyplot as plt pdDF = df_with_angle.toPandas() For polar histogram, I have adopted the technique used by Jeff Boeing. Counting histogram bins based on linear axis can be misleading since 359° and 1° are treated as extremes whereas in polar co-ordinate they represent adjacent points. The solution is to create twice the number of bins desired and sum them pairwise, with bins around edges ([-175,0] and [0,175]) summed together.\n1 2 3 4 5 6 7 8 9 10 def count_and_merge(n, bearings): ## make twice as many bins as desired, then merge them in pairs ## prevents bin-edge effects around common values like 0° and 90° bins = np.arange((-1)*n, n + 1) * 180 / n count, angles = np.histogram(bearings, bins=bins) ## move the last bin to the front, so eg 0.01° and 359.99° will be binned together count = np.roll(count, 1) return count[::2] + count[1::2], np.roll(angles[1::2],1) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def polar_plot(ax, bearings, n=36, title=\u0026#39;\u0026#39;): bins = np.arange((-1)*n, n + 1) * 180 / n radii, division = count_and_merge(n, bearings) theta = np.deg2rad(division) width = (2*np.pi) / n ax.set_theta_zero_location(\u0026#34;E\u0026#34;) title_font = {\u0026#39;family\u0026#39;:\u0026#39;Corbel\u0026#39;, \u0026#39;size\u0026#39;:24, \u0026#39;weight\u0026#39;:\u0026#39;bold\u0026#39;} xtick_font = {\u0026#39;family\u0026#39;:\u0026#39;Corbel\u0026#39;, \u0026#39;size\u0026#39;:10, \u0026#39;weight\u0026#39;:\u0026#39;bold\u0026#39;, \u0026#39;alpha\u0026#39;:1.0, \u0026#39;zorder\u0026#39;:3} ax.set_title(title.upper(), y=1.05, fontdict=title_font) ax.set_yticks(np.linspace(0, max(ax.get_ylim()), 5)) yticklabels = [\u0026#39;{:.2f}\u0026#39;.format(y) for y in ax.get_yticks()] yticklabels[0] = \u0026#39;\u0026#39; ax.set_yticklabels(labels=yticklabels, fontdict=ytick_font) xticklabels = [\u0026#39;E\u0026#39;, \u0026#39;\u0026#39;, \u0026#39;N\u0026#39;, \u0026#39;\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;\u0026#39;, \u0026#39;S\u0026#39;, \u0026#39;\u0026#39;] ax.set_xticklabels(labels=xticklabels, fontdict=xtick_font) ax.tick_params(axis=\u0026#39;x\u0026#39;, which=\u0026#39;major\u0026#39;, pad=-2) bars = ax.bar(theta, height=radii, width=width, align=\u0026#39;center\u0026#39;, bottom=0, zorder=2, color=\u0026#39;#003366\u0026#39;, edgecolor=\u0026#39;k\u0026#39;, linewidth=0.5, alpha=0.7) Conclusion Here is the resultant plot. The most obvious pattern that we can observe is that northernmost cities have most of their routes pointing towards south. This is expected because our model of earth is flat and hence it precludes the possibility of any polar routes between two points (which may be the actual path taken by an aircraft). Also note that the absence of pacific-bound routes (towards americas) from chinese airports or the relative dominance of south-eastern routes from London and north-eastern routes from Hong Kong. Doesn\u0026rsquo;t it tell something about the current politico-economic climate of the world? I am sure the shipping routes will tell a different story though.\n","date":"2019-01-05T12:00:00Z","permalink":"https://example.com/p/2019-01-05-using-spark-and-matplotlib-to-visualize-air-routes-for-major-airports-around-the-world/","title":"Visualizing air routes for major airports using Spark and Matplotlib"},{"content":"Security is a huge, complex, rapidly changing field. Advancements in infrastructue hosting, development methodologies has had the most impact on this domain. Thanks to automation, instances are spawned and deleted in a matter of second. Continuous development/Continuous Integration means that an average lifetime of a block of code is ever decreasing. Code review and vulnerability assessments based on static code and IP are hardly affordable at current rate of change. At the same time, the rate of proliferation of technology has seen comparable increase in risk vectors, vulnerabilities and attack methodologies. To keep up with this pace, automation in security operations has become more important than ever.\nUnfortunately even within the CyberSecurity field, this automation domain is quite nascent and has sparse literature. So I was quite excited when I got a chance to review \u0026ldquo;Security Automation with Ansible 2\u0026rdquo; by Akash Mahajan and Madhu Akula. Before reading any further, if you are a security engineer struggling with these problems, grab hold of this book. It is quite extensive in scope and examples.\nThe book starts with a brief introduction on Ansible, its installation procedures and works its way up to complex workflows, covering all major aspects of security automation. Some of the workflows discussed in the book includes- hardening various types of application deployments, continuous scanning for CICD workflow using Jenkins and OWASP ZAP, automated vulnerability scanning using Nessus, continuous security scans using OpenScap, vulnerability assessments of docker containers and cloud deployments. The book even goes on to discuss peripheral topics such as setup workflows for malware analysis, openstack, debops (Debian-based Data Center), private VPN using algo, anti-censorship software (Streisand) etc., In short you won\u0026rsquo;t find many tools missing in this book. Even if you are a practising expert on security automation, you will find something new to learn or inspired to use.\nAnsible is the engine for all the examples, acting as Swiss army knife to drive the automation of every aspects of security operations. Despite that, the authors didn\u0026rsquo;t discriminate against other automation tools and ignore them. For example, tools such Elasticsearch, AWS Lambda are already built for automation. Ansible\u0026rsquo;s role in these examples is to simplify the deployment of these tools and ensure consistency in deployment of these tools. One might argue this can encourage broader adoption of these tools on disparate workflows.\nIf I have any suggestions, I would love to see some of these examples distilled into abstract policies or best practices for DevSec automation. \u0026ldquo;The practice of System and Network Administration\u0026rdquo; by Limoncelli is the golden standard for practical IT related writing. In their own words, their book sets out to discuss \u0026ldquo;those principles and ideas of system administration which do not change on a day-to-day basis\u0026rdquo;. Too much focus on existing tools can potentially affect the longevity of the book. Admittedly this is somewhat harder to do for a field which is still nascent and unpredictable. Irrespective of this, the book is still a valuable resource and every CyberSecurity professional will find it useful. You can find a copy of the book in Amazon or Packt\n","date":"2018-05-20T12:00:00Z","permalink":"https://example.com/p/2018-05-20-book-review-security-automation-with-ansible-2/","title":"Book Review - Security Automation with Ansible 2"},{"content":"In the previous post we saw how to scrape raw data from a content rich webpage. In this post, we will explore how to process that raw data and use Machine Learning tools to predict the playing role of a cricket player just based on his career statistics.\nHere are the tools that we will use for this exercise. For interactive data analysis and number crunching:\nJupyter Pandas Numpy For visualizing data:\nSeaborn matplotlib For running Machine Learning models:\nTensorflow Keras Scikit-learn Importing data First let us load the necessary modules:\n1 2 3 4 import matplotlib.pyplot as plt import pandas as pd import numpy as np import seaborn as sns Import the CSV file which we scraped as a pandas data frame and inspect its contents.\n1 2 data = pd.read_csv(\u0026#39;data/players.csv\u0026#39;) data.dtypes \u0026mdash;\u0026ndash;|\u0026mdash;- Bat_100|object Bat_4s|object Bat_50|object Bat_6s|object Bat_Ave|object Bat_BF|object Bat_Ct|int64 Bat_HS|object Bat_Inns|object Bat_Mat|int64 Bat_NO|object Bat_Runs|object Bat_SR|object Bat_St|int64 Bio_Also_known_as|object Bio_Batting_style|object Bio_Born|object Bio_Bowling_style|object Bio_Current_age|object Bio_Died|object Bio_Education|object Bio_Fielding_position|object Bio_Full_name|object Bio_Height|object Bio_In_a_nutshell|object Bio_Major_teams|object Bio_Nickname|object Bio_Other|object Bio_Playing_role|object Bio_Relation|object Bowl_10|object Bowl_4w|object Bowl_5w|object Bowl_Ave|object Bowl_BBI|object Bowl_BBM|object Bowl_Balls|object Bowl_Econ|object Bowl_Inns|object Bowl_Mat|int64 Bowl_Runs|object Bowl_SR|object Bowl_Wkts|object dtype: object|\nWe can see that most of the fields are decoded as object data type which is a generic pandas datatype. It gets assigned if our data consists of mixed types such as characters and numerals. There are some obvious numerical fields which are getting detected as string. But before we recast all of them as string, we need to preprocess some of them to extract numeric value out of them.\nFor example, let us inspect Bowl_BBI and Bowl_BBM which stands for best bowling figures in an innings and a match respectively.\n1 data[[\u0026#39;Bowl_BBI\u0026#39;,\u0026#39;Bowl_BBM\u0026#39;]].head(n=10) Bowl_BBI Bowl_BBM 0 - 1 3/31 2 2/35 3 - 4 - 5 - 6 - 7 - 8 6/85 9 - Either fields can be made sense as a combination of two independent variables- Best Bowling Wickets \u0026amp; Best Bowling Runs. Similarly when we cast the field Bat_HS as integer, the notout values will be lost since they are suffixed with an asterisk which makes them a string data type. Let us go ahead to fix these potential issues.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Best bowling innings wickets bbi_df = pd.DataFrame(data[\u0026#39;Bowl_BBI\u0026#39;].str.replace(\u0026#39;-\u0026#39;,\u0026#39;\u0026#39;).str.split(\u0026#39;/\u0026#39;).tolist(), columns = [\u0026#39;Bowl_BBIW\u0026#39;,\u0026#39;Bowl_BBIR\u0026#39;]) bbm_df = pd.DataFrame(data[\u0026#39;Bowl_BBM\u0026#39;].str.replace(\u0026#39;-\u0026#39;,\u0026#39;\u0026#39;).str.split(\u0026#39;/\u0026#39;).tolist(), columns = [\u0026#39;Bowl_BBMW\u0026#39;,\u0026#39;Bowl_BBMR\u0026#39;]) data = data.join([bbi_df,bbm_df]) # Identify numeric columns numeric_cols = [\u0026#39;Bat_100\u0026#39;,\u0026#39;Bat_4s\u0026#39;,\u0026#39;Bat_50\u0026#39;,\u0026#39;Bat_6s\u0026#39;,\u0026#39;Bat_Ave\u0026#39;,\u0026#39;Bat_BF\u0026#39;, \u0026#39;Bat_Ct\u0026#39;,\u0026#39;Bat_HS\u0026#39;,\u0026#39;Bat_Inns\u0026#39;,\u0026#39;Bat_Mat\u0026#39;,\u0026#39;Bat_NO\u0026#39;,\u0026#39;Bat_Runs\u0026#39;, \u0026#39;Bat_SR\u0026#39;,\u0026#39;Bat_St\u0026#39;,\u0026#39;Bowl_10\u0026#39;,\u0026#39;Bowl_4w\u0026#39;,\u0026#39;Bowl_5w\u0026#39;,\u0026#39;Bowl_Ave\u0026#39;, \u0026#39;Bowl_Balls\u0026#39;,\u0026#39;Bowl_Econ\u0026#39;,\u0026#39;Bowl_Inns\u0026#39;,\u0026#39;Bowl_Mat\u0026#39;,\u0026#39;Bowl_Runs\u0026#39;, \u0026#39;Bowl_SR\u0026#39;, \u0026#39;Bowl_Wkts\u0026#39;,\u0026#39;Bowl_BBIW\u0026#39;,\u0026#39;Bowl_BBIR\u0026#39;,\u0026#39;Bowl_BBMW\u0026#39;,\u0026#39;Bowl_BBMR\u0026#39;] # regex replace * in High scores data[\u0026#39;Bat_HS\u0026#39;] = data[\u0026#39;Bat_HS\u0026#39;].replace(r\u0026#39;\\*$\u0026#39;,\u0026#39;\u0026#39;,regex=True) data[numeric_cols] = data[numeric_cols].replace(\u0026#39;-\u0026#39;,0) data[numeric_cols] = data[numeric_cols].apply(pd.to_numeric, errors=\u0026#39;coerce\u0026#39;) data[numeric_cols] = data[numeric_cols].fillna(0) If we check the data type again, we will see that all the numerical fields are interpreted as int or float datatype as expected.\nBe careful when filling NaN with zeroes. Idea is not to introduce false values in to the dataset. In this case, a value of zero is neutral since it represents the same value as absent numbers. But for certain types of data, such as temperature, zero introduces a false value in to the data set since temperature values can be less than zero.\nPre-processing Deriving new features When using data in our models we have to understand the units in which they are represented. Not all features are directly comparable. For instance, Average \u0026amp; Strike rates are already averaged over the number of matches that a player plays. But other aggregate statistics aren\u0026rsquo;t. So in effect it would be meaningless to compare run tally of a player who has played only 10 matches with that of another who has played a hundred matches.\nTo understand better, let us plot runs scored vs the matches played.\n1 sns.jointplot(x=\u0026#34;Bat_Runs\u0026#34;, y=\u0026#34;Bat_Inns\u0026#34;, data=data) Obviously there is a strong correlation between no. of matches played and no. of runs scored. Ideally we want our features to be as independent of each other as possible. To separate the influence of number of matches played on the batting runs feature, we will divide the aggregate statistics by number of matches played.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # select aggregate stats such as no. of hundreds, runs scored etc., bat_features_raw = [\u0026#39;Bat_100\u0026#39;, \u0026#39;Bat_4s\u0026#39;, \u0026#39;Bat_50\u0026#39;, \u0026#39;Bat_6s\u0026#39;, \u0026#39;Bat_BF\u0026#39;, \u0026#39;Bat_Ct\u0026#39;, \u0026#39;Bat_NO\u0026#39;, \u0026#39;Bat_Runs\u0026#39;,\u0026#39;Bat_St\u0026#39;] # column names for scaled features bat_features_scaled = [\u0026#39;Bat_100_sc\u0026#39;, \u0026#39;Bat_4s_sc\u0026#39;, \u0026#39;Bat_50_sc\u0026#39;, \u0026#39;Bat_6s_sc\u0026#39;, \u0026#39;Bat_BF_sc\u0026#39;, \u0026#39;Bat_Ct_sc\u0026#39;, \u0026#39;Bat_NO_sc\u0026#39;, \u0026#39;Bat_Runs_sc\u0026#39;,\u0026#39;Bat_St_sc\u0026#39;] # leave aside match and innings count and other aggregate stats such as best bowling figures, strike rate and average bowl_features_raw = [\u0026#39;Bowl_10\u0026#39;, \u0026#39;Bowl_4w\u0026#39;, \u0026#39;Bowl_5w\u0026#39;, \u0026#39;Bowl_Balls\u0026#39;, \u0026#39;Bowl_Runs\u0026#39;,\u0026#39;Bowl_Wkts\u0026#39;] # column names for scaled features bowl_features_scaled = [\u0026#39;Bowl_10_sc\u0026#39;, \u0026#39;Bowl_4w_sc\u0026#39;, \u0026#39;Bowl_5w_sc\u0026#39;, \u0026#39;Bowl_Balls_sc\u0026#39;, \u0026#39;Bowl_Runs_sc\u0026#39;,\u0026#39;Bowl_Wkts_sc\u0026#39;] # divide by innings count since it is more relevant than match count data[bat_features_scaled] = data[bat_features_raw].apply(lambda x: x/data[\u0026#39;Bat_Inns\u0026#39;]) data[bowl_features_scaled] = data[bowl_features_raw].apply(lambda x: x/data[\u0026#39;Bowl_Inns\u0026#39;]) # these are the meaningful features which will be the input for our model. features = [\u0026#39;Bat_Ave\u0026#39;,\u0026#39;Bat_HS\u0026#39;, \u0026#39;Bat_SR\u0026#39;] + bat_features_scaled + [\u0026#39;Bowl_Ave\u0026#39;,\u0026#39;Bowl_Econ\u0026#39;,\u0026#39;Bowl_SR\u0026#39;,\u0026#39;Bowl_BBIW\u0026#39;, \u0026#39;Bowl_BBIR\u0026#39;, \u0026#39;Bowl_BBMW\u0026#39;, \u0026#39;Bowl_BBMR\u0026#39;] + bowl_features_scaled # fill numerical features with zero data[features] = data[features].fillna(0) It can be argued that averaging the runs scored duplicates the batting average feature. Leaving aside subtle differences in the way in which batting averages are calculated, we would still keep both features to see how our model learns the difference in both the features and assigns weight accordingly.\nNow let us plot the scaled runs scored value vs the innings played.\n1 sns.jointplot(x=\u0026#34;Bat_Runs_sc\u0026#34;, y=\u0026#34;Bat_Inns\u0026#34;, data=data) Clearly this is a far better representation of batting capabilities of a player. You can see there is less dependency on the number of innings played. It is not hard to imagine how this scaling affects our final prediction. The impact is obvious when we plot batting runs and bowling wickets (likely to be the most important features) in a KDE plot. Here is the KDE plot before scaling:\n1 sns.jointplot(x=\u0026#34;Bowl_Wkts\u0026#34;, y=\u0026#34;Bat_Runs\u0026#34;, data=df,kind=\u0026#39;kde\u0026#39;) There is no clear clustering indicating that our classification is not going to be effective. In comparison, if we generate the same chart for scaled values, there is a clear grouping.\n1 sns.jointplot(x=\u0026#34;Bowl_Wkts_sc\u0026#34;, y=\u0026#34;Bat_Runs_sc\u0026#34;, data=df,kind=\u0026#39;kde\u0026#39;) This much more promising. Remember, your model will only perform as well as the data you feed in. If the input data is already confused, there is very little a mathematical model can do. Now that we have almost all that we need we will extract those records that have playing role information and use it for our training \u0026amp; testing. To avoid outliers corrupting our model, we will also exclude players who played less than 5 matches.\n1 2 # remove players who played less than 5 matches df = data[data[\u0026#39;Bio_Playing_role\u0026#39;].notnull() \u0026amp; (data[\u0026#39;Bat_Mat\u0026#39;] \u0026gt; 5)] Data Transformation Next let us look at our target feature which is playing role. We need to understand the values it can assume. Let us look at the unique values for the player features.\n1 2 # Check the unique playing roles to identify mapping function data[\u0026#39;Bio_Playing_role\u0026#39;].unique() 1 2 3 array([nan, \u0026#39;Top-order batsman\u0026#39;, \u0026#39;Bowler\u0026#39;, \u0026#39;Middle-order batsman\u0026#39;, \u0026#39;Wicketkeeper batsman\u0026#39;, \u0026#39;Allrounder\u0026#39;, \u0026#39;Batsman\u0026#39;, \u0026#39;Opening batsman\u0026#39;, \u0026#39;Wicketkeeper\u0026#39;, \u0026#39;Bowling allrounder\u0026#39;, \u0026#39;Batting allrounder\u0026#39;], dtype=object) The playing role definiton is too granular. We want fewer variety of roles so that each role gets sufficient sample data points to train the model. Also the role tagging done by Cricinfo is not consistent. For e.g., not all opening batsmen have been tagged with the opening batsman role. So we define a mapping function to group playing roles in to 4 different categories ['Batsman','Bowler','Wicketkeeper','Allrounder']\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def get_role(role): if pd.notnull(role): if \u0026#39;keeper\u0026#39; in role: return \u0026#34;Wicketkeeper\u0026#34; elif \u0026#39;rounder\u0026#39; in role: return \u0026#34;Allrounder\u0026#34; elif \u0026#39;atsman\u0026#39; in role: return \u0026#34;Batsman\u0026#34; elif \u0026#39;owler\u0026#39; in role: return \u0026#34;Bowler\u0026#34; else: return \u0026#34;\u0026#34; else: return \u0026#34;\u0026#34; data[\u0026#39;role\u0026#39;] = data[\u0026#39;Bio_Playing_role\u0026#39;].apply(get_role) Note that this feature is a categorical data. It is different from a numerical data such as height or weight. When we want to use Deep Neural Networks we need to represent the target features as numerical data. We will assign one column for each playing role and set its value to one when that playing role fits the player well. Then the function of our model will be to assign a value close to 1 for one of these columns and a value close to 0 for the rest.\nIt is called One-Hot encoding. Turns out this is a frequent task, so pandas has a handy inbuilt function to perform this.\n1 2 3 4 5 6 7 8 9 # y is categorical feature y = df[\u0026#39;role\u0026#39;] # Convert categorical data into numerical columns y_cat = pd.get_dummies(y) # X is the input features. We need to covert it from pandas dataframe to numpy array to feed in to our models X = df[features].as_matrix() Let us see the new y_cat dataframe\n1 y_cat.head() Allrounder Batsman Bowler Wicketkeeper 19 0 1 0 0 20 0 0 1 0 22 0 0 0 1 25 1 0 0 0 28 0 1 0 0 One might be tempted to assign unique numbers for each category ( say 1: Batsman, 2: Bowler etc.,) but that will not work. There is no quantitative relation between categories. Assigning raw numbers implies that there is a numerical progression to the categories. Sometimes it can work for contiguous data such as day of the month, but even then one has to be aware of the bounds and circularity of the target variables.\nScaling data Some fields vary over a larger range compared to the rest. Remember we did a preliminary scaling by dividing these values with the number of innings. But that is not sufficient since it only made sure that one feature (\u0026rsquo;no. of innings\u0026rsquo;) did not overtly influence another feature (\u0026lsquo;runs scored\u0026rsquo;). But each features themselves lie between different extremities. For e.g, Bowling Wickets Scaled only ranges from 0 to 5 whereas Batting Runs Scaled ranges from 0 to 50. Most machine learning models works the best when the features are vary within the same range. If we let these datapoints influence our calculation without modification, wickets taken will have negligible influence.\nSo we perform another round of scaling for all input data points. We will use the MinMax Scaler from Scikit library. This will scale the values such that largest value becomes one and smallest value becomes zero.\n1 2 3 4 5 from sklearn.preprocessing import MinMaxScaler mms = MinMaxScaler(feature_range=(0,1)).fit(X) # X_mms will our new input array with all values scaled to be between 0 and 1 X_mms = mms.transform(X) Training the model Deep Neural Network First we will try to run a Deep Neural Network model on this data. Here are the necessary modules to import.\n1 2 3 4 5 from keras.models import Sequential from keras.layers import Dense from keras.optimizers import SGD, Adam, Adadelta, RMSprop from keras.wrappers.scikit_learn import KerasClassifier import keras.backend as K Create the Keras Sequential model. I am using a DNN with 1 hidden layer and 1 output layer. The hidden layer has 15 nodes. The number of nodes in the output layer should as the number of categories. So we will go with 4.\n1 2 3 4 5 6 7 8 def create_baseline(): # create model model = Sequential() model.add(Dense(15, input_dim=25, kernel_initializer=\u0026#39;he_normal\u0026#39;, activation=\u0026#39;relu\u0026#39;)) model.add(Dense(4, kernel_initializer=\u0026#39;he_normal\u0026#39;, activation=\u0026#39;softmax\u0026#39;)) # Compile model model.compile(loss=\u0026#39;categorical_crossentropy\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) return model The softmax is a popular activation function for classification problems. In simple words, an activation function is a simple function that decides whether to output TRUE or FALSE for each category. This softmax function receives an array of values from the previous layer and returns a new array which adds up to 1. The category with the largest value is deemed likely match for our data.\nSoftmax highlights only the likeliest category for a data. Here for simplicity sake, we assume that our categories are mutually exclusive, i.e, a player can only belong to one category at a time. There are other data types where a single entry can belong to more than one category at a time. We may need to use different activation function for that. Again, know your data before deciding on activation function.\nThe loss function is categorical_crossentropy. A Loss Function can be thought of as a course correction function which measures the perceived error as our model navigates to ideal set of weights over multiple iterations. Categorical Cross-Entropy loss function penalizes weights that are sure to be wrong. It is a common loss function used for classification problems.\nThese are the important attributes that closely follows our problem definition. Most of the other parameters can be fiddled with.\nNext we will use Keras to train the model. The result is a Keras Classifier function whose weights are trained on our data. We can use this function to predict values for inputs which we haven\u0026rsquo;t seen so far.\n1 2 # evaluate model with standardized dataset estimator = KerasClassifier(build_fn=create_baseline, nb_epoch=100, batch_size=5, verbose=0) We cannot use all of the data to train our model. The model will closely follow our existing model. It won\u0026rsquo;t be useful to predict any values we haven\u0026rsquo;t seen so far. This is called overfitting.\nExample of overfitting - Source Wikipedia To avoid this, we will split the data into train and test datasets. We will use the former to train the model and compute the scores based on the testing against test data for each iteration of cross-validation. Scikit\u0026rsquo;s provides a helper function called cross_val_score to assist in this. StratifiedKFold is the genertor strategy we will use for selecting this train/test datasets. It splits the data into K folds (set to 10 in our case), trains it on K-1 datasets and tests it against the left out dataset, while preserving the class distribution of the data.\n1 2 3 4 # set the random state to a fixed number for reproducing the results kfold = StratifiedKFold(n_splits=10, shuffle=True,random_state=42) results = cross_val_score(estimator, X_mms, y.values, cv=kfold) print(\u0026#34;Results: %.2f%% (%.2f%%)\u0026#34; % (results.mean()*100, results.std()*100)) I got a result of 82.2% accuracy. Not bad for the first attempt, particularly since we employed gross simplifications and trained the model with only with around 450 records. The results that you get may be slightly different since we shuffle the data before generating folds.\nRandom Forest Classifier This time let us try to model the data using Random Forest classifier. Random Forest Classifier is a much simpler method than neural networks. It relies on building multiple decision trees and assembling the results of these decision trees.\n1 2 3 4 5 6 7 from sklearn.ensemble import RandomForestClassifier # Instantiate model with 1000 decision trees rf_estimator = RandomForestClassifier(n_estimators = 1000, random_state = 42) rf_results = cross_val_score(rf_estimator, X_mms, y.values, cv=kfold) print(\u0026#34;Results: %.2f%% (%.2f%%)\u0026#34; % (rf_results.mean()*100, rf_results.std()*100)) You will notice that Random Forest Classifier has performed significantly better than the DNN classifier. I got 87.28% accuracy, which is amazing since Random Forest is several times faster and less resource intensive than the DNN classifier. And I didn\u0026rsquo;t even have to run it on top of tensorflow and make use of GPU. Decision trees are quite effective at classification tasks but they tend to overfit.\nReviewing the results Since our Random Forest model has performed significantly better, we will use that model to predict the unseen roles of players.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Fit the estimator on available data rf_estimator.fit(X_mms, y.values) # np array to hold all of the input data P = data[features].as_matrix() # An ugly hack to drop infinity values introduced as part of some of the pre-processing tasks P[P \u0026gt; 1e308] = 0 # Min Max Scaling P_mms = min_max_scaler.fit_transform(P) # Prediction based on the Random forest model data[\u0026#39;predicted_role_rf\u0026#39;] = rf_estimator.predict(P_mms) Confusion Matrix A score alone is not a good indicator that our model has performed well. We need to review its performance by plotting Confusion Matrix. It is a simple matrix plot based on known test data with predicted values plotted against the true value. The diagonal entries represent correct prediction, rest represents confused values. Let us plot Confusin Matrix for our data.\n1 2 3 4 5 6 7 from sklearn.metrics import confusion_matrix mat = confusion_matrix(data[(data[\u0026#39;role\u0026#39;]!=\u0026#39;\u0026#39;)][\u0026#39;role\u0026#39;], data[(data[\u0026#39;role\u0026#39;]!=\u0026#39;\u0026#39;)][\u0026#39;predicted_role_rf\u0026#39;], labels = [\u0026#39;Batsman\u0026#39;,\u0026#39;Bowler\u0026#39;,\u0026#39;Allrounder\u0026#39;,\u0026#39;Wicketkeeper\u0026#39;]) sns.heatmap(mat.T, square=True, annot=True, fmt=\u0026#39;d\u0026#39;, cbar=False) plt.xlabel(\u0026#39;true label\u0026#39;) plt.ylabel(\u0026#39;predicted label\u0026#39;) We can see that the model is quite effective in matching the pure roles such as Batsman or Bowler. When it comes to mixed roles such as Allrounder or Wicketkeeper, it fares not that well. Part of the problem lies in our assumption that the roles are mutually exclusive i.e, a player cannot be both Batsman and Bowler at the same time. So we identify only around 37% of the all rounders succesfully. Later we will see that there are other reasons why the predicted role doesn\u0026rsquo;t match the role marked in cricinfo.\nReviewing the results Let us see the cases where our predictions differed from the roles defined in cricinfo:\n1 data[(data[\u0026#39;role\u0026#39;] != data[\u0026#39;predicted_role_rf\u0026#39;]) \u0026amp; (data[\u0026#39;role\u0026#39;] != \u0026#39;\u0026#39;) \u0026amp; (data[\u0026#39;Bat_Mat\u0026#39;] \u0026gt; 5 )][[\u0026#39;Bio_Full_name\u0026#39;,\u0026#39;predicted_role_rf\u0026#39;, \u0026#39;role\u0026#39;, \u0026#39;Bio_Playing_role\u0026#39;]] I have extracted the differences for popular players of recent times. Subjectively speaking our model hasn\u0026rsquo;t performed too bad. There seems to be some merit to the classification offered by the model compared to the playing role assigned in cricinfo bio page.\nBio_Full_name predicted_role_rf role Bio_Playing_role 38 Stephen Norman John O\u0026rsquo;Keefe Bowler Allrounder Allrounder 44 Glenn James Maxwell Batsman Allrounder Batting allrounder 88 Andrew Symonds Batsman Allrounder Allrounder 227 Shai Diego Hope Batsman Wicketkeeper Wicketkeeper batsman 281 Brendon Barrie McCullum Batsman Wicketkeeper Wicketkeeper batsman 970 Angelo Davis Mathews Batsman Allrounder Allrounder 1437 Abraham Benjamin de Villiers Batsman Wicketkeeper Wicketkeeper batsman In most cases, Cricinfo\u0026rsquo;s playing role is also based on the ODI and T20 formats. Some of the players like ABD Villiers and Brendon McCullum have donned multiple roles but given up gloves for the games longest format. So we can\u0026rsquo;t really fault the model here for identifying them as batsman. Then there are other cases of a player being regarded as All rounder based on the role they play in shorter formats.\nNext to the most interesting part- let us see how our model behaves for the data it hasn\u0026rsquo;t seen i.e., the classification of those players whose playing role is missing in their bio page. For ease of identification, I have filtered only those players who have played 100 matches or more.\n1 data[(data[\u0026#39;role\u0026#39;] != data[\u0026#39;predicted_role_rf\u0026#39;]) \u0026amp; (data[\u0026#39;role\u0026#39;] == \u0026#39;\u0026#39;) \u0026amp; (data[\u0026#39;Bat_Mat\u0026#39;] \u0026gt; 100 )][[\u0026#39;Bio_Full_name\u0026#39;,\u0026#39;predicted_role_rf\u0026#39;, \u0026#39;role\u0026#39;, \u0026#39;Bio_Playing_role\u0026#39;]] Bio_Full_name predicted_role_rf role Bio_Playing_role 134 Mark Edward Waugh Batsman NaN 137 Mark Anthony Taylor Batsman NaN 139 Ian Andrew Healy Wicketkeeper NaN 599 Sourav Chandidas Ganguly Batsman NaN 743 Anil Kumble Bowler NaN 925 Brian Charles Lara Batsman NaN 929 Carl Llewellyn Hooper Batsman NaN 937 Courtney Andrew Walsh Bowler NaN 957 Desmond Leo Haynes Batsman NaN 1072 Kapildev Ramlal Nikhanj Bowler NaN 1074 Dilip Balwant Vengsarkar Batsman NaN 1088 Sunil Manohar Gavaskar Batsman NaN 1257 Cuthbert Gordon Greenidge Batsman NaN 1258 Isaac Vivian Alexander Richards Batsman NaN 1284 Clive Hubert Lloyd Batsman NaN 1326 Warnakulasuriya Patabendige Ushantha Joseph Chaminda Vaas Bowler NaN 1463 Makhaya Ntini Bowler NaN 1474 Gary Kirsten Batsman NaN 1676 Alec James Stewart Batsman NaN 2020 Inzamam-ul-Haq Batsman NaN 2168 Graham Alan Gooch Batsman NaN 2205 Wasim Akram Bowler NaN 2216 Saleem Malik Batsman NaN 2320 Geoffrey Boycott Batsman NaN 2366 Michael Colin Cowdrey Batsman NaN 2381 Mohammad Javed Miandad Khan Batsman NaN Even a cursory look can tell us that our model worked splendidly. It is surprising how many prominent player bio pages has their playing role information missing. Well, it looks like even a simple ML model can fix that gap.\nLet us see how the two most critical features (Bat_Runs_sc and Bowl_Wkts_sc) affects our predicted role.\n1 2 3 4 sns.set_palette(\u0026#34;bright\u0026#34;) sns.lmplot(\u0026#39;Bowl_Wkts_sc\u0026#39;,\u0026#39;Bat_Runs_sc\u0026#39;,data[data[\u0026#39;Bat_Mat\u0026#39;] \u0026gt; 5 ], hue=\u0026#39;predicted_role_rf\u0026#39;, fit_reg=False, size=10) plt.plot([0,7.0],[100,0]) I have plotted a diagonal line, below which most of the points are clustered. It represents a kind of pareto-frontier which only exceptional players can breach. Note that there is no statistical basis for my choice of x and y intercepts, I just based it on visual inspection. Let us see the list of players who reside above this threshold.\n1 data[data[\u0026#39;Bat_Mat\u0026#39;] \u0026gt; 5].query(\u0026#39;Bowl_Wkts_sc*100 + Bat_Runs_sc*7 \u0026gt; 700 \u0026#39;)[[\u0026#39;Bio_Full_name\u0026#39;,\u0026#39;Bat_Mat\u0026#39;,\u0026#39;predicted_role_rf\u0026#39;,\u0026#39;Bowl_Wkts_sc\u0026#39;,\u0026#39;Bat_Runs_sc\u0026#39;]] Bio_Full_name Bat_Mat predicted_role_rf Bowl_Wkts_sc Bat_Runs_sc 62 Steven Peter Devereux Smith 59 Batsman 0.288136 98.237288 377 Ravindrasinh Anirudhsinh Jadeja 35 Bowler 4.714286 33.600000 380 Ravichandran Ashwin 55 Bowler 5.527273 37.363636 456 Christopher Lance Cairns 62 Allrounder 3.516129 53.548387 526 Shakib Al Hasan 51 Allrounder 3.686275 70.470588 720 Sikandar Raza Butt 9 Allrounder 1.444444 84.111111 832 Donald George Bradman 52 Batsman 0.038462 134.538462 1133 Herbert Vivian Hordern 7 Bowler 6.571429 36.285714 1177 Richard John Hadlee 86 Bowler 5.011628 36.325581 1240 Yasir Shah 28 Bowler 5.892857 15.892857 1468 Jacques Henry Kallis 166 Allrounder 1.759036 80.054217 1548 Charles Thomas Biass Turner 17 Bowler 5.941176 19.000000 1748 Garfield St Aubrun Sobers 93 Allrounder 2.526882 86.365591 1825 Michael John Procter 7 Bowler 5.857143 32.285714 1838 Robert Graeme Pollock 23 Batsman 0.173913 98.086957 1849 Edgar John Barlow 30 Allrounder 1.333333 83.866667 1860 Trevor Leslie Goddard 41 Allrounder 3.000000 61.365854 2157 Ian Terence Botham 102 Allrounder 3.754902 50.980392 2285 Mulvantrai Himmatlal Mankad 44 Allrounder 3.681818 47.931818 2386 Imran Khan Niazi 88 Allrounder 4.113636 43.261364 2522 George Aubrey Faulkner 25 Allrounder 3.280000 70.160000 2741 George Joseph Thompson 6 Allrounder 3.833333 45.500000 2769 Sydney Francis Barnes 27 Bowler 7.000000 8.962963 2809 Thomas Richardson 14 Bowler 6.285714 12.642857 2821 John James Ferris 9 Bowler 6.777778 12.666667 2845 George Alfred Lohmann 18 Bowler 6.222222 11.833333 The list is dominated by exceptional all-rounders. Among specialists, bowlers fare better. Perhaps it is my fault that I set the bar for greatness too high. The Bat_Runs_sc of Bradman is so far ahead of the rest, that it one tends to choose a higher value for y-intercept.\nFinally let us plot Bat_Runs_sc against predicted playing role using a violin plot. This will shows distribution of runs scored across the multiple categories of playing role. We can see that for batsmen, the bulk of the violin plot is top heavy whereas for the bowlers it is bottom heavy.\n1 2 3 4 sns.violinplot(x=\u0026#39;predicted_role_rf\u0026#39;, y=\u0026#39;Bat_Runs_sc\u0026#39;, data=data, scale=\u0026#39;width\u0026#39;) Conclusion If you review the length of the posts, less than 20% is allocated to running the actual machine learning code. That closely reflects the time spent on this project as well. Bulk of the time is spent in collecting and curating the data. Also the results from RandomForest Classifier is revealing. Right tool for the right job is often more effective than a generic tool which is universally useful.\nMachine Learning and Data science is a vast subject. Despite the length of this post, I have barely touched the surface of this domain. Apart from the knowledge of tools and procedures, one needs to have a good understanding of the data and be conscious of the inherent biases in the numerical models.\nFinally, scikit-learn is an excellent resource for learning and practising Machine learning. It has excellent documentation and helper functions for many of the common tasks. I found Python Data Science Handbook to be another great freely available resource.\n","date":"2018-04-28T12:00:00Z","permalink":"https://example.com/p/2018-04-28-predicting-the-playing-role-of-a-cricketer-using-machine-learning-part-2/","title":"Predicting the playing role of a cricketer using Machine Learning (Part 2)"},{"content":"In this project, we will apply Machine Learning techniques to predict whether a particular cricket player is a batsman or bowler based on his career stats. First we will use Deep Neural Networks (DNN) model and later compare the results with a simpler classifier algorithm such as Random Forest Classifier.\nFor the uninitiated, Cricket is a game of bat and ball, much similar to baseball. Cricket players have different roles to play in a game. Batsman\u0026rsquo;s role is to score runs while not losing their wicket, bowlers role is to get the batsman\u0026rsquo;s wicket while restricting their run score in due process. Allrounders can do both batting and bowling role well. A wicketkeeper is a specialist role whose purpose is to catch behind the wickets during the bowling innings. If you really intend to understand tne data behind this exercise, my half-baked explanations are barely sufficient. I suggest to refer to better reference such as this.\nFor the sake of consistency, we will consider only test players. Apart from being a puritan choice, the playing role of a player may differ from format to format and so we can\u0026rsquo;t expect a consistent prediction. The cricinfo player bio page has this playing role information, but that covers only around 20% of the test players. For vast majority of players we only have numerical stats but no description whether the player is a batsman or a bowler.\nThis is a perfect problem for ML to assist. We have a decent amount of training data (600+ players) and good amount of features (statistics) to base our prediction on. It would also be interesting to observe which features are relevant to our consideration of a player as batsman or bowler.\nThis is by no means a comprehensive tutorial for ML. The goal of this post is to serve as gentle introduction and wet the appetite for newcomers. So in the process, I would cover the following areas:\nData collection Data processing Machine Learning Data visualization Data Collection Most ML examples work with pre-processed data but in reality data is seldom available prepackaged. For this problem, we will have to scrape the data off cricinfo website. We will use Scrapy for getting this info off cricinfo website.\nThe Scrapy tool is well-documented and has some nice tutorials to get started. However I will attempt my own brief introduction.\nTo start we create a project as follows:\nscrapy startproject \u0026lt;project_name\u0026gt; This will create the necessary folder structure. It will also create some boilerplate code which we can customize to our will later on.\nThe default folder structure creates a parent folder and child folder with the same name. Do not get confused. All Scrapy commands are executed in the parent folder (where )\nUltimately our objective is to perform a web scraping using scrapy crawl command which looks like the one below:\ncd \u0026lt;project_name\u0026gt; Scrapy crawl \u0026lt;spider_name\u0026gt; -o \u0026lt;file_name\u0026gt;.csv In this command, the spider_name is the name attribute of a spider class. This spider class file should be located under the spiders directory (\u0026lt;project_name\u0026gt;/\u0026lt;project_name\u0026gt;/spiders/*.py). The file_name is the output file name, csv being our chosen export format. Let us see the steps involved in reaching to this stage.\nSpider class Start with the skeleton code required for scraping. Here is the skeleton spider class file we will use:\n1 2 3 4 5 6 7 8 9 10 11 import Scrapy class CricinfoSpider(Scrapy.Spider): name = \u0026#34;players\u0026#34; start_urls = [ # list of starting urls ] def parse(self, response): # do some fancy web scraping pass The file name of this python class is arbitrary\nThe spiders directory can contain multiple python file each implementing its own spider logic. The spider name attribute should be unique across all these files.\nA Spider class should contain the following:\nname attribute to be called from the Scrapy command line start_urls is a list holding the URLS which act as the starting pages of our scraping. parse function which performs the grunt of scraping the data. The parse function can also recursively call itself to scrape more pages or it can also have a callback function which can be further customized. The starting page for our scraping should be the one containing links to all player profile pages. In cricinfo, there is no such global directory. However there is a page for players who represented a country. The URL is like this\nhttp://www.espncricinfo.com/ci/content/player/caps.html?country=1;class=1 Notice the parameters in the URL which we can manipulate to enumerate all test playing nations. The class parameter represents the game type- for test matches it is 1. Since the list of test playing nations is only a handful, I manually determined that the country code for test playing nations are 1 to 10 and 25 (25 is for Bangladesh, the latest addition to test playing nations). Armed with this info, we can define our start_urls as follows:\n1 start_urls = [\u0026#39;http://www.espncricinfo.com/ci/content/player/caps.html?country={0};class=1\u0026#39;.format(str(x)) for x in list(range(1,10))+[25]] During a scraping process, Scrapy will issue a GET request to each one of these start_urls. If we get a valid HTTP 200 response, the response body will be passed on to parse function.\nParse function: The parse function receives the response object and extracts useful info from it. Our starting page doesn\u0026rsquo;t have the data that we need but contains the links to the player stats page which is what we want. So we will extract the links to player stats page and define another parse function to extract the statistics.\nTo extract the links, we will use CSS selectors. Looking at the source code of the page, we can see that all links to player stats has a class name of ColumnistSmry. But that is not enough to uniquely identify player links since match links also has the same class name. We can further filter by matching the target URL which is expected to contain the string \u0026ldquo;player\u0026rdquo;. So our CSS selector for extracting relevant URLs would be:\nresponse.css('a.ColumnistSmry[href*=\u0026quot;player\u0026quot;]::attr(href)') Once we extract the link, Scrapy provides a handy response.follow function which navigates to the extracted link and can execute a callback function. This callback function will handle all our stats extraction since it will work directly with the player stats page. At this point, rest of the code is just a matter of studying the source code and extracting features using css selectors.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def parse(self,response): #follow player links for href in response.css(\u0026#39;a.ColumnistSmry[href*=\u0026#34;player\u0026#34;]::attr(href)\u0026#39;): yield response.follow(href, self.parse_player) def parse_player(self, response): player = PlayerItem() d = {} info = response.css(\u0026#34;.ciPlayerinformationtxt\u0026#34;) tables = response.css(\u0026#39;.engineTable\u0026#39;) values = [i.css(\u0026#39;span::text\u0026#39;).extract() for i in info] keys = [i.css(\u0026#39;b::text\u0026#39;).extract() for i in info] batting = tables[0] batting_keys = batting.css(\u0026#39;th::text\u0026#39;).extract() batting_values = batting.css(\u0026#39;tr.data1\u0026#39;)[0].css(\u0026#39;td::text\u0026#39;).extract()[-len(batting_keys):] bowling = tables[1] bowling_keys = bowling.css(\u0026#39;th::text\u0026#39;).extract() bowling_values = bowling.css(\u0026#39;tr.data1\u0026#39;)[0].css(\u0026#39;td::text\u0026#39;).extract()[-len(bowling_keys):] for item in zip(keys, values): key = \u0026#39;Bio_\u0026#39; + self.title_clean(item[0][0]) player[key] = self.clean(item[1]) for item in zip(batting_keys, batting_values): key = \u0026#39;Bat_\u0026#39; + self.title_clean(item[0]) player[key] = self.clean(item[1]) for item in zip(bowling_keys, bowling_values): key = \u0026#39;Bowl_\u0026#39; + self.title_clean(item[0]) player[key] = self.clean(item[1]) return player If you find it difficult to define appropriate CSS selector for any element, you can always use inspector tool from Developer tools in Chrome or firefox. You can right click on the interesting code block and copy CSS selector or Xpath (Xpath is also supported in Scrapy)\nItems Class: In the parse function, you can note that I am updating the parsed information in to a class called PlayerItem(). It is an instance of Scrapy.Item class and its attributes are defined in items.py file. Here is a brief snippet of my items.py file.\n1 2 3 4 5 6 7 8 9 from Scrapy import Item, Field class PlayerItem(Item): # define the fields for your item here like: Bio_Full_name = Field() Bio_Born = Field() Bio_Current_age = Field() Bio_Major_teams = Field() Bio_Playing_role = Field() You don\u0026rsquo;t have to always use a Item class. One can simply yield the values from parse function and it will output to CSV just fine. However without an Item class, the fields of the CSV file is determined by the fields in the first record processed. Any new field that may appear in subsequent scraped records will be silently dropped. For that reason, it is preferrable to define the structure of our parsed data ahead using an Item class.\nConclusion Finally we can save the result of our hardwork to a precocious CSV file and guard it (Just kidding\u0026hellip; If you lose or corrupt the file, just delete it and scrape again! ).\nscrapy crawl players -o players.csv In the next post we will look at pre-processing this data for our machine learning work. To learn more about Scrapy, do head to its official site. The documentation is excellent and once you grasp hold of the structure, rest falls in to place quite easily.\n","date":"2018-04-23T12:00:00Z","permalink":"https://example.com/p/2018-04-23-predicting-the-playing-role-of-a-cricketer-using-machine-learning-part-1/","title":"Predicting the playing role of a cricketer using Machine Learning (Part 1)"},{"content":"This post is a gentle introduction to networking with Openstack using the Neutron module. Being an introduction, we will not focus on setting up OpenStack from scratch. Instead we will familiarize ourselves with core concepts of Neutron and common administrative tasks. We will use the latest release of Openstack, Queens.\nFor ease of setup, we will make use of pre-packaged Devstack environment. Devstack is a set of scripts from official OpenStack community that allows us to quickly build an Openstack instance with latest versions of all modules. It is very useful for testing and training purposes.\nNeutron Concepts Let us go through the important terms first:\nNetwork - A Network is a logical container of entities. To think of it in physical networking terms, it can represent a site, a DC, a campus- or anything else termed as a single network. In AWS terms, a network is similar to a VPC. Note that the definition of a Network is not entirely arbitrary as we may encounter some design choices when deciding CIDR, load balancers, routers, external gateway etc.,\nSubnet - Subnet is similar to a vlan in traditional networking. All hosts in the same Subnet can talk to each other and share the same address space. For beginners, it is possible to confuse a Subnet with Network since both terms are used interchangeably in traditional networking. But keep in mind that one network can contain many subnets. The concept of a Subnet is similar both in OpenStack and AWS.\nPort - A port is a virtual NIC card which is used by resources to access network. A port is tied to a subnet and assigned with a Security group (introduced below). In AWS terms, it is similar to ENI (Elastic Network Interface)\nSecurity Group - Security groups are like port ACL in traditional networking, except that the filtering happens at the hypervisor level. Multiple ports can share the same security group. By default, entities assigned to the same Security Group can talk to each other. It is also similar in scope and function to the Security Group in AWS.\nFloating IP - This is similar to elastic IP in AWS. These are not tied to any port and can be re-assigned to other ports dynamically. The closest feature in traditional networking would be a static NAT. In AWS terms, it is called Elastic IP.\nObjective We will create the below topology on Openstack. Hosts host-1 \u0026amp; host-2 will be on same subnet while host-3 will be on another subnet. We will enable reachability between all 3 hosts. Finally, we will also enable bi-directional connectivity to host-1 from internet.\nPreparing the environment Make home directory for our devstack installation and download the latest copy of DevStack there. I am using Ubuntu 16.04 for my environment.\n1 2 $ sudo mkdir -p /opt/stack \u0026amp;\u0026amp; cd /opt/stack $ sudo git clone https://git.openstack.org/openstack-dev/devstack Switch to devstack folder and create a user account using the pre-built script. The script grants passwordless root privilege to this account. Switch to the stack account to proceed with rest of the steps.\n1 2 $ cd devstack $ sudo bash tools/create-stack-user.sh Switch to stack user and create a config file. Provide your preferred password which will be used for rest of the setup. The local.conf should be present in the root directory of devstack (same location as stack.sh)\n1 2 3 4 5 6 7 $ sudo su stack $ vi local.conf [[local|localrc]] ADMIN_PASSWORD=password DATABASE_PASSWORD=$ADMIN_PASSWORD RABBIT_PASSWORD=$ADMIN_PASSWORD SERVICE_PASSWORD=$ADMIN_PASSWORD Run the setup script stack.sh and watch the magic happen. It takes a while since the script installs almost every component of Openstack.\n1 ./stack.sh DevStack also provides convenient script to tear down the environment and start from scratch. So once we are done with our testing, we can run unstack.sh to bring the environment back to a blank state.\nAfter the installation is complete, source the openrc file to import openstack commands in your searchpath.\n1 source openrc Building subnets Before we create new subnets, let us list the set of pre-built subnets that were setup as part of devstack deployment.\n1 2 3 4 5 6 7 stack@openstack-instance-2:~/devstack$ openstack network list -f yaml - ID: 1fe4e71c-d94e-400c-bcee-8067b621c827 Name: public Subnets: c0718110-7493-46bf-ba48-720762e47934, fab15575-ae4f-4528-bf0d-ac40d2000484 - ID: 2ebe2084-6dae-40dd-8704-7954944238d7 Name: private Subnets: 244ed014-8b9f-4e4d-ab12-9931ecec4238, 265de598-d881-4d58-a185-1a69be39b2fd The default output format is table. To simplify horizontal scrolling, I have selected yaml output using the -f switch.\nCreate a new network called inside.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 stack@openstack-instance-2:~/devstack$ openstack network create inside +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | UP | | availability_zone_hints | | | availability_zones | | | created_at | 2018-04-01T09:14:41Z | | description | | | dns_domain | None | | id | a0fc23dd-f1a5-4e9b-9d58-ea63b30d076b | | ipv4_address_scope | None | | ipv6_address_scope | None | | is_default | False | | is_vlan_transparent | None | | mtu | 1450 | | name | inside | | port_security_enabled | True | | project_id | 9d390c83cc4e46c7a40167dee68075f0 | | provider:network_type | None | | provider:physical_network | None | | provider:segmentation_id | None | | qos_policy_id | None | | revision_number | 2 | | router:external | Internal | | segments | None | | shared | False | | status | ACTIVE | | subnets | | | tags | | | updated_at | 2018-04-01T09:14:41Z | +---------------------------+--------------------------------------+ Next we will create new subnets. Before that, let us list down the existing subnets for reference.\n1 2 3 4 5 6 7 8 9 stack@openstack-instance-2:~/devstack$ openstack subnet list -f yaml - ID: 244ed014-8b9f-4e4d-ab12-9931ecec4238 Name: private-subnet Network: 2ebe2084-6dae-40dd-8704-7954944238d7 Subnet: 10.0.0.0/26 - ID: 265de598-d881-4d58-a185-1a69be39b2fd Name: ipv6-private-subnet Network: 2ebe2084-6dae-40dd-8704-7954944238d7 Subnet: fd7b:34b0:9a57::/64 Create a new subnet within the network. We will call it net-01 and assign a subnet range of 10.1.1.0/24\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 stack@openstack-instance-2:~/devstack$ openstack subnet create net-01 --network inside --subnet-range 10.1.1.0/24 +-------------------+--------------------------------------+ | Field | Value | +-------------------+--------------------------------------+ | allocation_pools | 10.1.1.2-10.1.1.254 | | cidr | 10.1.1.0/24 | | created_at | 2018-04-01T09:20:13Z | | description | | | dns_nameservers | | | enable_dhcp | True | | gateway_ip | 10.1.1.1 | | host_routes | | | id | 0317aa4a-84d3-44df-8781-3f04d558a473 | | ip_version | 4 | | ipv6_address_mode | None | | ipv6_ra_mode | None | | name | net-01 | | network_id | a0fc23dd-f1a5-4e9b-9d58-ea63b30d076b | | project_id | 9d390c83cc4e46c7a40167dee68075f0 | | revision_number | 0 | | segment_id | None | | service_types | | | subnetpool_id | None | | tags | | | updated_at | 2018-04-01T09:20:13Z | +-------------------+--------------------------------------+ As mentioned earlier, the relation between subnet and network is many-to-one. So we can create another subnet net-02 within the same inside network.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 stack@openstack-instance-2:~/devstack$ openstack subnet create net-02 --network inside --subnet-range 10.1.2.0/24 +-------------------+--------------------------------------+ | Field | Value | +-------------------+--------------------------------------+ | allocation_pools | 10.1.2.2-10.1.2.254 | | cidr | 10.1.2.0/24 | | created_at | 2018-04-01T09:21:07Z | | description | | | dns_nameservers | | | enable_dhcp | True | | gateway_ip | 10.1.2.1 | | host_routes | | | id | 6e5f2220-29da-4679-962f-22934f2c3d49 | | ip_version | 4 | | ipv6_address_mode | None | | ipv6_ra_mode | None | | name | net-02 | | network_id | a0fc23dd-f1a5-4e9b-9d58-ea63b30d076b | | project_id | 9d390c83cc4e46c7a40167dee68075f0 | | revision_number | 0 | | segment_id | None | | service_types | | | subnetpool_id | None | | tags | | | updated_at | 2018-04-01T09:21:07Z | +-------------------+--------------------------------------+ Creating nova compute instances We need to know the image names available before creating an instance. List the images available.\n1 2 3 4 stack@openstack-instance-2:~/devstack$ openstack image list -f yaml - ID: 2a5fbb6b-b694-4f76-ac19-4af358d1c7e8 Name: cirros-0.3.5-x86_64-disk Status: active Cirros is a tiny image available as part of devstack, useful for testing purpose. Let us create two instances under inside network running cirros image.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 stack@openstack-instance-2:~/devstack$ openstack server create / --flavor m1.tiny --image cirros-0.3.5-x86_64-disk --network inside host --min 2 --max 2 +-----------------------------+-----------------------------------------------------------------+ | Field | Value | +-----------------------------+-----------------------------------------------------------------+ | OS-DCF:diskConfig | MANUAL | | OS-EXT-AZ:availability_zone | | | OS-EXT-STS:power_state | NOSTATE | | OS-EXT-STS:task_state | scheduling | | OS-EXT-STS:vm_state | building | | OS-SRV-USG:launched_at | None | | OS-SRV-USG:terminated_at | None | | accessIPv4 | | | accessIPv6 | | | addresses | | | adminPass | odJy58yPdHnz | | config_drive | | | created | 2018-04-01T09:25:55Z | | flavor | m1.tiny (1) | | hostId | | | id | dbda4926-4ca6-40be-b673-d0c811ae43c2 | | image | cirros-0.3.5-x86_64-disk (2a5fbb6b-b694-4f76-ac19-4af358d1c7e8) | | key_name | None | | name | host-1 | | progress | 0 | | project_id | 9d390c83cc4e46c7a40167dee68075f0 | | properties | | | security_groups | name=\u0026#39;default\u0026#39; | | status | BUILD | | updated | 2018-04-01T09:25:55Z | | user_id | dfd4e794f59f48549016de1263c30dbb | | volumes_attached | | +-----------------------------+-----------------------------------------------------------------+ Either min or max parameter is necessary. or else server build fails miserably with no clue as to what went wrong.\nNote that in the above command we specified only the network and not subnet. But this network has two subnets. So let us see which subnet is chosen to run the nova compute instances.\n1 2 3 4 5 6 7 8 9 10 11 12 13 stack@openstack-instance-2:~/devstack$ openstack server list -f yaml - Flavor: m1.tiny ID: 5ea3d968-aa29-4911-b658-e0b6498b09f4 Image: cirros-0.3.5-x86_64-disk Name: host-2 Networks: inside=10.1.1.9 Status: ACTIVE - Flavor: m1.tiny ID: dbda4926-4ca6-40be-b673-d0c811ae43c2 Image: cirros-0.3.5-x86_64-disk Name: host-1 Networks: inside=10.1.1.10 Status: ACTIVE It turns out the first subnet net-01 we created is being used. But if we want to specify net-02 for instance creation, it is more tricky. We need to create a port in that subnet and attach its nic during instance creation.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 stack@openstack-instance-2:~/devstack$ openstack port create \\ --network a0fc23dd-f1a5-4e9b-9d58-ea63b30d076b --fixed-ip subnet=6e5f2220-29da-4679-962f-22934f2c3d49 if-host-02 +-----------------------+-------------------------------------------------------------------------+ | Field | Value | +-----------------------+-------------------------------------------------------------------------+ | admin_state_up | UP | | allowed_address_pairs | | | binding_host_id | None | | binding_profile | None | | binding_vif_details | None | | binding_vif_type | None | | binding_vnic_type | normal | | created_at | 2018-04-01T09:32:31Z | | data_plane_status | None | | description | | | device_id | | | device_owner | | | dns_assignment | None | | dns_domain | None | | dns_name | None | | extra_dhcp_opts | | | fixed_ips | ip_address=\u0026#39;10.1.2.5\u0026#39;, subnet_id=\u0026#39;6e5f2220-29da-4679-962f-22934f2c3d49\u0026#39; | | id | 2b9635bf-8f93-4663-9c6d-f40f0492195c | | ip_address | None | | mac_address | fa:16:3e:40:08:73 | | name | if-host-02 | | network_id | a0fc23dd-f1a5-4e9b-9d58-ea63b30d076b | | option_name | None | | option_value | None | | port_security_enabled | True | | project_id | 9d390c83cc4e46c7a40167dee68075f0 | | qos_policy_id | None | | revision_number | 6 | | security_group_ids | bd41b78c-974c-4854-b808-dec11575964b | | status | DOWN | | subnet_id | None | | tags | | | trunk_details | None | | updated_at | 2018-04-01T09:32:31Z | +-----------------------+-------------------------------------------------------------------------+ Now create an instance in this subnet. Use the port id from above output to specify the nic.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 stack@openstack-instance-2:~/devstack$ openstack server create --flavor m1.tiny \\ --image cirros-0.3.5-x86_64-disk --nic port-id=2b9635bf-8f93-4663-9c6d-f40f0492195c host-3 --max 1 +-----------------------------+-----------------------------------------------------------------+ | Field | Value | +-----------------------------+-----------------------------------------------------------------+ | OS-DCF:diskConfig | MANUAL | | OS-EXT-AZ:availability_zone | | | OS-EXT-STS:power_state | NOSTATE | | OS-EXT-STS:task_state | scheduling | | OS-EXT-STS:vm_state | building | | OS-SRV-USG:launched_at | None | | OS-SRV-USG:terminated_at | None | | accessIPv4 | | | accessIPv6 | | | addresses | | | adminPass | hnN294KgUTrY | | config_drive | | | created | 2018-04-01T09:52:23Z | | flavor | m1.tiny (1) | | hostId | | | id | 376c3d59-f74c-4b3b-a4c7-5b9fb9d3eef7 | | image | cirros-0.3.5-x86_64-disk (2a5fbb6b-b694-4f76-ac19-4af358d1c7e8) | | key_name | None | | name | host-3 | | progress | 0 | | project_id | 9d390c83cc4e46c7a40167dee68075f0 | | properties | | | security_groups | name=\u0026#39;default\u0026#39; | | status | BUILD | | updated | 2018-04-01T09:52:23Z | | user_id | dfd4e794f59f48549016de1263c30dbb | | volumes_attached | | +-----------------------------+-----------------------------------------------------------------+ Before we move on to next section, tet us check the server instances we have created so far.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 stack@openstack-instance-2:~/devstack$ openstack server list -f yaml - Flavor: m1.tiny ID: 376c3d59-f74c-4b3b-a4c7-5b9fb9d3eef7 Image: cirros-0.3.5-x86_64-disk Name: host-3 Networks: inside=10.1.2.5 Status: ACTIVE - Flavor: m1.tiny ID: 5ea3d968-aa29-4911-b658-e0b6498b09f4 Image: cirros-0.3.5-x86_64-disk Name: host-2 Networks: inside=10.1.1.9 Status: ACTIVE - Flavor: m1.tiny ID: dbda4926-4ca6-40be-b673-d0c811ae43c2 Image: cirros-0.3.5-x86_64-disk Name: host-1 Networks: inside=10.1.1.10 Status: ACTIVE Neutron networking between hosts Let us see the subnet membership of each ports.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 stack@openstack-instance-2:~/devstack$ openstack port list -f yaml --fixed-ip subnet=net-01 - Fixed IP Addresses: \u0026#39;ip_address=\u0026#39;\u0026#39;10.1.1.2\u0026#39;\u0026#39;, subnet_id=\u0026#39;\u0026#39;0317aa4a-84d3-44df-8781-3f04d558a473\u0026#39;\u0026#39; ip_address=\u0026#39;\u0026#39;10.1.2.2\u0026#39;\u0026#39;, subnet_id=\u0026#39;\u0026#39;6e5f2220-29da-4679-962f-22934f2c3d49\u0026#39;\u0026#39;\u0026#39; ID: 6764de9f-deec-494b-a9b4-0903bfbfefea MAC Address: fa:16:3e:8f:47:41 Name: \u0026#39;\u0026#39; Status: ACTIVE - Fixed IP Addresses: ip_address=\u0026#39;10.1.1.9\u0026#39;, subnet_id=\u0026#39;0317aa4a-84d3-44df-8781-3f04d558a473\u0026#39; ID: ab283b4a-5738-48c9-8a53-343a1e5c795d MAC Address: fa:16:3e:9d:b6:7f Name: \u0026#39;\u0026#39; Status: ACTIVE - Fixed IP Addresses: ip_address=\u0026#39;10.1.1.10\u0026#39;, subnet_id=\u0026#39;0317aa4a-84d3-44df-8781-3f04d558a473\u0026#39; ID: de7ee5de-e694-4ff3-9763-d7fb86e0038c MAC Address: fa:16:3e:7a:ce:84 Name: \u0026#39;\u0026#39; Status: ACTIVE stack@openstack-instance-2:~/devstack$ openstack port list -f yaml --fixed-ip subnet=net-02 - Fixed IP Addresses: ip_address=\u0026#39;10.1.2.5\u0026#39;, subnet_id=\u0026#39;6e5f2220-29da-4679-962f-22934f2c3d49\u0026#39; ID: 2b9635bf-8f93-4663-9c6d-f40f0492195c MAC Address: fa:16:3e:40:08:73 Name: if-host-02 Status: ACTIVE - Fixed IP Addresses: \u0026#39;ip_address=\u0026#39;\u0026#39;10.1.1.2\u0026#39;\u0026#39;, subnet_id=\u0026#39;\u0026#39;0317aa4a-84d3-44df-8781-3f04d558a473\u0026#39;\u0026#39; ip_address=\u0026#39;\u0026#39;10.1.2.2\u0026#39;\u0026#39;, subnet_id=\u0026#39;\u0026#39;6e5f2220-29da-4679-962f-22934f2c3d49\u0026#39;\u0026#39;\u0026#39; ID: 6764de9f-deec-494b-a9b4-0903bfbfefea MAC Address: fa:16:3e:8f:47:41 Name: \u0026#39;\u0026#39; Status: ACTIVE We can see that one port appears in both the subnets (MAC address: fa:16:3e:8f:47:41). This is the default DNS server of the network and gets attached to the subnet automatically unless we explicitly specify otherwise.\nTo test connectivity we need to console in to the hosts. We will use virsh to console in to the instances. First let us list the hosts.\n1 2 3 4 5 6 stack@openstack-instance-2:~/devstack$ sudo virsh list Id Name State ---------------------------------------------------- 1 instance-00000001 running 2 instance-00000002 running 3 instance-00000003 running Console to the first instance host-1.\n1 2 3 4 5 6 7 8 9 10 11 stack@openstack-instance-2:~/devstack$ sudo virsh console 1 Connected to domain instance-00000001 Escape character is ^] login as \u0026#39;cirros\u0026#39; user. default password: \u0026#39;cubswin:)\u0026#39;. use \u0026#39;sudo\u0026#39; for root. cirros login: cirros Password: $ ip add | grep \u0026#39;inet.*10\u0026#39; inet 10.1.1.10/24 brd 10.1.1.255 scope global eth0 $ sudo hostname cirros-1-1-1-10 $ export PS1=\u0026#34;\\h$ \u0026#34; We will see that this host can talk to other host host-2 within the same subnet.\n1 2 3 4 5 6 7 8 9 cirros-1-1-1-10$ ping 10.1.1.9 PING 10.1.1.9 (10.1.1.9): 56 data bytes 64 bytes from 10.1.1.9: seq=0 ttl=64 time=4.409 ms 64 bytes from 10.1.1.9: seq=1 ttl=64 time=1.070 ms 64 bytes from 10.1.1.9: seq=2 ttl=64 time=1.437 ms --- 10.1.1.9 ping statistics --- 3 packets transmitted, 3 packets received, 0% packet loss round-trip min/avg/max = 1.070/2.305/4.409 ms But it cannot reach host-3 which is a different subnet (net-02)\n1 2 3 4 5 cirros-1-1-1-10$ ping 10.1.2.5 PING 10.1.2.5 (10.1.2.5): 56 data bytes --- 10.1.2.5 ping statistics --- 3 packets transmitted, 0 packets received, 100% packet loss Likewise, the host cannot reach external network such as internet.\n1 2 3 4 5 cirros-1-1-1-10$ ping 8.8.8.8 PING 8.8.8.8 (8.8.8.8): 56 data bytes --- 8.8.8.8 ping statistics --- 3 packets transmitted, 0 packets received, 100% packet loss Like in any tradional network, we need a router to allow traffic between different subnets. OpenStack Neutron provides a software defined router which can attach to multiple subnets and provide the same functionality.\nLet us go ahead and create a router.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 stack@openstack-instance-2:~/devstack$ openstack router create demo_router +-------------------------+--------------------------------------+ | Field | Value | +-------------------------+--------------------------------------+ | admin_state_up | UP | | availability_zone_hints | | | availability_zones | | | created_at | 2018-04-02T00:14:29Z | | description | | | distributed | False | | external_gateway_info | None | | flavor_id | None | | ha | False | | id | f36a24b0-e4a3-41de-9fdd-73f85df9dbaa | | name | demo_router | | project_id | 9d390c83cc4e46c7a40167dee68075f0 | | revision_number | 1 | | routes | | | status | ACTIVE | | tags | | | updated_at | 2018-04-02T00:14:29Z | +-------------------------+--------------------------------------+ List the routers visible to validate the result. You can see an existing pre-built router and the one we created just now.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 stack@openstack-instance-2:~/devstack$ openstack router list -f yaml - Distributed: false HA: false ID: 58cc905f-4c8b-4641-ba92-a1a491d818ab Name: router1 Project: 9d390c83cc4e46c7a40167dee68075f0 State: UP Status: ACTIVE - Distributed: false HA: false ID: f36a24b0-e4a3-41de-9fdd-73f85df9dbaa Name: demo_router Project: 9d390c83cc4e46c7a40167dee68075f0 State: UP Status: ACTIVE Now we will attach our subnets to the router.\n1 2 stack@openstack-instance-2:~/devstack$ openstack router add subnet demo_router net-01 stack@openstack-instance-2:~/devstack$ openstack router add subnet demo_router net-02 You can see one new port from each subnet getting attached to the router.\n1 2 3 4 5 6 7 8 9 10 11 stack@openstack-instance-2:~/devstack$ openstack port list -f yaml --router demo_router - Fixed IP Addresses: ip_address=\u0026#39;10.1.1.1\u0026#39;, subnet_id=\u0026#39;0317aa4a-84d3-44df-8781-3f04d558a473\u0026#39; ID: 468ff846-fb1d-4ef2-85b0-3957885c59a8 MAC Address: fa:16:3e:b1:31:fa Name: \u0026#39;\u0026#39; Status: ACTIVE - Fixed IP Addresses: ip_address=\u0026#39;10.1.2.1\u0026#39;, subnet_id=\u0026#39;6e5f2220-29da-4679-962f-22934f2c3d49\u0026#39; ID: ee15064b-26f2-46ae-bc09-02db440edd3b MAC Address: fa:16:3e:7c:62:28 Name: \u0026#39;\u0026#39; Status: ACTIVE To reduce the output I have filtered the port list with an argument \u0026ndash;router . Most list commands have ability to filter output like this. Use the interactive help menu to figure out the filter options.\nLet us try to reach the host on the other network.\n1 2 3 4 5 6 7 8 cirros-1-1-1-10$ ping 10.1.2.5 PING 10.1.2.5 (10.1.2.5): 56 data bytes 64 bytes from 10.1.2.5: seq=0 ttl=63 time=4.141 ms 64 bytes from 10.1.2.5: seq=1 ttl=63 time=1.499 ms --- 10.1.2.5 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 1.499/2.820/4.141 ms Success! As expected, the host is one hop away.\n1 2 3 4 cirros-1-1-1-10$ traceroute 10.1.2.5 traceroute to 10.1.2.5 (10.1.2.5), 30 hops max, 46 byte packets 1 host-10-1-1-1.openstacklocal (10.1.1.1) 2.217 ms 0.360 ms 0.301 ms 2 host-10-1-2-5.openstacklocal (10.1.2.5) 1.568 ms 1.155 ms 0.803 ms But the outside world connection is not ready yet. For that we need to attach it to a public network. Recall that there was a pre-built public network as part of devstack setup. We will use this network to communicate to outside world.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 stack@openstack-instance-2:~/devstack$ openstack router set --external-gateway public demo_router stack@openstack-instance-2:~/devstack$ openstack router show demo_router -f yaml admin_state_up: UP availability_zone_hints: \u0026#39;\u0026#39; availability_zones: nova created_at: \u0026#39;2018-04-02T00:14:29Z\u0026#39; description: \u0026#39;\u0026#39; distributed: false external_gateway_info: \u0026#39;{\u0026#34;network_id\u0026#34;: \u0026#34;1fe4e71c-d94e-400c-bcee-8067b621c827\u0026#34;, \u0026#34;enable_snat\u0026#34;: true, \u0026#34;external_fixed_ips\u0026#34;: [{\u0026#34;subnet_id\u0026#34;: \u0026#34;c0718110-7493-46bf-ba48-720762e47934\u0026#34;, \u0026#34;ip_address\u0026#34;: \u0026#34;172.24.4.3\u0026#34;}, {\u0026#34;subnet_id\u0026#34;: \u0026#34;fab15575-ae4f-4528-bf0d-ac40d2000484\u0026#34;, \u0026#34;ip_address\u0026#34;: \u0026#34;2001:db8::6\u0026#34;}]}\u0026#39; flavor_id: null ha: false id: f36a24b0-e4a3-41de-9fdd-73f85df9dbaa interfaces_info: \u0026#39;[{\u0026#34;subnet_id\u0026#34;: \u0026#34;0317aa4a-84d3-44df-8781-3f04d558a473\u0026#34;, \u0026#34;ip_address\u0026#34;: \u0026#34;10.1.1.1\u0026#34;, \u0026#34;port_id\u0026#34;: \u0026#34;468ff846-fb1d-4ef2-85b0-3957885c59a8\u0026#34;}, {\u0026#34;subnet_id\u0026#34;: \u0026#34;6e5f2220-29da-4679-962f-22934f2c3d49\u0026#34;, \u0026#34;ip_address\u0026#34;: \u0026#34;10.1.2.1\u0026#34;, \u0026#34;port_id\u0026#34;: \u0026#34;ee15064b-26f2-46ae-bc09-02db440edd3b\u0026#34;}]\u0026#39; name: demo_router project_id: 9d390c83cc4e46c7a40167dee68075f0 revision_number: 5 routes: \u0026#39;\u0026#39; status: ACTIVE tags: \u0026#39;\u0026#39; updated_at: \u0026#39;2018-04-02T00:32:54Z\u0026#39; The show command is similar to list command, but can show more detailed information about a single resource. As expected, it requires an additional unique identifier argument to identify the resource which we want to display.\nSee that there is a new external_gateway_info section. Now let us try to reach the internet.\n1 2 3 4 5 6 7 8 9 10 11 12 13 stack@openstack-instance-2:~/devstack$ sudo virsh console 1 Connected to domain instance-00000001 Escape character is ^] cirros-1-1-1-10$ ping 8.8.8.8 PING 8.8.8.8 (8.8.8.8): 56 data bytes 64 bytes from 8.8.8.8: seq=0 ttl=50 time=7.129 ms 64 bytes from 8.8.8.8: seq=1 ttl=51 time=1.483 ms 64 bytes from 8.8.8.8: seq=2 ttl=51 time=1.137 ms \u0003 --- 8.8.8.8 ping statistics --- 3 packets transmitted, 3 packets received, 0% packet loss round-trip min/avg/max = 1.137/3.249/7.129 ms Success! To see how this traffic reaches out to internet, we will do tcpdump on the hypervisor.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 stack@openstack-instance-2:~/ sudo tcpdump \u0026#39;host 8.8.8.8\u0026#39; -X tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on br-ex, link-type EN10MB (Ethernet), capture size 262144 bytes 00:37:14.019550 IP 172.24.4.3 \u0026gt; google-public-dns-a.google.com: ICMP echo request, id 49153, seq 0, length 64 0x0000: 4500 0054 0979 4000 3f01 7205 ac18 0403 E..T.y@.?.r..... 0x0010: 0808 0808 0800 5076 c001 0000 73cd 73ba ......Pv....s.s. 0x0020: 0000 0000 0000 0000 0000 0000 0000 0000 ................ 0x0030: 0000 0000 0000 0000 0000 0000 0000 0000 ................ 0x0040: 0000 0000 0000 0000 0000 0000 0000 0000 ................ 0x0050: 0000 0000 .... 00:37:14.020493 IP google-public-dns-a.google.com \u0026gt; 172.24.4.3: ICMP echo reply, id 49153, seq 0, length 64 0x0000: 4500 0054 0000 0000 3401 c67e 0808 0808 E..T....4..~.... 0x0010: ac18 0403 0000 5876 c001 0000 73cd 73ba ......Xv....s.s. 0x0020: 0000 0000 0000 0000 0000 0000 0000 0000 ................ 0x0030: 0000 0000 0000 0000 0000 0000 0000 0000 ................ 0x0040: 0000 0000 0000 0000 0000 0000 0000 0000 ................ 0x0050: 0000 0000 The traffic from the host is being source NAT by the router to go out to the internet (recall from the earlier output that 172.24.4.3 belongs to the external gateway of the router). In traditional networking term this is called NAT overloading. This can provide only outbound connectivity for the host.\nIn order to provide inbound connectivity to the host, we need to create a floating ip. This is similar to elastic IP in AWS or staic NAT in traditional networking.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 stack@openstack-instance-2:~/devstack$ openstack floating ip create public +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | created_at | 2018-04-02T00:44:14Z | | description | | | fixed_ip_address | None | | floating_ip_address | 172.24.4.5 | | floating_network_id | 1fe4e71c-d94e-400c-bcee-8067b621c827 | | id | e2ff37e4-f8f5-4413-98aa-e0df516c1a3b | | name | 172.24.4.5 | | port_id | None | | project_id | 9d390c83cc4e46c7a40167dee68075f0 | | qos_policy_id | None | | revision_number | 0 | | router_id | None | | status | DOWN | | subnet_id | None | | tags | [] | | updated_at | 2018-04-02T00:44:14Z | +---------------------+--------------------------------------+ Attach this to our host-1 instance. You can now see the floating IP also listed in the openstack server list output.\n1 2 3 4 5 6 7 8 stack@openstack-instance-2:~/devstack$ openstack server add floating ip host-1 172.24.4.5 stack@openstack-instance-2:~/devstack$ openstack server list --name host-1 -f yaml - Flavor: m1.tiny ID: dbda4926-4ca6-40be-b673-d0c811ae43c2 Image: cirros-0.3.5-x86_64-disk Name: host-1 Networks: inside=10.1.1.10, 172.24.4.5 Status: ACTIVE Here you can see the floating ip is mapped to our port-id.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 stack@openstack-instance-2:~/devstack$ openstack floating ip show 172.24.4.5 +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | created_at | 2018-04-02T00:44:14Z | | description | | | fixed_ip_address | 10.1.1.10 | | floating_ip_address | 172.24.4.5 | | floating_network_id | 1fe4e71c-d94e-400c-bcee-8067b621c827 | | id | e2ff37e4-f8f5-4413-98aa-e0df516c1a3b | | name | 172.24.4.5 | | port_id | de7ee5de-e694-4ff3-9763-d7fb86e0038c | | project_id | 9d390c83cc4e46c7a40167dee68075f0 | | qos_policy_id | None | | revision_number | 2 | | router_id | f36a24b0-e4a3-41de-9fdd-73f85df9dbaa | | status | ACTIVE | | subnet_id | None | | tags | [] | | updated_at | 2018-04-02T00:47:14Z | +---------------------+--------------------------------------+ We require one last step to initiate inbound connections to the host. Recall that each ports are assigned with a security group. Since this is an inbound connection, we need to explicitly permit access in security group.\nCheck the Security Group assigned to our port.\n1 2 3 stack@openstack-instance-2:~/devstack$ openstack port \\show de7ee5de-e694-4ff3-9763-d7fb86e0038c -c \u0026#39;security_group_ids\u0026#39; -f yaml security_group_ids: bd41b78c-974c-4854-b808-dec11575964b Let us see the rules which are permitted by this security group.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 stack@openstack-instance-2:~/devstack$ openstack security group show bd41b78c-974c-4854-b808-dec11575964b -f yaml created_at: \u0026#39;2018-04-01T09:01:20Z\u0026#39; description: Default security group id: bd41b78c-974c-4854-b808-dec11575964b name: default project_id: 9d390c83cc4e46c7a40167dee68075f0 revision_number: 6 rules: \u0026#39;created_at=\u0026#39;\u0026#39;2018-04-01T09:01:20Z\u0026#39;\u0026#39;, direction=\u0026#39;\u0026#39;ingress\u0026#39;\u0026#39;, ethertype=\u0026#39;\u0026#39;IPv4\u0026#39;\u0026#39;, id=\u0026#39;\u0026#39;2ab4f8a5-43ed-4f3e-8c43-cc854bc8feb3\u0026#39;\u0026#39;, remote_group_id=\u0026#39;\u0026#39;bd41b78c-974c-4854-b808-dec11575964b\u0026#39;\u0026#39;, updated_at=\u0026#39;\u0026#39;2018-04-01T09:01:20Z\u0026#39;\u0026#39; created_at=\u0026#39;\u0026#39;2018-04-01T09:01:20Z\u0026#39;\u0026#39;, direction=\u0026#39;\u0026#39;egress\u0026#39;\u0026#39;, ethertype=\u0026#39;\u0026#39;IPv6\u0026#39;\u0026#39;, id=\u0026#39;\u0026#39;5c33d0ac-8ee9-4bd7-951c-dca998931828\u0026#39;\u0026#39;, updated_at=\u0026#39;\u0026#39;2018-04-01T09:01:20Z\u0026#39;\u0026#39; created_at=\u0026#39;\u0026#39;2018-04-01T09:01:20Z\u0026#39;\u0026#39;, direction=\u0026#39;\u0026#39;ingress\u0026#39;\u0026#39;, ethertype=\u0026#39;\u0026#39;IPv6\u0026#39;\u0026#39;, id=\u0026#39;\u0026#39;d7c7a642-485b-482e-8d26-a958aaafa19e\u0026#39;\u0026#39;, remote_group_id=\u0026#39;\u0026#39;bd41b78c-974c-4854-b808-dec11575964b\u0026#39;\u0026#39;, updated_at=\u0026#39;\u0026#39;2018-04-01T09:01:20Z\u0026#39;\u0026#39; created_at=\u0026#39;\u0026#39;2018-04-01T09:01:20Z\u0026#39;\u0026#39;, direction=\u0026#39;\u0026#39;egress\u0026#39;\u0026#39;, ethertype=\u0026#39;\u0026#39;IPv4\u0026#39;\u0026#39;, id=\u0026#39;\u0026#39;f41405df-d0c9-4427-ba11-f0a26b025c3e\u0026#39;\u0026#39;, updated_at=\u0026#39;\u0026#39;2018-04-01T09:01:20Z\u0026#39;\u0026#39;\u0026#39; tags: [] updated_at: \u0026#39;2018-04-02T01:56:25Z\u0026#39; Here you can see 4 rules, two each for IPv4 and IPv6. For egress rules, you can see that there is no remote prefix or group id configured. It means all outbound connections are permitted. For ingress traffic, you can see that the remote_group_id references to self. It means that all ports assigned to the same security group can send inbound traffic unhindered.\nLet us add more rules to permit ICMP and SSH\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 stack@openstack-instance-2:~/devstack$ openstack security group rule create \\ --protocol icmp --ingress --remote-ip 0.0.0.0/0 default +-------------------+--------------------------------------+ | Field | Value | +-------------------+--------------------------------------+ | created_at | 2018-04-02T01:54:12Z | | description | | | direction | ingress | | ether_type | IPv4 | | id | df23fec9-b87a-47ef-8f38-67472de46071 | | name | None | | port_range_max | None | | port_range_min | None | | project_id | 9d390c83cc4e46c7a40167dee68075f0 | | protocol | icmp | | remote_group_id | None | | remote_ip_prefix | 0.0.0.0/0 | | revision_number | 0 | | security_group_id | bd41b78c-974c-4854-b808-dec11575964b | | updated_at | 2018-04-02T01:54:12Z | +-------------------+--------------------------------------+ stack@openstack-instance-2:~/devstack$ openstack security group rule create \\ --protocol tcp --dst-port 22 --ingress --remote-ip 0.0.0.0/0 default +-------------------+--------------------------------------+ | Field | Value | +-------------------+--------------------------------------+ | created_at | 2018-04-02T01:56:25Z | | description | | | direction | ingress | | ether_type | IPv4 | | id | adee3f3f-a5a0-497f-a12d-644e64c7015e | | name | None | | port_range_max | 22 | | port_range_min | 22 | | project_id | 9d390c83cc4e46c7a40167dee68075f0 | | protocol | tcp | | remote_group_id | None | | remote_ip_prefix | 0.0.0.0/0 | | revision_number | 0 | | security_group_id | bd41b78c-974c-4854-b808-dec11575964b | | updated_at | 2018-04-02T01:56:25Z | +-------------------+--------------------------------------+ Note that I am using the security group name default as argument instead of security group id. As with most other arguments, we can use either name or the id. We can find the name from the output of previous show command.\nNow you can see 2 new rules added to the group\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 stack@openstack-instance-2:~/devstack$ openstack security group rule list -c \u0026#39;IP Protocol\u0026#39; -c \u0026#39;IP Range\u0026#39; -c \u0026#39;Port Range\u0026#39; -f yaml - IP Protocol: null IP Range: null Port Range: \u0026#39;\u0026#39; - IP Protocol: null IP Range: null Port Range: \u0026#39;\u0026#39; - IP Protocol: tcp IP Range: 0.0.0.0/0 Port Range: \u0026#39;22:22\u0026#39; - IP Protocol: null IP Range: null Port Range: \u0026#39;\u0026#39; - IP Protocol: icmp IP Range: 0.0.0.0/0 Port Range: \u0026#39;\u0026#39; - IP Protocol: null IP Range: null Port Range: \u0026#39;\u0026#39; Note that in the previous list command output, there were lot of fields. Here I have used -c argument to select the interesting fields. To select multiple columns, we have to repeat this -c argument.\nNow we can initiate SSH and ICMP connections to this VM host using floating ip address.\n1 2 3 4 5 6 7 8 9 10 11 stack@openstack-instance-2:~/devstack$ ping 172.24.4.5 PING 172.24.4.5 (172.24.4.5) 56(84) bytes of data. 64 bytes from 172.24.4.5: icmp_seq=1 ttl=63 time=0.628 ms 64 bytes from 172.24.4.5: icmp_seq=2 ttl=63 time=0.529 ms ^C --- 172.24.4.5 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1003ms rtt min/avg/max/mdev = 0.529/0.578/0.628/0.055 ms stack@openstack-instance-2:~/devstack$ ssh cirros@172.24.4.5 cirros@172.24.4.5\u0026#39;s password: cirros-1-1-1-10$ Recall that the hosts were able to ping each other even before we updated the security group. It is because all the ports were assigned to the default security group and hosts on security group can talk to each other by virtue of an implicit rule.\nConclusion I have barely scratched the surface of things possible with Openstack. However, this post should give a good idea about the general command structure and operation procedures in Openstack. For further learning, dig around the official documentation (pay attention to openstack version in the docs, there can be major differences between each versions) or setup a lab environment and play around.\nOn the Neutron front, there are a lot of topics to explore such as ML2 network types, virtual switch types, L3 agents etc., Redhat\u0026rsquo;s official documentation provides a good introduction to many of these components.\n","date":"2018-03-22T12:00:00Z","permalink":"https://example.com/p/2018-03-22-introduction-to-openstack-networking-for-network-engineers/","title":"Introduction to OpenStack Networking for Network Engineers"},{"content":"Constraint programming (CP) is a subset of Operations Research (OR) where our task is to identify all feasible solutions to a given problem that satisfies a set of constraints. This is different from an optimization problem, where an objective function is defined and we arrive at solutions that either maximizes or minimizes an objective function.\nCP is mostly well suited for solving logic puzzles, since most logic puzzles are based on constraints and enumerating feasible solutions. But apart from recreational maths, CP also has a lot of practical applications in Scheduling, Resource allocation, Manufacturing etc.,\nRecently I came across or-tools from Google github repo. It is a suite of libraries for solving Operations Research problems. I wanted to give it a try by solving a simple logic puzzle. The puzzle I chose is called Hundred Fowls Problem. Let us see how it goes.\nHundred Fowls problem This puzzle found in the sixth-century work of mathematician Chang Chiu-chen called the \u0026ldquo;hundred fowls\u0026rdquo; problem asks:\nIf a rooster is worth five coins, a hen three coins, and three chickens together are worth one coin, how many roosters, hens, and chicks totalling 100 can be bought for 100 coins?\nThis translates to solving a set of 2 algebraic equations with 3 variables. In real numbers, there are infinite solutions to this puzzle since we are short by one non-equivalent equation to bind values to 3 variables. However, buying non-integer number of fowls can get tricky, so we can safely assume that we are dealing with 3 integer valued variables here. This reduces the solution space to a finite count, but still there are more than one feasible solution. This is a perfect candidate for constraint programming since we need to identify all feasible solutions and not maximize/minimize an objective function.\nComponents of an OR model A model for an optimization problem can be thought to have 3 components:\nDecision variables Constraints Objectives Decision variables are the answer an OR problem tries to solve. In our case, it is the valid number of fowls of each type that we can buy. Constraints are the limits imposed on the problem. Here we have total cost and total bird count imposed as limits. An objective is the goal of an OR problem such as maximize profit or minimize project time etc., Most often in Constraint Programming, we don\u0026rsquo;t need to work with objectives and we don\u0026rsquo;t have one for our problem as well.\nSetting up the code Let us first import the necessary modules and create a Solver and define our parameters.\n1 2 3 4 5 6 7 8 9 10 from ortools.constraint_solver import pywrapcp solver = pywrapcp.Solver(\u0026#34;hundred_fowls\u0026#34;) ROOSTER_COST = 5 HEN_COST = 3 THREE_CHICK_COST = 1 total_cost = 100 total_fowls = 100 Decision Variables Let us first try to define the limits our decision variables can take i.e., the maximum number of roosters, hens or chicks that we need to consider. One can enumerate each fowl type up to the total count (100) but this increases the search domain unnecessarily. For e.g, we cannot buy more than 20 roosters since we would exceed 100 coins nor can we buy 300 chicks for 100 coins. So, we try to establish the upper limit for each of our decision variables as below:\n1 2 3 4 5 from math import floor max_rooster = min(total_fowls, floor(total_cost/ROOSTER_COST)) max_hen = min(total_fowls, floor(total_cost/HEN_COST)) max_chick_set = min(total_fowls, floor(total_cost/THREE_CHICK_COST)) # a set of 3 chickens Notice that for chick count, we are grouping them in sets of three. This is because the unit cost of a chick is a fraction (1/3) and the solver doesn\u0026rsquo;t allow us to multiply float numbers with IntVar object (which we will come to know later). This is perfectly acceptable for our scenario since chick count always needs to be a multiple of 3 or else the total cost will never be an integer (100 in this case). Besides, this also reduces the search space for chick count by a factor of 3, eliminating obvious non-solutions.\nWe now have to spec out our decision variables. OR-Tools supports different types of decision variables such as Integers, Intervals etc., In our current challenge the decision variable is an integer so we assign it by using IntVar method. The first two values gives the lower and upper bounds for the variables. The last value is an arbitrary string handle for output representation. I am not sure under which scenarios the last variable will come to use, but likely it is an object name required for the underlying C++ code. The python module we use for or-tools is actually a wrapper for the core C++ code.\nFor our problem the decision variables can be defined as below:\n1 2 3 4 5 # since the puzzle requires us to buy atleast one fowl # of each type we will start enumerating from one rooster_count = solver.IntVar(1, max_rooster, \u0026#34;Rooster\u0026#34;) hen_count = solver.IntVar(1, max_hen, \u0026#34;Hen\u0026#34;) chick_set_count = solver.IntVar(1, max_chick_set, \u0026#34;Chick\u0026#34;) Constraints Next we add the constraints for our model. It is done using the \u0026ldquo;Add\u0026rdquo; method of Solver object.\n1 2 3 4 5 6 7 solver.Add(rooster_count + hen_count + chick_set_count*3 == total_fowls) solver.Add(rooster_count*ROOSTER_COST + hen_count*HEN_COST + chick_set_count*THREE_CHICK_COST == total_cost) Navigating the search space Next we create a decision builder which specifies how to iterate through all possible values for our problems. It is done using the Phase method. The first arguemnt is an array of our decision variables. Second argument is how we choose the next value to try for our decision variable. Last argument is how we start assigning value to our variable (from minimum or maximum). We will go with the defauls although for larger problems, we could use some sort of heuristics to search only interesting portions of a search space. You can refer to or-tools documentation for other search strategies.\n1 2 3 db = solver.Phase([rooster_count, hen_count, chick_set_count], solver.CHOOSE_FIRST_UNBOUND, solver.ASSIGN_MIN_VALUE) Solving the model Finally we solve our model and iterate through our solutions.\n1 2 3 4 5 6 7 solver.Solve(db) count = 0 while solver.NextSolution(): count += 1 print(\u0026#34;{0} Roosters, {1} Hen and {2} Chicks\u0026#34;.format(rooster_count, hen_count, chick_set_count.Value()*3)) print(\u0026#34;Number of unique solutions found - {0}\u0026#34;.format(count)) This produces 3 solutions to the puzzle.\n1 2 3 4 Rooster(4) Roosters, Hen(18) Hen and 78 Chicks Rooster(8) Roosters, Hen(11) Hen and 81 Chicks Rooster(12) Roosters, Hen(4) Hen and 84 Chicks Number of unique solutions found - 3 Further Reading or-tools manual on Constraint Programming is quite comprehensive. As always, best way to learn is to study existing code and implementations. In this regards, the example scripts from or-tools public repo is quite indispensable for learning. There is a good amount of example scripts for some of the most popular OR challenges.\n","date":"2017-12-09T12:00:00Z","permalink":"https://example.com/p/2017-12-09-using-constraint-programming-to-solve-an-ancient-chinese-math-puzzle/","title":"Using Constraint Programming Tools to solve an ancient Chinese math puzzle"},{"content":"Recently we had a cabling issue in our core infrastructure which caused around 3 to 12% packet loss across few IP streams. One of my colleagues made an interesting observation that when he tried to ping with large packet size (5000 bytes) the packet loss rose up as high as 40%. In his opinion, that meant some applications were experiencing up to 40% packet loss. I seldom do large packet ping tests unless I am troubleshooting MTU issues, so to me this observation was interesting.\nAt the outset, it may look like an aggravated problem. Yet you know that your network path MTU doesn\u0026rsquo;t support jumbo frames end-to-end. If so, why is there a difference in packet loss rate when you ping with large datagrams? The answer is not too obvious. The important thing to note is that a ping test result is not a measure of ethernet frame loss but ICMP datagram loss. In most cases (when the ICMP datagram is smaller than ethernet MTU) both are the same. But why do large ICMP datagrams have higher loss percentage than individual ethernet frames? Enter Math.\nNormal ping vs Large ping In windows a normal ping packet size is 32 bytes and in most environments, the default MTU is 1500 bytes. So a single frame is sufficient to transmit a ping packet. Things get weirder when we ping with large packets. In windows, to simulate larger packets you can use the -l option to specify packet size. Note that this size doesn\u0026rsquo;t include the packet header (20 bytes for IP header + 8 bytes for ICMP header). Which means that we can only fit 1472 bytes of ICMP payload inside a 1500 MTU ethernet frame. Any length above this must be fragmented.\nWe can test this easily. Below is the result when pinging with 1472 as the ping size (ping 8.8.8.8 -n 2 -l 1472)\n1 2 3 4 Capturing on \u0026#39;Ethernet 2\u0026#39; 1 0.000000 10.1.1.1 → 8.8.8.8 ICMP 1514 Echo (ping) request id=0x0001, seq=8/2048, ttl=128 2 0.015698 8.8.8.8 → 10.1.1.1 ICMP 106 Echo (ping) reply id=0x0001, seq=8/2048, ttl=45 2 packets captured When we ping with just one more byte, you can see that 2 packets are sent in place of 1 ((ping 8.8.8.8 -n 2 -l 1473)\n1 2 3 4 Capturing on \u0026#39;Ethernet 2\u0026#39; 1 0.000000 10.1.1.1 → 8.8.8.8 IPv4 1514 Fragmented IP protocol (proto=ICMP 1, off=0, ID=4fab) 2 0.000016 10.1.1.1 → 8.8.8.8 ICMP 35 Echo (ping) request id=0x0001, seq=10/2560, ttl=128 2 packets captured So when we ping with 5000 bytes, 4 packets are sent. And ICMP protocol considers a datagram to be lost even when one of them fails. So the probability of the ICMP datagram loss is higher than the probability of single frame loss.\nIs this what is happening in the ping test result? We can calculate the probability of datagram loss using probability theory but let us defer to it later on and do a numerical simulation first using Monte Carlo simulation.\nMonte Carlo Simulation Monte carlo simulation is a rather fancy title for a simple simulation using random event generator, but it is quite handy and widely used. Usually Monte Carlo simulation is useful for simulating events that are truly random in nature. In a chaotic backbone network, that handles traffic stream of different kinds, we can assume the frame loss to be random.\nLet us write a short program to simulate random packet loss.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import random import numpy as np sampleCount = 100000 # total events in our simulation p = 0.03 # ethernet frame loss probability grpSize = 4 # packet count per datagram, 5000 bytes = 4 packets grpEventCount = int(sampleCount/grpSize) # datagram count # generate random packets with p% packet loss events = np.random.choice([0,1], size=sampleCount, p=[p,1-p]) # group discrete packets into a datagram grpEvents = events.reshape(grpEventCount,grpSize) # function to determine datagram loss def checkFailure(grpEvent): return (np.count_nonzero(grpEvent) \u0026lt; grpSize) # Return 1 if the success count is less than 3 # count the result failCount = 0 for grpEvent in grpEvents: failCount += checkFailure(grpEvent) print(\u0026#34;The probability of a group failure is {:.2f}%\u0026#34;.format(failCount/len(grpEvents)*100)) The probability of a group failure is 11.78% There you see! Even a 3% ethernet frame loss translates to 12% packet loss for jumbo ping test. This is same as what we observed. Now this is just a simulation with random input. Does the math agree?\nUsing Probability Theory If p is the probability of a single frame loss, (1-p) is the probability of a successful transfer. And a datagram is successful only if all of its frames are successful. So a 4 frame long ICMP datagram transmission is successful only if 4 consecutive ethernet frame transmissions are successful. The probability is (1-p)**n where n is the number of frames. To calculate the failure rate, just take its inverse.\n1 2 n = 4 1- (1-p)**n 0.11470719000000007 As expected the simulation is slightly off from the calculated probability. But it will get closer to the real figure when we increase the simulation count.\nConclusion The exactness of our calculation hinges on the assumption of random nature of packet loss. While it happened to be close to true in my case, it need not be the case all the time. The link may have a bursty load and since our ping streams are evenly spaced over time, their chances of failure may not be truly random.\nNevertheless, we should be wary of the difference between a datagram loss and ethernet loss while interpreting results. Consider the MTU of the network path while testing with different packet sizes.\n","date":"2017-11-24T08:01:28Z","permalink":"https://example.com/p/2017-11-24-using-monte-carlo-simulation-to-model-ping-test-results/","title":"Using Monte-Carlo Simulation to model ping test results"},{"content":"Often we have to discover the devices on a network. I use a very simple nmap command for performing a pingsweep.\nsudo nmap -sn \u0026lt;subnet or ip range\u0026gt;\nOn my Windows PC, I wrap it around in a batch script and place it in the search PATH. On Linux, it can be dropped in as an alias in bashrc.\nIt is handy, but not complete. I would like to have some extra information such as hostnames (collected by various means not just DNS reverse lookup), platform info etc., Such details are available in tools such as AngryIP scanner, but I don\u0026rsquo;t prefer to launch a GUI tool for single task and keep it running until the task is done.\nSo let us try to implement a similar function using nmap script. There are existing scripts in nmap which performs advanced discovery and reconnaissance, but we want something lightweight, more generic and customizable to support more protocols. Nmap scripts run on top of Nmap Scripting Engine which runs on Lua. Learning it would expand the scope of these tools from just being a capable tool to a powerful tool with limitless possibilities.\nAlthough this was my first attempt at Lua scripting, the endeavour turned out to be fairly simple. True it was not without its share of frustrations, most of which were related to wrapping my head around the way NSE (Nmap Scripting Engine) tosses the data around and lua data structures. But once you get the hang of it, it is really simple. So without much ado, let us get started.\nNSE Script structure A basic NSE script will have the following 3 sections:\nThe Head section is for meta information about the script. We need not worry about it for primer purpose but it is a good habit to put documentation info here while packaging the script for production. This section also feeds in to the NSE Documentation module (NSEDoc) which provides a consistent way to represent the meta information about our script. The RULE section determines the scope of the script. It basically acts as a filter of nmap port-scan results that gets filtered to the Action section. The rule section should contain one of the following functions: prerule() hostrule(host) portrule(host, port) postrule() The Action section is mostly the brain of the script (although rule section can also contain some of the script logic). This section contains an Action section which reads data from the nmap scanning engine and carries out the script logic. The value returned by this function is also printed on the screen and captured by other nmap output methods. Note that this script is executed iteratively over either list of hosts or a list of (host, port) tuples. The ACTION section is indicated by the presence of a function named action. Diving In So let us go ahead and create a basic script. Our script will scan the target network and fetch the hostname from NETBIOS. Create a script file with .nse (I used host-discover.nse) extension as follows:\n1 2 3 4 5 6 7 8 9 10 11 local netbios = require \u0026#34;netbios\u0026#34; local shortport = require \u0026#34;shortport\u0026#34; -- The Rule Section -- portrule = shortport.portnumber({137}, \u0026#34;udp\u0026#34;) -- The Action Section -- action = function(host,port) local nbt_status, netbios_name = netbios.get_server_name(host) return netbios_name end Now run this script against your local network as below:\nnmap --script host-discover.nse 10.1.1.0/24 -sU -p 137\nNow take a minute or two to let this sink in\u0026hellip; we just created our very own NETBIOS scanner, all in just 7 lines of functional code. There are dedicated standalone tools that performs this singular task and we managed to do it using nmap. With just a little more effort, we can add more bells and whistles to this.\nThe magic that enables this are the excellent inbuilt scanning mechanisms of nmap and hot-pluggable libraries that carry out much of the grunt. By scripting in NSE, we can tap in to this massive capabilities of nmap and automate to our needs. Let us now see how this script works.\nCode Walkthrough In this script there is no HEAD section. So we start by importing the libraries needed for our script. In Lua, modules are included using require function. And we assign the module to a local variable in order to access its namespace (i.e, call the methods that belong to the module).\n1 2 local netbios = require \u0026#34;netbios\u0026#34; local shortport = require \u0026#34;shortport\u0026#34; The purpose of these libraries will be evident in the later section.\nRULE section Next we move to the RULE section. Notice that in Lua, comments are prepended by -- sequence.\nAs mentioned earlier, RULE section acts as filter to identify hosts or ports relevant to our script. Since our script is only interested with NETBIOS query, we have to pick only the hosts that are listening on port UDP/137.\nSince this is a common check, nmap includes a \u0026lsquo;shortport\u0026rsquo; module that provides shorthand functions to check port states. shortport.portnumber is one such function which will return true only for those ports and protocols listed in its arguements. Refer to online documentation for exact syntax of this function.\n1 2 -- The Rule Section -- portrule = shortport.portnumber({137}, \u0026#34;udp\u0026#34;) NOTE: The Rule section only filters port numbers passed on from the upper layer i.e, the original nmap scan. It doesn\u0026rsquo;t trigger a port scan on its own. That is why when we launch the script, we had to specify port number explicitly (nmap -sU -p U:137 \u0026lt;host\u0026gt;). It is still possible to launch a port scan in this section, by calling nmap socket libraries but those are advanced scripting scenarios.\nACTION Section Next comes the ACTION section. Our action section here is a function that takes (host, port) as argument. It means the host object and port object are available to this function for evaluating logic. If we need other variables, they have to be declared outside this function.\nSince for NETBIOS query we only need the host our action function will take only \u0026lsquo;host\u0026rsquo; as argument. We call the get_server_name function of netbios module. From the documentation we can see that this function returns two values, query status and query result which we capture under two local variables. For our simple task, we need not check the result status and go ahead to return the name variable directly. In Lua, if a variable doesn\u0026rsquo;t exist it returns nil which is acceptable for our scenario.\n1 2 3 4 5 -- The Action Section -- action = function(host,port) local nbt_status, netbios_name = netbios.get_server_name(host) return netbios_name end This returned value is processed by nmap scripting engine and printed in the output window.\nNOTE: In Lua functions are assigned to a variable like how we assign a string or integer value to a variable. In this case, code block defining the ACTION logic is assigned to a variable called action. Lua functions are of the format foo = function ( args ) body end. It can be defined in a single line. To call the function, call the variable with argument such as foo('bar')\nWhere to go next Since this is only a basic script, we have not customized the output format at all. The hostnames when available gets printed below each host-discovered. If you notice, the print action is executed within the ACTION function whose scope is limited to one host at a time. If we need our output to be consolidated in a tabular form, we can write a postrule function, store and retrieve our findings from nmap registry. Refer to my script (work in progress) to see one way of doing it.\nI strongly recommend to try this walkthrough as well. It greatly helped me to get started with NMAP scripting and understanding the way a NSE script is structured.\nIt won\u0026rsquo;t hurt to improve your understanding of Lua programming language as well. I found this 15 minute primer to be very useful.\nLastly the best resource for advanced nmap script writing is the existing script library. There are hundreds of scripts and libraries available, study and explore them to see different ways of tackling a challenge. Good luck scripting!\n","date":"2017-10-29T12:08:28Z","permalink":"https://example.com/p/2017-10-29-emulating-angryip-scanner-with-nmap-scripting-engine-a-lua-scripting-primer/","title":"Emulating Angry IP Scanner with nmap scripting engine - A lua scripting primer"},{"content":"Recently I bought a bluetooth RS232 serial convertor. I wasn\u0026rsquo;t sure whether it would work with my Linux laptop. But it turned out to be quite simple to setup.\nPre-requisites The following packages are required:\nbluez bluez-utils byobu (optional) Bluez provides the bluetooth protocol stack (most likely shipped with the OS), bluez-utils provides the bluetoothctl utility and byobu is a wrapper around screen terminal emulator. You can also use \u0026lsquo;screen\u0026rsquo; directly. Install these using your distributions recommended procedure.\nSteps Start daemon: 1 Swanky:~$ systemctl start bluetooth Discover using bluetoothctl: 1 2 3 4 Swanky:~$ bluetoothctl [NEW] Controller \u0026lt;controller-mac-address\u0026gt; xkgt-Swanky [default] [bluetooth]# power on [bluetooth]# scan on Once you can see your device, turn off the scan and pair 1 2 [bluetooth]# scan off [bluetooth]# pair \u0026lt;device-mac-address\u0026gt; Exit blutoothctl and create serial device (Note that root privileges are required): 1 2 [bluetooth]# exit Swanky:~$ sudo rfcomm bind 0 \u0026lt;device-mac-address\u0026gt; You should now have /dev/rfcomm0. Connect to it using byobu-screen utility: 1 Swanky:~$ byobu-screen /dev/rfcomm0 Enjoy your wireless console connection!\n","date":"2017-08-24T15:41:28Z","permalink":"https://example.com/p/2017-08-24-using-a-bluetooth-serial-console-with-linux/","title":"Using a bluetooth serial console with linux"},{"content":"While Linux has proliferated extensively in the server arena in the recent past, Windows still dominates client networks. Things that we take for granted in a client environment such as DDNS (Dynamic DNS) are not as matured as they are in Windows environment. One may wonder whether the recent surge in Linux based clients such as IoT devices has changed this equation.\nSo, what does it take to make a Linux client register dynamically in a Windows environment?\nAt its basic, the entire process relies on Dynamic DNS as explained in RFC2136. In a traditional windows environment with AD, this process is taken care by client OS. Every time a Windows PC gets an IP address from DHCP server, it would send a DNS Update (Opcode = 5) request to its registered DNS server. Performed manually, this is same as typing “ipconfig /registerdns” at an elevated command prompt. This behaviour can be modified by accessing DNS section of Advanced TCP/IP settings of a network adapter.\nWhen we ask a Linux client to do the same (later I will explain how it can be configured to ask), it won’t work unless the DNS server is configured to accept “Insecure updates” (Which is a major security risk if you need to ask).\nTake a look at the capture of Linux client performing DNS update, you can see that the server comes back with a UPDATE REFUSED response.\nThis is because our DNS server is enabled with secure updates which means only authenticated clients can send update.\nThe client is expected to send a transaction signature along with the update request. There are different types of signatures such as a TSIG resource or the SIG(0) or GSS-TSIG signatures. In Windows world however, only GSS-TSIG signatures as described in RFC3645 are understood and accepted.\nLooking at a capture from a Windows PC joined to domain, one can see the Windows Device sending Update request with GSS-TSIG resource.\nGiven this background, there are 3 options available to set up DDNS for Linux based clients.\nConfigure DHCP server to perform DNS registration on behalf of the clients Join the Linux devices to AD domain and configure them to dynamically update Setup a new sub-domain running a dedicated Linux BIND server and configure DNS forwarding on Microsoft DNS server. The first option is the simplest and best resort if you just have a one-off requirement. The other two solutions are more involved but robust. To demonstrate this, let us set up a test environment like below:\nMicrosoft Active Directory environment with DNS server installed in Domain controller and a DHCP server running separately on a different host. All are running on Windows Server 2008 R2. DNS is configured to accept only Secure updates. Two Linux devices running Debian Stretch operating system. One of them will act as DNS server in one of the scenarios. The solutions we discuss should meet the following objectives:\nUpdate DNS when the device gets an IP address Perform periodic updates to DNS server to protect against expiry Fully automated with very little or no hand-coding on client devices, assume no automation tools like Puppet or Chef Scalable to hundreds or thousands of devices Point 3 is important to me since I had to work out a solution at work where we are using hundreds of Raspberry Pi’s, all booting the same image cloned on to flash disks. So, editing config files on each of them is not an option (we will come to this later).\nConfiguring DHCP server to perform DNS registration on behalf of the clients This method makes use of DHCP option 81 as defined in RFC4702, which is used to convey a client’s FQDN to a DHCP server as part of DHCP process.\nAn aside: RFC doesn’t mandate whether a DHCP server should register client’s DNS or not. It is left to site-specific policies, which may differ per the security context of the site\nThe default setting in a Microsoft DHCP server scope is as follows (Right click on scope name -\u0026gt; Properties to reach here):\nUnderstandably, this only updates to DNS server if requested by the client. What happens if we select the option to “Always dynamically update DNS A and PTR records”? Is that what we want?\nIf you trigger a DHCP request from the client, you will notice that this doesn’t work.\nThis setting merely controls whether a DHCP server should update ‘A’ record or not. The label “Always dynamically update DNS A and PTR records” is misleading since it applies only for the clients that request a DNS update. By default, a client is responsible for updating the A record and DHCP server is responsible for updating the PTR record. Selecting the second option forces DHCP server to update A record as well. But the prerequisite is that the client should request for DNS update.\nThe two options above correspond to the two cases discussed in RFC4702\nFor our Linux clients, the option we need is the last check box. Let us turn this on and trigger a DHCP request from our client.\nWhen we check the DNS server, we can see that the A record successfully is created.\nOn the capture, we can see secure DNS update message being sent from the DHCP server (Note that the DNS clients always tries insecure updates first and gets rejected by the server).\nFor a home environment, this is almost enough. But for production environments, with multiple DHCP servers, this is not enough. The problem is that, in such setup the DHCP server becomes the owner of the A and PTR records (see below). It is fine as long as the DHCP server is alive to create and remove records. But when it goes down, its peer DHCP server won’t be able to do anything about those records.\nThis link explains the issue in more detail. Let us follow the advice, create a dedicated user account for updating DNS and delete the old record with DHCP server as owner. Do not grant any extra privilege to this account. Just adding to DNSUpdateProxy group should be sufficient (Right click on IPv4 -\u0026gt; Properties -\u0026gt; Advanced).\nAs usual, let us go ahead to trigger an update.\nAs expected, new A and PTR record gets created.\nIf we check the ownership, we can find that the record is owned by DNSProxyUpdate group.\nFinally, let us discuss the option called “Name Protection” at the bottom of the dialog box.\nThis forces DHCP server to manage the entire lifecycle of your client’s A and PTR records. If you are going to let your DHCP server manage client’s A record, I don’t see any reason to keep this disabled. It will also protect you from “Name Squatting” by offline clients. RFC4701 describes the problem as:\nLet us see what it means to turn on this option. First, we keep it disabled and bring two clients online with same hostname, one after other. All is well when the first client comes online and gets an IP address 192.168.179.50.\nDNS also gets updated accordingly.\nLet us bring another Linux client online and change the hostname to same as this host. Then perform a DHCP request from this host.\nDHCP server assigns IP address 192.168.179.51 and sends an update to DNS server. Note that the DHCP server makes no fuss about two hosts sharing the same hostname. For all it knows, it could be the same host with multiple interfaces.\nOn the DNS sever side, we see that it accepts this update without any hesitation. The only problem is that this overwrites the existing record, while the client is still online. So, anyone trying to talk the first node ends up talking to the second node.\nClearly, DHCP server is not a reliable source of identity. RFC4703 briefly mentions the inability of DHCP server to provide any sort of assurance.\nLet us see what happens when we enable “Name Protection”.\nAs soon as we enable this option, first thing we notice is that all other options are greyed out. This is because, with Name Protection enabled, it is always the responsibility of DHCP server to perform both A record and PTR record updates.\nLet us wipe the slate clean, by releasing IP address from both the clients and deleting the existing DNS \u0026amp; DHCP records.\nNow when you bring the first Linux client online, you can see that the DHCP server performs a new type of record registration called DHCID.\nA new record type DHCID appears in the DNS server.\nLet us bring up the impostor and request DHCP address. It gets an IP address of 192.168.179.51.\nAs usual, DHCP server is very generous about having two hosts sharing the same hostname.\nBut no new DNS entry is created.\nLooking at the capture, we can see that the DNS registration fails with a response that RRset does not exist.\nThis message means that DHCID value calculated from the new update packet doesn’t match with any DHCID RR’s stored in the server. This behaviour is described in RFC4701.\nThis is as much as we need to know about configuring a Microsoft DHCP server to perform Dynamic DNS for Linux clients.\n","date":"2017-07-19T15:43:13Z","permalink":"https://example.com/p/2017-07-19-dynamic-registration-of-dns-for-linux-devices-in-an-active-directory-environment-with-windows-dns-server/","title":"Dynamic registration of DNS for Linux devices in an Active Directory environment with Windows DNS server"}]