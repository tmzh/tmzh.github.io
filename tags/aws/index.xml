<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>aws on Ephemeral Dance Of Electrons</title><link>https://tmzh.github.io/tags/aws/</link><description>Recent content in aws on Ephemeral Dance Of Electrons</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Thu, 16 Sep 2021 12:00:00 +0000</lastBuildDate><atom:link href="https://tmzh.github.io/tags/aws/index.xml" rel="self" type="application/rss+xml"/><item><title>Using Hugging Face Transformers on AWS Sagemaker</title><link>https://tmzh.github.io/post/2021-09-16-using-hugging-face-transformers-on-aws-sagemaker/</link><pubDate>Thu, 16 Sep 2021 12:00:00 +0000</pubDate><guid>https://tmzh.github.io/post/2021-09-16-using-hugging-face-transformers-on-aws-sagemaker/</guid><description>&lt;p>In July 2021, AWS and Hugging Face announced collaboration to make Hugging Face a first party framework within SageMaker. Earlier, you had to use PyTorch container and install packages manually to do this. With the new Hugging Face Deep Learning Containers (DLC) availabe in Amazon SageMaker, the process of training and deploying models is greatly simplified.&lt;/p>
&lt;p>In this post, we will go through a high level overview of Hugging Face Transformers library before looking at how to use the newly announced Hugging Face DLCs within Sagemaker.&lt;/p></description></item></channel></rss>