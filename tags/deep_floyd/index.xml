<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>deep_floyd on Ephemeral Dance Of Electrons</title><link>https://tmzh.github.io/tags/deep_floyd/</link><description>Recent content in deep_floyd on Ephemeral Dance Of Electrons</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 22 Jan 2024 12:00:00 +0000</lastBuildDate><atom:link href="https://tmzh.github.io/tags/deep_floyd/index.xml" rel="self" type="application/rss+xml"/><item><title>Generating Visual Illusions Using Diffusion models</title><link>https://tmzh.github.io/post/2024-01-22-running-deep-floyd-with-limited-memory/</link><pubDate>Mon, 22 Jan 2024 12:00:00 +0000</pubDate><guid>https://tmzh.github.io/post/2024-01-22-running-deep-floyd-with-limited-memory/</guid><description>Introduction By now, many of us may be familiar with text-to-image models like Midjourney, DALLÂ·E 3, StableDiffusion etc., Recently, I came across an interesting project called Visual Anagrams that utilizes text-to-image model to generate picture illusions. This project enables us to input two different text prompts, and the model generates pictures that match the prompts under various transformations, such as flips, rotations, or pixel permutations. Growing up, I had a nerdy fascination with illusions and ambigrams, so I was thrilled to give this a try.</description></item></channel></rss>