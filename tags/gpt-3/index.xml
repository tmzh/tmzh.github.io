<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>gpt-3 on Ephemeral Dance Of Electrons</title><link>https://tmzh.github.io/tags/gpt-3/</link><description>Recent content in gpt-3 on Ephemeral Dance Of Electrons</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 20 Sep 2020 12:00:00 +0000</lastBuildDate><atom:link href="https://tmzh.github.io/tags/gpt-3/index.xml" rel="self" type="application/rss+xml"/><item><title>GPT-3 and prospects of Artificial General Intelligence</title><link>https://tmzh.github.io/post/2020-09-20-gpt-3-and-prospects-of-artificial-general-intelligence/</link><pubDate>Sun, 20 Sep 2020 12:00:00 +0000</pubDate><guid>https://tmzh.github.io/post/2020-09-20-gpt-3-and-prospects-of-artificial-general-intelligence/</guid><description>&lt;p>Last year OpenAI released the Generative Pre-trained Transformer 2 (GPT-2) model. GPT-2 was a language model with 1.5 billion parameters, trained on 8 million web pages. It generated quite a buzz as it could generate coherent text, comprehend paragraphs, answer questions, and summarize text and do all sorts of smart stuff&amp;hellip; all without any task-specific learning. OpenAI even deemed the model too dangerous to release but eventually ended up releasing them.&lt;/p>
&lt;p>In May 2020, OpenAI released their follow-up GPT-3 model which took the game several notches higher. They trained it with 175 billion parameters, using close to half-a-trillion tokens. The model and its weights alone would take up 300GB VRAM. This is a drastic increase in scale and complexity, anyway you look at it. So what can a huge model like this achieve and why has it reinvigorated the talks ?&lt;/p>
&lt;p>&lt;img src="https://tmzh.github.io/images/2020-09-26-gpt-3-training-size.png" alt="GPT-3 Training Size">&lt;/p></description></item></channel></rss>