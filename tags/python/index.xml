<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>python on Ephemeral Dance Of Electrons</title><link>https://tmzh.github.io/tags/python/</link><description>Recent content in python on Ephemeral Dance Of Electrons</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sat, 02 Mar 2019 12:00:00 +0000</lastBuildDate><atom:link href="https://tmzh.github.io/tags/python/index.xml" rel="self" type="application/rss+xml"/><item><title>Using OpenCV object detection to keep kids away from TV</title><link>https://tmzh.github.io/post/2019-03-02-using-opencv-object-detection-to-keep-kids-away-from-tv/</link><pubDate>Sat, 02 Mar 2019 12:00:00 +0000</pubDate><guid>https://tmzh.github.io/post/2019-03-02-using-opencv-object-detection-to-keep-kids-away-from-tv/</guid><description>&lt;blockquote>
&lt;p>Give me a dozen healthy infants, well formed, and my own specified world to bring them up in and I’ll guarantee to take any one at random and train him to become any type of specialist I might select—doctor, lawyer, artist, merchant-chief and yes, even beggar-man thief, regardless of his talents, penchants, tendencies, abilities, vocations, and race of his ancestors.&lt;/p>
&lt;/blockquote>
&lt;p>This was John Watson, one of the founders of &lt;a href="https://www.wikiwand.com/en/Behaviorism">Behaviorism&lt;/a>, writing around 1925. He believed that human behavior is completely malleable and that it can be shaped into anything given the right environment. While I don&amp;rsquo;t harbor any grand objectives or sinister experiments like Watson did, I do hope to be able to teach my kids good habits using controlled environments. For instance, my two year old kids started developing the habit of getting too close to the TV. I didn&amp;rsquo;t want to use force or impose restrictions on them, so I thought I could use technology to discourage them from getting too close to TV.&lt;/p></description></item><item><title>Predicting the playing role of a cricketer using Machine Learning (Part 1)</title><link>https://tmzh.github.io/post/2018-04-23-predicting-the-playing-role-of-a-cricketer-using-machine-learning-part-1/</link><pubDate>Mon, 23 Apr 2018 12:00:00 +0000</pubDate><guid>https://tmzh.github.io/post/2018-04-23-predicting-the-playing-role-of-a-cricketer-using-machine-learning-part-1/</guid><description>&lt;p>In this project, we will apply Machine Learning techniques to predict whether a particular cricket player is a batsman or bowler based on his career stats. First we will use Deep Neural Networks (DNN) model and later compare the results with a simpler classifier algorithm such as Random Forest Classifier.&lt;/p></description></item><item><title>Using Constraint Programming Tools to solve an ancient Chinese math puzzle</title><link>https://tmzh.github.io/post/2017-12-09-using-constraint-programming-to-solve-an-ancient-chinese-math-puzzle/</link><pubDate>Sat, 09 Dec 2017 12:00:00 +0000</pubDate><guid>https://tmzh.github.io/post/2017-12-09-using-constraint-programming-to-solve-an-ancient-chinese-math-puzzle/</guid><description>&lt;p>&lt;a href="https://www.wikiwand.com/en/Constraint_programming">Constraint programming (CP)&lt;/a> is a subset of Operations Research (OR) where our task is to identify all feasible solutions to a given problem that satisfies a set of constraints. This is different from an optimization problem, where an objective function is defined and we arrive at solutions that either maximizes or minimizes an objective function.&lt;/p>
&lt;p>CP is mostly well suited for solving logic puzzles, since most logic puzzles are based on constraints and enumerating feasible solutions. But apart from recreational maths, CP also has a lot of practical applications in Scheduling, Resource allocation, Manufacturing etc.,&lt;/p></description></item><item><title>Using Monte-Carlo Simulation to model ping test results</title><link>https://tmzh.github.io/post/2017-11-24-using-monte-carlo-simulation-to-model-ping-test-results/</link><pubDate>Fri, 24 Nov 2017 08:01:28 +0000</pubDate><guid>https://tmzh.github.io/post/2017-11-24-using-monte-carlo-simulation-to-model-ping-test-results/</guid><description>&lt;p>Recently we had a cabling issue in our core infrastructure which caused around 3 to 12% packet loss across few IP streams. One of my colleagues made an interesting observation that when he tried to ping with large packet size (5000 bytes) the packet loss rose up as high as 40%. In his opinion, that meant some applications were experiencing up to 40% packet loss. I seldom do large packet ping tests unless I am troubleshooting MTU issues, so to me this observation was interesting.&lt;/p>
&lt;p>At the outset, it may look like an aggravated problem. Yet you know that your network path MTU doesn&amp;rsquo;t support jumbo frames end-to-end. If so, why is there a difference in packet loss rate when you ping with large datagrams? The answer is not too obvious. The important thing to note is that a ping test result is not a measure of ethernet frame loss but ICMP datagram loss. In most cases (when the ICMP datagram is smaller than ethernet MTU) both are the same. But why do large ICMP datagrams have higher loss percentage than individual ethernet frames? Enter Math.&lt;/p></description></item></channel></rss>