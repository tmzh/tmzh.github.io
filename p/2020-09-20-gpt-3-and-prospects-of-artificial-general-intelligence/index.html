<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Last year OpenAI released the Generative Pre-trained Transformer 2 (GPT-2) model. GPT-2 was a language model with 1.5 billion parameters, trained on 8 million web pages. It generated quite a buzz as it could generate coherent text, comprehend paragraphs, answer questions, and summarize text and do all sorts of smart stuff&mldr; all without any task-specific learning. OpenAI even deemed the model too dangerous to release but eventually ended up releasing them.\nIn May 2020, OpenAI released their follow-up GPT-3 model which took the game several notches higher. They trained it with 175 billion parameters, using close to half-a-trillion tokens. The model and its weights alone would take up 300GB VRAM. This is a drastic increase in scale and complexity, anyway you look at it. So what can a huge model like this achieve and why has it reinvigorated the talks ?\n"><title>GPT-3 and prospects of Artificial General Intelligence</title>
<link rel=canonical href=https://example.com/p/2020-09-20-gpt-3-and-prospects-of-artificial-general-intelligence/><link rel=stylesheet href=/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="GPT-3 and prospects of Artificial General Intelligence"><meta property='og:description' content="Last year OpenAI released the Generative Pre-trained Transformer 2 (GPT-2) model. GPT-2 was a language model with 1.5 billion parameters, trained on 8 million web pages. It generated quite a buzz as it could generate coherent text, comprehend paragraphs, answer questions, and summarize text and do all sorts of smart stuff&mldr; all without any task-specific learning. OpenAI even deemed the model too dangerous to release but eventually ended up releasing them.\nIn May 2020, OpenAI released their follow-up GPT-3 model which took the game several notches higher. They trained it with 175 billion parameters, using close to half-a-trillion tokens. The model and its weights alone would take up 300GB VRAM. This is a drastic increase in scale and complexity, anyway you look at it. So what can a huge model like this achieve and why has it reinvigorated the talks ?\n"><meta property='og:url' content='https://example.com/p/2020-09-20-gpt-3-and-prospects-of-artificial-general-intelligence/'><meta property='og:site_name' content='Ephemeral Dance Of Electrons'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='gpt-3'><meta property='article:tag' content='philosophy'><meta property='article:published_time' content='2020-09-20T12:00:00+00:00'><meta property='article:modified_time' content='2020-09-20T12:00:00+00:00'><meta property='og:image' content='https://example.com/images/2020-09-26-meta-learning.png'><meta name=twitter:title content="GPT-3 and prospects of Artificial General Intelligence"><meta name=twitter:description content="Last year OpenAI released the Generative Pre-trained Transformer 2 (GPT-2) model. GPT-2 was a language model with 1.5 billion parameters, trained on 8 million web pages. It generated quite a buzz as it could generate coherent text, comprehend paragraphs, answer questions, and summarize text and do all sorts of smart stuff&mldr; all without any task-specific learning. OpenAI even deemed the model too dangerous to release but eventually ended up releasing them.\nIn May 2020, OpenAI released their follow-up GPT-3 model which took the game several notches higher. They trained it with 175 billion parameters, using close to half-a-trillion tokens. The model and its weights alone would take up 300GB VRAM. This is a drastic increase in scale and complexity, anyway you look at it. So what can a huge model like this achieve and why has it reinvigorated the talks ?\n"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://example.com/images/2020-09-26-meta-learning.png'></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"light")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=images/ars_longa.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>ðŸŽ“</span></figure><div class=site-meta><h1 class=site-name><a href=/>Ephemeral Dance Of Electrons</a></h1><h2 class=site-description>Ars Longa, vita brevis</h2></div></header><ol class=menu-social><li><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://twitter.com target=_blank title=Twitter rel=me><svg class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#learning-how-to-learn>Learning how to learn</a></li><li><a href=#is-language-enough-to-model-reality>Is Language enough to model reality?</a></li><li><a href=#the-upper-bound-of-the-effectiveness-of-scale>The upper-bound of the effectiveness of scale</a></li><li><a href=#what-do-the-critics-say>What do the critics say?</a><ol><li><a href=#it-is-not-as-good-as-sota>It is not as good as SOTA</a></li><li><a href=#it-is-just-an-autocomplete>It is just an autocomplete</a></li><li><a href=#it-has-already-seen-the-data-it-is-predicting-on>It has already seen the data it is predicting on</a></li></ol></li><li><a href=#conclusion>Conclusion</a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/2020-09-20-gpt-3-and-prospects-of-artificial-general-intelligence/><img src=/images/2020-09-26-meta-learning.png loading=lazy alt="Featured image of post GPT-3 and prospects of Artificial General Intelligence"></a></div><div class=article-details><header class=article-category><a href=/categories/artificial-intelligence/>Artificial Intelligence</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/2020-09-20-gpt-3-and-prospects-of-artificial-general-intelligence/>GPT-3 and prospects of Artificial General Intelligence</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>2020-09-20</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>8 minute read</time></div></footer></div></header><section class=article-content><p>Last year OpenAI released the Generative Pre-trained Transformer 2 (GPT-2) model. GPT-2 was a language model with 1.5 billion parameters, trained on 8 million web pages. It generated quite a buzz as it could generate coherent text, comprehend paragraphs, answer questions, and summarize text and do all sorts of smart stuff&mldr; all without any task-specific learning. OpenAI even deemed the model too dangerous to release but eventually ended up releasing them.</p><p>In May 2020, OpenAI released their follow-up GPT-3 model which took the game several notches higher. They trained it with 175 billion parameters, using close to half-a-trillion tokens. The model and its weights alone would take up 300GB VRAM. This is a drastic increase in scale and complexity, anyway you look at it. So what can a huge model like this achieve and why has it reinvigorated the talks ?</p><p><img src=/images/2020-09-26-gpt-3-training-size.png loading=lazy alt="GPT-3 Training Size"></p><p>GPT-3 can write poetry, mimic <a class=link href=https://www.gwern.net/GPT-3#literary-parodies target=_blank rel=noopener>the writing style of personalities</a>. It also performs better than an average college applicant in <a class=link href="https://arxiv.org/pdf/2005.14165.pdf#page=25" target=_blank rel=noopener>SAT analogy problems</a>, generates <a class=link href=https://medium.com/@aidungeon/ai-dungeon-dragon-model-upgrade-7e8ea579abfe target=_blank rel=noopener>cohesive stories</a>, writes news articles that are <a class=link href="https://arxiv.org/pdf/2005.14165.pdf#page=27" target=_blank rel=noopener>hard to distinguish</a> from a human-written article. You can even ask it to do system admin jobs in natural language and it will come up with shell commands and execute them- like a seasoned sysadmin.</p><p><img src=/images/2020-09-26-nlsh.png loading=lazy alt="Natural Language Shell"></p><h2 id=learning-how-to-learn>Learning how to learn</h2><p>One of the amazing aspects of the GPT-3 model is that it doesn&rsquo;t need any task-specific fine-tuning and yet achieves decent results on many of the Natural Language Processing (NLP) benchmarks and tasks. OpenAI&rsquo;s claim is that if an NLP model is sufficiently complex and if it has been trained on a large volume of data, it can learn to do new tasks only by looking at few examples prompts i.e, the model has <a class=link href=https://www.gwern.net/newsletter/2020/05#meta-learning target=_blank rel=noopener>learned the ability to learn</a> new tasks on the go. They call it &ldquo;Few Shot Learning&rdquo;. You can see that in action in the chart below.</p><p><img src=/images/2020-09-26-meta-learning.png loading=lazy alt=Meta-learning></p><p>In the above chart, you can see when the GPT-3 175B model is given with few examples, the accuracy rate shoots up compared to a poor learner (1.3B or even the 13B parameter model). The ability to learn is one of the defining characteristics of Artificial General Intelligence (AGI) and now you can understand why GPT-3 is generating the buzz.</p><h2 id=is-language-enough-to-model-reality>Is Language enough to model reality?</h2><p>GPT-3 has no direct exposure to reality, except via a large corpus of text. Yet the knowledge imparted from this large volume of text allows GPT-3 to reason (arguably) about the physical world as seen in the <a class=link href=https://www.lesswrong.com/posts/L5JSMZQvkBAx9MD5A/to-what-extent-is-gpt-3-capable-of-reasoning target=_blank rel=noopener>example</a> below</p><p><img src=/images/2020-09-26-reasoning.png loading=lazy alt="Reasoning about the physical world"></p><p>In fact, GPT-3 also demonstrates surprising ability outside the traditional NLP domain. It can do certain non-linguistic tasks like simple arithmetics. This begs the question, whether modeling language alone is sufficient to form an understanding of reality? We use language to describe our world. So a relation between words should correspond to a relation between the real world entities represented by those words. <a class=link href=https://deponysum.com/2020/01/16/recent-advances-in-natural-language-processing-some-woolly-speculations/ target=_blank rel=noopener>Some</a> reason that ML models are transitive (i.e, if X models Y and Y models Z, then X models Z) and since language itself is an approximate model of the world, a language model should be an approximate representation of the world.</p><h2 id=the-upper-bound-of-the-effectiveness-of-scale>The upper-bound of the effectiveness of scale</h2><p>Another surprising outcome of GPT-3 is that the performance has not tapered as the model got more complex. When GPT-2 got released most people speculated that we will soon hit the upper bound on the returns from further scaling up the model. But that hasn&rsquo;t happened with GPT-3. With a more complex model, GPT-3 has shown big improvements in its performance and seems to get better at learning new tasks (see the widening gap between zero-shot and few-shot benchmarks).</p><p>As recently as last year, training a model with 175B seemed far off into the future. The Human brain is estimated to contain around 100 trillion neurons, how long before we reach that figure and what would AI look like as it approaches the figure?</p><p><img src=/images/2020-09-26-scaling-hypothesis.png loading=lazy alt="Scaling hypothesis"></p><blockquote><p>Now I agree that I am playing to the gallery here and being sensational. Biological neurons and ML neurons are quite different. A CNN model can do object detection (in fact better than us) but it doesn&rsquo;t share the same structure with our visual cortex. This is why CNN outperforms humans in certain tasks like detecting anomalies in medical images, yet they can get <a class=link href=https://www.iflscience.com/technology/ai-camera-ruins-soccar-game-for-fans-after-mistaking-referees-bald-head-for-ball/ target=_blank rel=noopener>confused between a ball and a referee&rsquo;s head</a>.</p></blockquote><p>Not everyone buys the scaling hypothesis. Detractors like Marcus claim that just because we know how to stack ladders doesn&rsquo;t mean that we can build ladders to the moon. The argument is loaded with an assumption. In the case of ladders to the moon, we know the governing rules and the limits of possibility. But to extend the metaphor to AGI, we don&rsquo;t understand our own intelligence well enough and we cannot say for sure that the chasm between a statistical model and causal reasoning is impervious. Like the expression <a class=link href=https://www.wikiwand.com/en/Turtles_all_the_way_down target=_blank rel=noopener>Turtles all the way down</a>, perhaps our intelligence is also made of statistical models stacked all the way down.</p><h2 id=what-do-the-critics-say>What do the critics say?</h2><h3 id=it-is-not-as-good-as-sota>It is not as good as SOTA</h3><p>To be correct, GPT-3 is not the best model out there. On most NLP tasks & benchmarks, other State-of-the-art (SOTA) algorithms perform better than GPT-3. But those criticisms are missing the point.</p><ol><li>Most of the SOTA algorithms are fine-tuned for specific tasks whereas GPT-3 is not. But GPT-3 has still learned to do these tasks by understanding (in a specific sense) the English language, somewhat similar to how humans learn language tasks. And this, not the benchmark performance, is the most talked-about aspect of GPT-3.</li><li>SOTA also performs better than humans on many specific tasks like object recognition, mastering games like Chess and Go. But that doesn&rsquo;t weigh against the ability of our general intelligence. General intelligence, in a sense, always has to underfit to have wider applicability.</li></ol><p>One should also note that, in spite of being a general-purpose model, GPT-3 betters fine-tuned SOTA in some benchmarks (PhysicalQA, LAMBADA, Penn Tree Bank to name a few)</p><h3 id=it-is-just-an-autocomplete>It is just an autocomplete</h3><p>Another common criticism is that GPT-3 is just a sophisticated <a class=link href="https://www.forbes.com/sites/robtoews/2020/07/19/gpt-3-is-amazingand-overhyped/?sh=5907df881b1c" target=_blank rel=noopener>text prediction engine</a>. It doesn&rsquo;t understand what those words mean. But this is a hasty criticism based on an <a class=link href=https://plato.stanford.edu/entries/chinese-room/#IntuRepl target=_blank rel=noopener>intuitive definition</a> understanding. Our intuitions about intelligence, understanding, and meaning are not precise enough for such claims.</p><p>Let us say our brain is a sophisticated lookup dictionary populated by past experiences and culturally accumulated knowledge. Suppose a non-human entity possesses a similar dictionary but populated using a different process (by training on text corpus for example). Can it claim to have an understanding? This was the crux of the argument laid down by Searle in his <a class=link href=https://plato.stanford.edu/entries/chinese-room/#Over target=_blank rel=noopener>Chinese room thought experiment</a>.</p><p>Searle claims that looking up a dictionary doesn&rsquo;t represent understanding, but not everyone agrees with that take. On a similar note, if we dismiss GPT-3&rsquo;s output as a mere statistical calculation, there is no telling that our brain&rsquo;s neuro-biological process is any different.</p><h3 id=it-has-already-seen-the-data-it-is-predicting-on>It has already seen the data it is predicting on</h3><p>This is a more serious allegation than the ones above. Since GPT-3 was trained on a huge volume of data, there is a chance that it has already seen the inputs and it is simply recalling them from memory. For example, Yannic Kilcher in his <a class=link href=https://youtu.be/SY5PvZrJhLE target=_blank rel=noopener>video</a> suspects that it can do arithmetic predictions because the model is likely to have seen the same data in the training dataset. OpenAI team claims to have done sufficient deduplication to remove the testing dataset from the training data. But given the volume of data, their deduplication is only <a class=link href="https://arxiv.org/pdf/2005.14165.pdf#page=25" target=_blank rel=noopener>optimistic</a>.</p><p>But there is evidence that transformer models can indeed generate answers that it has not seen before. Karpathy trained a <a class=link href=https://github.com/karpathy/minGPT target=_blank rel=noopener>mini GPT model</a> exclusively on synthetic data and it was already able to do 2 digit arithmetic. In this case, there is a clear separation between the training set and the validation set. So to conclude, the results we see from GPT-3 is not impossible and need not necessarily come out of a training data set corruption.</p><h2 id=conclusion>Conclusion</h2><p>All these comparisons with AGI don&rsquo;t mean we should haphazardly claim that GPT-3 is an AGI or that it definitively proves that AGI is possible. However, there are few interesting observations from the results of GPT-3:</p><ol><li>It shows that NLP models can be a shortcut to AGI as they have the ability to indirectly model the physical world.</li><li>The performance of the GPT model continues to scale with the model size and hasn&rsquo;t tapered off yet. So there is the promise that bigger models can be more intelligent.</li><li>At higher model complexities, interesting behaviors seem to emerge, such as the ability to learn new tasks, perform non-linguistic tasks, etc.,</li></ol><p>The year 2018 has been called the <a class=link href=https://thegradient.pub/nlp-imagenet/ target=_blank rel=noopener>ImageNet moment</a> for NLP and we can see why. The amount of progress made in NLP during the last couple of years is staggering and there is continued interest and investments in the domain. And mastering the NLP domain is closely linked with mastering General Intelligence.</p><p>There are many who believe that intelligence is somewhat innate to biological processes and cannot be reduced to mathematical models. There are others who believe that it is possible. In a way, this is just another incarnation of the nature vs nurture debate raging in the philosophical world for centuries. Except for this time, we are not relying on thought experiments. We get to do real experiments and see the answers in reality. Whichever side of the fence one sits on, the world waits with bated breath as the story unfolds.</p></section><footer class=article-footer><section class=article-tags><a href=/tags/gpt-3/>Gpt-3</a>
<a href=/tags/philosophy/>Philosophy</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/2023-03-15-gpt-4-stable-diffusion-and-beyond/><div class=article-image><img src=/images/2020-09-26-meta-learning.png loading=lazy data-key=2023-03-15-gpt-4-stable-diffusion-and-beyond data-hash=/images/2020-09-26-meta-learning.png></div><div class=article-details><h2 class=article-title>GPT-4, Stable Diffusion, and Beyond: How Generative AI Will Shape Human Society</h2></div></a></article></div></div></aside><div class=disqus-container></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2024 Ephemeral Dance Of Electrons</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.29.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>