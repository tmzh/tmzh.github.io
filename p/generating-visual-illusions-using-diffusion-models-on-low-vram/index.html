<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Introduction By now, many of us may be familiar with text-to-image models like Midjourney, DALL路E 3, StableDiffusion etc., Recently, I came across an interesting project called Visual Anagrams that utilizes text-to-image model to generate picture illusions. This project enables us to input two different text prompts, and the model generates pictures that match the prompts under various transformations, such as flips, rotations, or pixel permutations. Growing up, I had a nerdy fascination with illusions and ambigrams, so I was thrilled to give this a try.\n"><title>Generating Visual Illusions Using Diffusion Models On Low VRAM</title>
<link rel=canonical href=https://example.com/p/generating-visual-illusions-using-diffusion-models-on-low-vram/><link rel=stylesheet href=/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="Generating Visual Illusions Using Diffusion Models On Low VRAM"><meta property='og:description' content="Introduction By now, many of us may be familiar with text-to-image models like Midjourney, DALL路E 3, StableDiffusion etc., Recently, I came across an interesting project called Visual Anagrams that utilizes text-to-image model to generate picture illusions. This project enables us to input two different text prompts, and the model generates pictures that match the prompts under various transformations, such as flips, rotations, or pixel permutations. Growing up, I had a nerdy fascination with illusions and ambigrams, so I was thrilled to give this a try.\n"><meta property='og:url' content='https://example.com/p/generating-visual-illusions-using-diffusion-models-on-low-vram/'><meta property='og:site_name' content='Ephemeral Dance Of Electrons'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='deep_floyd'><meta property='article:tag' content='generative_ai'><meta property='article:published_time' content='2024-01-22T12:00:00+00:00'><meta property='article:modified_time' content='2024-01-22T12:00:00+00:00'><meta name=twitter:title content="Generating Visual Illusions Using Diffusion Models On Low VRAM"><meta name=twitter:description content="Introduction By now, many of us may be familiar with text-to-image models like Midjourney, DALL路E 3, StableDiffusion etc., Recently, I came across an interesting project called Visual Anagrams that utilizes text-to-image model to generate picture illusions. This project enables us to input two different text prompts, and the model generates pictures that match the prompts under various transformations, such as flips, rotations, or pixel permutations. Growing up, I had a nerdy fascination with illusions and ambigrams, so I was thrilled to give this a try.\n"></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"light")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=images/ars_longa.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji></span></figure><div class=site-meta><h1 class=site-name><a href=/>Ephemeral Dance Of Electrons</a></h1><h2 class=site-description>Ars Longa, vita brevis</h2></div></header><ol class=menu-social><li><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://twitter.com target=_blank title=Twitter rel=me><svg class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#deepfloyd-if-model-memory-requirements-and-optimization>DeepFloyd IF Model: Memory Requirements and Optimization</a></li></ol><ol><li><a href=#import-and-setup-what-we-need>Import and setup what we need</a></li><li><a href=#load-textencoder-model>Load TextEncoder Model</a><ol><li><a href=#addendum>Addendum</a></li></ol></li><li><a href=#create-text-embeddings>Create text embeddings</a></li><li><a href=#main-diffusion-process>Main Diffusion Process</a></li><li><a href=#generate-image>Generate Image</a></li><li><a href=#results>Results</a></li><li><a href=#more-examples>More Examples</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/generative-ai/>Generative AI</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/generating-visual-illusions-using-diffusion-models-on-low-vram/>Generating Visual Illusions Using Diffusion Models On Low VRAM</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>2024-01-22</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>5 minute read</time></div></footer></div></header><section class=article-content><h1 id=introduction>Introduction</h1><p>By now, many of us may be familiar with text-to-image models like Midjourney, DALL路E 3, StableDiffusion etc., Recently, I came across an interesting project called Visual Anagrams that utilizes text-to-image model to generate picture illusions. This project enables us to input two different text prompts, and the model generates pictures that match the prompts under various transformations, such as flips, rotations, or pixel permutations. Growing up, I had a nerdy fascination with illusions and ambigrams, so I was thrilled to give this a try.</p><div class=table-wrapper><table><thead><tr><th></th><th></th><th></th></tr></thead><tbody><tr><td><img src=/images/2024-01-28-waterfall.deer.mp4-output.gif loading=lazy alt=animation></td><td><img src=/images/2024-01-29-rotate_cw.oil.painting.houses.medieval.village.ship.ocean.gif loading=lazy alt=animation></td><td><img src=/images/2024-01-28-rotate_180.pop.art.wolverine.joker.gif loading=lazy alt=animation></td></tr><tr><td><img src=/images/2024-01-28-line-drawing-old-man-girl.gif loading=lazy alt=animation></td><td><img src=/images/2024-01-28-square_hinge.oil.painting.Medieval.village.scene.with.busy.gif loading=lazy alt=animation></td><td><img src=/images/2024-01-29-negate.photo.woman.man.gif loading=lazy alt=animation></td></tr><tr><td><img src=/images/2024-01-28-jigsaw.oil.painting.classroom.playground.gif loading=lazy alt=animation></td><td><img src=/images/2024-01-28-rotate_180.line.drawing.cat.bunny.gif loading=lazy alt=animation></td><td><img src=/images/2024-01-29-rotate_180.oil.painting.forest.fire.truck.gif loading=lazy alt=animation></td></tr></tbody></table></div><h2 id=deepfloyd-if-model-memory-requirements-and-optimization>DeepFloyd IF Model: Memory Requirements and Optimization</h2><p>Behind the scenes, Visual Anagrams utilizes the DeepFloyd IF model, which takes a unique approach to Stable diffusion. Unlike StableDiffusion which performs denoising in a latent space, DeepFloyd IF operates directly in the pixel space. This approach enables the model to better align with text and generate legible images, addressing a challenge faced by Stable Diffusion.</p><p>However, these advantages come at a cost of significantly higher memory requirements. DeepFloyd IF is a modular model composed of a frozen text encoder and three cascaded pixel diffusion modules. Running the model in full float32 precision would require at least 37GB of memory.</p><figure><img src=/images/2024-01-28-deep-floyd-if-scheme.jpg alt="DeepFloyd-IF model card" width=80%><figcaption><i>source: <a href=https://huggingface.co/DeepFloyd/IF-I-XL-v1.0>DeepFloyd-IF model card</a></i></figcaption></figure><p>Fortunately, it is possible to run this model on Google Colab or even on consumer hardware for free. The Diffusers API from HuggingFace allows us to load individual components modularly, reducing the memory requirements by loading components selectively.</p><h1 id=inference-process>Inference process</h1><h2 id=import-and-setup-what-we-need>Import and setup what we need</h2><p>First let us install the dependencies and a copy of visual anagrams repo.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=err>!</span> <span class=n>pip</span> <span class=n>install</span> <span class=o>-</span><span class=n>q</span>
</span></span><span class=line><span class=cl>    <span class=n>diffusers</span>
</span></span><span class=line><span class=cl>    <span class=n>transformers</span>
</span></span><span class=line><span class=cl>    <span class=n>safetensors</span>
</span></span><span class=line><span class=cl>    <span class=n>sentencepiece</span>
</span></span><span class=line><span class=cl>    <span class=n>accelerate</span>
</span></span><span class=line><span class=cl>    <span class=n>bitsandbytes</span>
</span></span><span class=line><span class=cl>    <span class=n>einops</span>
</span></span><span class=line><span class=cl>    <span class=n>mediapy</span>
</span></span><span class=line><span class=cl>    <span class=n>accelerate</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=err>!</span><span class=n>pip</span> <span class=n>install</span> <span class=o>-</span><span class=n>q</span> <span class=n>git</span><span class=o>+</span><span class=n>https</span><span class=p>:</span><span class=o>//</span><span class=n>github</span><span class=o>.</span><span class=n>com</span><span class=o>/</span><span class=n>dangeng</span><span class=o>/</span><span class=n>visual_anagrams</span><span class=o>.</span><span class=n>git</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=load-textencoder-model>Load TextEncoder Model</h2><p>The TextEncoder model used in DeepFloyd-IF is <code>T5</code>. To begin, we load this <code>T5</code> model in half-precision (fp16) and utilize the <code>device_map</code> flag to enable transformers to offload model layers to either CPU or disk. This reduces the memory requirements by more than half. For more information on device_map, refer to the transformers <a class=link href=https://huggingface.co/docs/accelerate/usage_guides/big_modeling#designing-a-device-map target=_blank rel=noopener>documentation</a>.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>T5EncoderModel</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>text_encoder</span> <span class=o>=</span> <span class=n>T5EncoderModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;DeepFloyd/IF-I-L-v1.0&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>subfolder</span><span class=o>=</span><span class=s2>&#34;text_encoder&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>variant</span><span class=o>=</span><span class=s2>&#34;fp16&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float16</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=addendum>Addendum</h3><p>To further reduce memory utilization, we can also load the same <code>T5</code> model using 8-bit quantization. Transformers directly supports bitsandbytes through the load_in_8bit flag. Set the variant=&ldquo;8bit&rdquo; flag to download pre-quantized weights. This allows loading the text encoders in as little as 8GB of memory.</p><h2 id=create-text-embeddings>Create text embeddings</h2><p>Next, we need to generate embeddings for the two prompts that describe the visual illusions. DiffusionPipeline from HuggingFace Diffusers library contains methods to load models necessary for running diffusion networks. We can override the individual models used by changing the keyword arguments to <code>from_pretrained</code>. In this case, we pass the previously instantiated <code>text_encoder</code> for the text_encoder argument and <code>None</code> for the unet argument to avoid loading the UNet into memory, enabling us to load only the necessary models to run the text embedding portion of the diffusion process.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>diffusers</span> <span class=kn>import</span> <span class=n>DiffusionPipeline</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>pipe</span> <span class=o>=</span> <span class=n>DiffusionPipeline</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;DeepFloyd/IF-I-L-v1.0&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>text_encoder</span><span class=o>=</span><span class=n>text_encoder</span><span class=p>,</span> <span class=c1># pass the previously instantiated text encoder</span>
</span></span><span class=line><span class=cl>    <span class=n>unet</span><span class=o>=</span><span class=kc>None</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>We can now use this pipeline to encode the two prompts. The prompts need to be concatenated for the illusion.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Feel free to change me:</span>
</span></span><span class=line><span class=cl><span class=n>prompts</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>      <span class=s1>&#39;an oil painting of a deer&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=s1>&#39;an oil painting of a waterfall&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Embed prompts using the T5 model</span>
</span></span><span class=line><span class=cl><span class=n>prompt_embeds</span> <span class=o>=</span> <span class=p>[</span><span class=n>pipe</span><span class=o>.</span><span class=n>encode_prompt</span><span class=p>(</span><span class=n>prompt</span><span class=p>)</span> <span class=k>for</span> <span class=n>prompt</span> <span class=ow>in</span> <span class=n>prompts</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>prompt_embeds</span><span class=p>,</span> <span class=n>negative_prompt_embeds</span> <span class=o>=</span> <span class=nb>zip</span><span class=p>(</span><span class=o>*</span><span class=n>prompt_embeds</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>prompt_embeds</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>(</span><span class=n>prompt_embeds</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>negative_prompt_embeds</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>(</span><span class=n>negative_prompt_embeds</span><span class=p>)</span>  <span class=c1># These are just null embeds</span>
</span></span></code></pre></td></tr></table></div></div><p>Flush to free memory for the next stages.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>gc</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>flush</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>gc</span><span class=o>.</span><span class=n>collect</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>empty_cache</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>del</span> <span class=n>text_encoder</span>
</span></span><span class=line><span class=cl><span class=k>del</span> <span class=n>pipe</span>
</span></span><span class=line><span class=cl><span class=n>flush</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=main-diffusion-process>Main Diffusion Process</h2><p>With the available GPU memory, we can reload the DiffusionPipeline using only the UNet to execute the main diffusion process. Note that once again we are loading the weights in 16-bit floating point format using the variant and torch_dtype keyword arguments.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>diffusers</span> <span class=kn>import</span> <span class=n>DiffusionPipeline</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>stage_1</span> <span class=o>=</span> <span class=n>DiffusionPipeline</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;DeepFloyd/IF-I-L-v1.0&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>text_encoder</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>variant</span><span class=o>=</span><span class=s2>&#34;fp16&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float16</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>stage_1</span><span class=o>.</span><span class=n>enable_model_cpu_offload</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>stage_1</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=s1>&#39;cuda&#39;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>stage_2</span> <span class=o>=</span> <span class=n>DiffusionPipeline</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;DeepFloyd/IF-II-L-v1.0&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>text_encoder</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>variant</span><span class=o>=</span><span class=s2>&#34;fp16&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>              <span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>stage_2</span><span class=o>.</span><span class=n>enable_model_cpu_offload</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>stage_2</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=s1>&#39;cuda&#39;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=generate-image>Generate Image</h2><p>Choose one of the view transformations supported by the Visual Anagrams repository.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>visual_anagrams.views</span> <span class=kn>import</span> <span class=n>get_views</span>
</span></span><span class=line><span class=cl><span class=c1># UNCOMMENT ONE OF THESE</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># views = get_views([&#39;identity&#39;, &#39;rotate_180&#39;])</span>
</span></span><span class=line><span class=cl><span class=c1># views = get_views([&#39;identity&#39;, &#39;rotate_cw&#39;])</span>
</span></span><span class=line><span class=cl><span class=c1># views = get_views([&#39;identity&#39;, &#39;flip&#39;])</span>
</span></span><span class=line><span class=cl><span class=c1># views = get_views([&#39;identity&#39;, &#39;jigsaw&#39;])</span>
</span></span><span class=line><span class=cl><span class=n>views</span> <span class=o>=</span> <span class=n>get_views</span><span class=p>([</span><span class=s1>&#39;identity&#39;</span><span class=p>,</span> <span class=s1>&#39;negate&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=c1># views = get_views([&#39;identity&#39;, &#39;skew&#39;])</span>
</span></span><span class=line><span class=cl><span class=c1># views = get_views([&#39;identity&#39;, &#39;patch_permute&#39;])</span>
</span></span><span class=line><span class=cl><span class=c1># views = get_views([&#39;identity&#39;, &#39;pixel_permute&#39;])</span>
</span></span><span class=line><span class=cl><span class=c1># views = get_views([&#39;identity&#39;, &#39;inner_circle&#39;])</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=results>Results</h2><p>Now, we are ready to generate the visual illusions. The <code>sample_stage_1</code> function from visual anagrams repo accomplishes this and produces a $64 \times 64$ image. Similarly, the <code>sample_stage_2</code> function upsamples the resulting image while denoising all views, generating a $256 \times 256$ image.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>visual_anagrams.samplers</span> <span class=kn>import</span> <span class=n>sample_stage_1</span><span class=p>,</span> <span class=n>sample_stage_2</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>visual_anagrams.utils</span> <span class=kn>import</span> <span class=n>add_args</span><span class=p>,</span> <span class=n>save_illusion</span><span class=p>,</span> <span class=n>save_metadata</span>
</span></span><span class=line><span class=cl><span class=n>image_64</span> <span class=o>=</span> <span class=n>sample_stage_1</span><span class=p>(</span><span class=n>stage_1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                          <span class=n>prompt_embeds</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                          <span class=n>negative_prompt_embeds</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                          <span class=n>views</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                          <span class=n>num_inference_steps</span><span class=o>=</span><span class=mi>40</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                          <span class=n>guidance_scale</span><span class=o>=</span><span class=mf>10.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                          <span class=n>reduction</span><span class=o>=</span><span class=s1>&#39;mean&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                          <span class=n>generator</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>mp</span><span class=o>.</span><span class=n>show_images</span><span class=p>([</span><span class=n>im_to_np</span><span class=p>(</span><span class=n>view</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>image_64</span><span class=p>[</span><span class=mi>0</span><span class=p>]))</span> <span class=k>for</span> <span class=n>view</span> <span class=ow>in</span> <span class=n>views</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>image</span> <span class=o>=</span> <span class=n>sample_stage_2</span><span class=p>(</span><span class=n>stage_2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                       <span class=n>image_64</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                       <span class=n>prompt_embeds</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                       <span class=n>negative_prompt_embeds</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                       <span class=n>views</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                       <span class=n>num_inference_steps</span><span class=o>=</span><span class=mi>30</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                       <span class=n>guidance_scale</span><span class=o>=</span><span class=mf>10.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                       <span class=n>reduction</span><span class=o>=</span><span class=s1>&#39;mean&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                       <span class=n>noise_level</span><span class=o>=</span><span class=mi>50</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                       <span class=n>generator</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>mp</span><span class=o>.</span><span class=n>show_images</span><span class=p>([</span><span class=n>im_to_np</span><span class=p>(</span><span class=n>view</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>image</span><span class=p>[</span><span class=mi>0</span><span class=p>]))</span> <span class=k>for</span> <span class=n>view</span> <span class=ow>in</span> <span class=n>views</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><p><img src=/images/2024-01-28-waterfall.deer.mp4-output.gif loading=lazy alt=animation></p><h2 id=more-examples>More Examples</h2><p>Here are few more examples of illusions generated using this model.</p><p><img src=/images/2024-01-29-rotate_cw.oil.painting.houses.medieval.village.ship.ocean.gif loading=lazy alt=animation>
<img src=/images/2024-01-28-rotate_180.pop.art.wolverine.joker.gif loading=lazy alt=animation>
<img src=/images/2024-01-28-line-drawing-old-man-girl.gif loading=lazy alt=animation>
<img src=/images/2024-01-28-square_hinge.oil.painting.Medieval.village.scene.with.busy.gif loading=lazy alt=animation>
<img src=/images/2024-01-29-negate.photo.woman.man.gif loading=lazy alt=animation>
<img src=/images/2024-01-28-jigsaw.oil.painting.classroom.playground.gif loading=lazy alt=animation>
<img src=/images/2024-01-28-rotate_180.line.drawing.cat.bunny.gif loading=lazy alt=animation>
<img src=/images/2024-01-29-rotate_180.oil.painting.forest.fire.truck.gif loading=lazy alt=animation> |</p><h1 id=conclusion>Conclusion</h1><p>With this, we get a pretty impressive image of a waterfall which when inverted looks like a deer. I have a notebook version of the same code, you can give it a try in colab and try different transformation views. It is fascinating to observe how details from different objects and scenes can be embedded into a picture and how our visual apparatus end up seeing what we want to see.</p><a target=_blank href=https://colab.research.google.com/github/tmzh/visual_anagrams/blob/main/notebooks/visual_anagrams_colab_free.ipynb><img src=https://colab.research.google.com/assets/colab-badge.svg alt="Open In Colab"></a><h1 id=references>References</h1><ul><li><a class=link href=https://huggingface.co/docs/diffusers/main/en/api/pipelines/deepfloyd_if target=_blank rel=noopener>https://huggingface.co/docs/diffusers/main/en/api/pipelines/deepfloyd_if</a></li><li><a class=link href=https://huggingface.co/DeepFloyd/IF-I-XL-v1.0 target=_blank rel=noopener>https://huggingface.co/DeepFloyd/IF-I-XL-v1.0</a></li><li><a class=link href=https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/deepfloyd_if_free_tier_google_colab.ipynb target=_blank rel=noopener>https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/deepfloyd_if_free_tier_google_colab.ipynb</a></li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/deep_floyd/>Deep_floyd</a>
<a href=/tags/generative_ai/>Generative_ai</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/building-a-codenames-ai-assistant-with-multi-modal-llms/><div class=article-details><h2 class=article-title>Building a Codenames AI Assistant with Multi-Modal LLMs</h2></div></a></article><article><a href=/p/2023-06-24-llm-powered-faq-chat-bot/><div class=article-details><h2 class=article-title>Exploring Retrieval-Augmentated Generation with Open Source Large Language Models</h2></div></a></article><article class=has-image><a href=/p/2023-05-16-paint-like-bob-ross-using-stable-diffusion/><div class=article-image><img src=/images/2023-05-16-stable-diffusion-art.png loading=lazy data-key=2023-05-16-paint-like-bob-ross-using-stable-diffusion data-hash=/images/2023-05-16-stable-diffusion-art.png></div><div class=article-details><h2 class=article-title>Create Beautiful Paintings From Rough Sketches Using Stable diffusion</h2></div></a></article></div></div></aside><div class=disqus-container></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2024 Ephemeral Dance Of Electrons</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.29.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>