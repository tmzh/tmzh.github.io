<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="In July 2021, AWS and Hugging Face announced collaboration to make Hugging Face a first party framework within SageMaker. Earlier, you had to use PyTorch container and install packages manually to do this. With the new Hugging Face Deep Learning Containers (DLC) availabe in Amazon SageMaker, the process of training and deploying models is greatly simplified.\nIn this post, we will go through a high level overview of Hugging Face Transformers library before looking at how to use the newly announced Hugging Face DLCs within Sagemaker.\n"><title>Using Hugging Face Transformers on AWS Sagemaker</title>
<link rel=canonical href=https://example.com/p/2021-09-16-using-hugging-face-transformers-on-aws-sagemaker/><link rel=stylesheet href=/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="Using Hugging Face Transformers on AWS Sagemaker"><meta property='og:description' content="In July 2021, AWS and Hugging Face announced collaboration to make Hugging Face a first party framework within SageMaker. Earlier, you had to use PyTorch container and install packages manually to do this. With the new Hugging Face Deep Learning Containers (DLC) availabe in Amazon SageMaker, the process of training and deploying models is greatly simplified.\nIn this post, we will go through a high level overview of Hugging Face Transformers library before looking at how to use the newly announced Hugging Face DLCs within Sagemaker.\n"><meta property='og:url' content='https://example.com/p/2021-09-16-using-hugging-face-transformers-on-aws-sagemaker/'><meta property='og:site_name' content='Ephemeral Dance Of Electrons'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='aws'><meta property='article:tag' content='huggingface'><meta property='article:tag' content='transformers'><meta property='article:published_time' content='2021-09-16T12:00:00+00:00'><meta property='article:modified_time' content='2021-09-16T12:00:00+00:00'><meta property='og:image' content='https://huggingface.co/blog/assets/17_the_partnership_amazon_sagemaker_and_hugging_face/cover.png'><meta name=twitter:title content="Using Hugging Face Transformers on AWS Sagemaker"><meta name=twitter:description content="In July 2021, AWS and Hugging Face announced collaboration to make Hugging Face a first party framework within SageMaker. Earlier, you had to use PyTorch container and install packages manually to do this. With the new Hugging Face Deep Learning Containers (DLC) availabe in Amazon SageMaker, the process of training and deploying models is greatly simplified.\nIn this post, we will go through a high level overview of Hugging Face Transformers library before looking at how to use the newly announced Hugging Face DLCs within Sagemaker.\n"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://huggingface.co/blog/assets/17_the_partnership_amazon_sagemaker_and_hugging_face/cover.png'></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"light")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/ars_longa_hu11119055095056425454.png width=300 height=299 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>ðŸŽ“</span></figure><div class=site-meta><h1 class=site-name><a href=/>Ephemeral Dance Of Electrons</a></h1><h2 class=site-description>Ars Longa, vita brevis</h2></div></header><ol class=menu-social><li><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://twitter.com target=_blank title=Twitter rel=me><svg class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#tokenizer>Tokenizer</a></li><li><a href=#hugging-face-model>Hugging Face Model</a></li></ol><ol><li><a href=#running-a-training-job>Running a Training job</a><ol><li><a href=#preparing-a-training-script>Preparing a training script</a></li><li><a href=#run-training-using-hugging-face-estimator>Run training using Hugging Face estimator</a></li></ol></li><li><a href=#deploying-the-model-for-inference>Deploying the model for inference</a><ol><li><a href=#advanced-features-of-the-inference-toolkit>Advanced features of the Inference toolkit</a></li><li><a href=#customizing-inference-script>Customizing Inference script</a></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/2021-09-16-using-hugging-face-transformers-on-aws-sagemaker/><img src=https://huggingface.co/blog/assets/17_the_partnership_amazon_sagemaker_and_hugging_face/cover.png loading=lazy alt="Featured image of post Using Hugging Face Transformers on AWS Sagemaker"></a></div><div class=article-details><header class=article-category><a href=/categories/machine-learning/>Machine Learning</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/2021-09-16-using-hugging-face-transformers-on-aws-sagemaker/>Using Hugging Face Transformers on AWS Sagemaker</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>2021-09-16</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>10 minute read</time></div></footer></div></header><section class=article-content><p>In July 2021, AWS and Hugging Face announced collaboration to make Hugging Face a first party framework within SageMaker. Earlier, you had to use PyTorch container and install packages manually to do this. With the new Hugging Face Deep Learning Containers (DLC) availabe in Amazon SageMaker, the process of training and deploying models is greatly simplified.</p><p>In this post, we will go through a high level overview of Hugging Face Transformers library before looking at how to use the newly announced Hugging Face DLCs within Sagemaker.</p><h1 id=introduction-to-hugging-face-transformers>Introduction to Hugging Face Transformers</h1><p>The Hugging Face Transformers is a library that makes it easy to use NLP models. It allows developers to leverage hundreds of pretrained models for Natural Language Understanding (NLU) tasks as well as making it simple to train new transformer models. The API of this library is based around 3 broad classes:</p><ol><li><strong>Model</strong> - PyTorch or Keras models that we can use in training loop or for prediction</li><li><strong>Configuration</strong> - Stores all the configuration required to build a model</li><li><strong>Tokenizer</strong> - Stores the vocabulary and methods for encoding and decoding between strings and tokens</li></ol><p>The transformers library offers a simple abstraction over the above 3 models using the <code>pipeline</code> method. This is the simplest way to get started using the pre-trained models from model hub.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>classifier</span> <span class=o>=</span> <span class=n>pipeline</span><span class=p>(</span><span class=s1>&#39;sentiment-analysis&#39;</span><span class=p>,</span> <span class=n>model</span><span class=o>=</span><span class=s2>&#34;distilbert-base-uncased-finetuned-sst-2-english&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>classifier</span><span class=p>(</span><span class=s1>&#39;We are very happy to show you the ðŸ¤— Transformers library.&#39;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>The first argument is a Hugging Face NLP task, in this case it is <code>sentiment analysis</code>. Some of the supported tasks are:</p><ul><li>Sequence Classification</li><li>Sentiment Analysis</li><li>Question Answering</li><li>Language Modelling</li><li>Text Generation</li><li>Named Entity Recognition (NER)</li><li>Summarization</li><li>Translation</li></ul><p>See <a class=link href=https://huggingface.co/transformers/task_summary.html# target=_blank rel=noopener>here</a> for an overview of the tasks supported by the library.</p><p>Under the hood, calling the pipeline method roughly covers the following steps:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoTokenizer</span><span class=p>,</span> <span class=n>AutoModelForSequenceClassification</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>nn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># download the model and tokenizer</span>
</span></span><span class=line><span class=cl><span class=n>model_name</span> <span class=o>=</span> <span class=s2>&#34;distilbert-base-uncased-finetuned-sst-2-english&#34;</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForSequenceClassification</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># tokenize the input</span>
</span></span><span class=line><span class=cl><span class=n>batch</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>([</span><span class=s2>&#34;The Hugging Face Transformers library is amazing&#34;</span><span class=p>],</span> 
</span></span><span class=line><span class=cl>                  <span class=n>padding</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>truncation</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>max_length</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                  <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># run predictions</span>
</span></span><span class=line><span class=cl><span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=o>**</span><span class=n>batch</span><span class=p>)</span> <span class=c1># returns logits for classification task</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># get predictions</span>
</span></span><span class=line><span class=cl><span class=n>predictions</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>functional</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>logits</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>predictions</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>The downloaded models are stored in <code>~/.cache/huggingface/transformers</code>.</p><p>Here is the process:</p><ul><li>Instantiate <code>AutoTokenizer</code> to download the tokenizer associated to the model we picked and instantiate it.</li><li>Use <code>AutoModelForSequenceClassification</code> to download the model itself.</li><li>Build a sequence from the input sentence, using the correct model-specific separators token type ids and attention masks</li><li>Pass this sequence through the model to get the logits</li><li>Compute the softmax of the result to get probabilities over the classes</li></ul><h2 id=tokenizer>Tokenizer</h2><p>Tokenizer&rsquo;s job is to preprocess your text into tokens suitable for training or inference. Tokens can be a word (<code>predict</code>) or a subword (<code>##ly</code>). For example, a tokenizer may split the word <code>Transformers</code> into (<code>transform</code>, <code>##ers</code>) so that the model&rsquo;s vocabulary doesn&rsquo;t explode. The tokenizer can also take care of other pre-processing tasks such as normalizing cases and punctuations.</p><p>The tokenization logic is tied to the model we use. That is why in our example we derived the model and tokenizer from the same model name. The <code>AutoTokenizer</code> and <code>AutoModelForxxx</code> classes ensures that the tokenizers and models are paired correctly.</p><p>When we apply a tokenizer to an input text, it returns a dictionary containing <code>ids</code> of the tokens and <code>attention mask</code>. <code>ids</code> are the numerical representation of tokens. To learn about attention mask and other details related to Tokenizers refer <a class=link href=https://huggingface.co/transformers/tokenizer_summary.html target=_blank rel=noopener>here</a>.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>tokenizer</span><span class=p>(</span><span class=s2>&#34;We are very happy to show you the ðŸ¤— Transformers library.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>{</span><span class=s1>&#39;input_ids&#39;</span><span class=p>:</span> <span class=p>[</span><span class=mi>101</span><span class=p>,</span> <span class=mi>2057</span><span class=p>,</span> <span class=mi>2024</span><span class=p>,</span> <span class=mi>2200</span><span class=p>,</span> <span class=mi>3407</span><span class=p>,</span> <span class=mi>2000</span><span class=p>,</span> <span class=mi>2265</span><span class=p>,</span> <span class=mi>2017</span><span class=p>,</span> <span class=mi>1996</span><span class=p>,</span> <span class=mi>100</span><span class=p>,</span> <span class=mi>19081</span><span class=p>,</span> <span class=mi>3075</span><span class=p>,</span> <span class=mi>1012</span><span class=p>,</span> <span class=mi>102</span><span class=p>],</span> <span class=s1>&#39;attention_mask&#39;</span><span class=p>:</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>]}</span>
</span></span></code></pre></td></tr></table></div></div><p>Note that the tokens also consists of some special tokens which encodes special meaning in the sentences. They differ from model to model. In our model, they are:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>special_tokens_map</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=s1>&#39;cls_token&#39;</span><span class=p>:</span> <span class=s1>&#39;[CLS]&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl> <span class=s1>&#39;mask_token&#39;</span><span class=p>:</span> <span class=s1>&#39;[MASK]&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl> <span class=s1>&#39;pad_token&#39;</span><span class=p>:</span> <span class=s1>&#39;[PAD]&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl> <span class=s1>&#39;sep_token&#39;</span><span class=p>:</span> <span class=s1>&#39;[SEP]&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl> <span class=s1>&#39;unk_token&#39;</span><span class=p>:</span> <span class=s1>&#39;[UNK]&#39;</span>
</span></span><span class=line><span class=cl> <span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=hugging-face-model>Hugging Face Model</h2><p>Once the input text has been preprocessed by the tokenizer, we can pass it directly to the model</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=o>**</span><span class=n>batch</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>The contents of the model output depends on the task. For <a class=link href=https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput target=_blank rel=noopener>SequenceClassification</a> we get back <code>logit</code>, an optional <code>loss</code>, <code>hidden_states</code> and <code>attentions</code> attributes.</p><p>The <code>model</code> class can also be used to do transfer learning for custom NLP tasks. The Transformers library provides a <code>Trainer</code> API that takes this model as input, extracts the pre-trained weights and fine tunes it.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>Trainer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Build the trainer class</span>
</span></span><span class=line><span class=cl><span class=n>trainer</span> <span class=o>=</span> <span class=n>Trainer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>    <span class=n>args</span><span class=o>=</span><span class=n>training_args</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>    <span class=n>train_dataset</span><span class=o>=</span><span class=n>small_train_dataset</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>    <span class=n>eval_dataset</span><span class=o>=</span><span class=n>small_eval_dataset</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Fine-tune the model</span>
</span></span><span class=line><span class=cl><span class=n>trainer</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p>This covers a high level overview of the Hugging Face Transformers library. Next we will see how to use the library along with Sagemaker.</p><h1 id=using-hugging-face-on-sagemaker>Using Hugging Face on Sagemaker</h1><p>Hugging Face in collaboration with AWS released Sagemaker Hugging Face Deep Learning Containers (DLCs) that makes it easy to train and deploy Hugging Face models using AWS platform. In the following section, we will see how to use these DLCs to train and deploy Hugging Face Transformer models in AWS.</p><h2 id=running-a-training-job>Running a Training job</h2><h3 id=preparing-a-training-script>Preparing a training script</h3><p>First we need to prepare the training script. This would be similar to any Transformers training script. A minimal training script would look like this:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>%%</span><span class=n>writefile</span> <span class=n>train</span><span class=o>.</span><span class=n>py</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForSequenceClassification</span><span class=p>,</span> <span class=n>Trainer</span><span class=p>,</span> <span class=n>TrainingArguments</span><span class=p>,</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.metrics</span> <span class=kn>import</span> <span class=n>accuracy_score</span><span class=p>,</span> <span class=n>precision_recall_fscore_support</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>datasets</span> <span class=kn>import</span> <span class=n>load_from_disk</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>sys</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>argparse</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>parser</span> <span class=o>=</span> <span class=n>argparse</span><span class=o>.</span><span class=n>ArgumentParser</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Data, model, and output directories</span>
</span></span><span class=line><span class=cl>    <span class=n>parser</span><span class=o>.</span><span class=n>add_argument</span><span class=p>(</span><span class=s2>&#34;--model_dir&#34;</span><span class=p>,</span> <span class=nb>type</span><span class=o>=</span><span class=nb>str</span><span class=p>,</span> <span class=n>default</span><span class=o>=</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>&#34;SM_MODEL_DIR&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>parser</span><span class=o>.</span><span class=n>add_argument</span><span class=p>(</span><span class=s2>&#34;--training_dir&#34;</span><span class=p>,</span> <span class=nb>type</span><span class=o>=</span><span class=nb>str</span><span class=p>,</span> <span class=n>default</span><span class=o>=</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>&#34;SM_CHANNEL_TRAIN&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>parser</span><span class=o>.</span><span class=n>add_argument</span><span class=p>(</span><span class=s2>&#34;--test_dir&#34;</span><span class=p>,</span> <span class=nb>type</span><span class=o>=</span><span class=nb>str</span><span class=p>,</span> <span class=n>default</span><span class=o>=</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>&#34;SM_CHANNEL_TEST&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># load datasets</span>
</span></span><span class=line><span class=cl>    <span class=n>train_dataset</span> <span class=o>=</span> <span class=n>load_from_disk</span><span class=p>(</span><span class=n>args</span><span class=o>.</span><span class=n>training_dir</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>test_dataset</span> <span class=o>=</span> <span class=n>load_from_disk</span><span class=p>(</span><span class=n>args</span><span class=o>.</span><span class=n>test_dir</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># download model from model hub</span>
</span></span><span class=line><span class=cl>    <span class=n>model_name</span> <span class=o>=</span> <span class=s2>&#34;distilbert-base-uncased-finetuned-sst-2-english&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForSequenceClassification</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># define training args</span>
</span></span><span class=line><span class=cl>    <span class=n>training_args</span> <span class=o>=</span> <span class=n>TrainingArguments</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>output_dir</span><span class=o>=</span><span class=n>args</span><span class=o>.</span><span class=n>model_dir</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># create Trainer instance</span>
</span></span><span class=line><span class=cl>    <span class=n>trainer</span> <span class=o>=</span> <span class=n>Trainer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>args</span><span class=o>=</span><span class=n>training_args</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>train_dataset</span><span class=o>=</span><span class=n>train_dataset</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>eval_dataset</span><span class=o>=</span><span class=n>test_dataset</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>tokenizer</span><span class=o>=</span><span class=n>tokenizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># train model</span>
</span></span><span class=line><span class=cl>    <span class=n>trainer</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># save the model</span>
</span></span><span class=line><span class=cl>    <span class=n>trainer</span><span class=o>.</span><span class=n>save_model</span><span class=p>(</span><span class=n>args</span><span class=o>.</span><span class=n>model_dir</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>For a more complete version of the script covering model evaluation, logging and additional training arguments, refer to this <a class=link href=https://github.com/huggingface/notebooks/blob/9664c8b62e38083952b849da8912af13a35312b0/sagemaker/01_getting_started_pytorch/scripts/train.py target=_blank rel=noopener>sample script</a>.</p><p>As with any Sagemaker training job, we need to ensure that this script reads data from DLC&rsquo;s data input directory and saves the model to model directory. The following lines of codes takes care of this.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># loading datasets</span>
</span></span><span class=line><span class=cl><span class=n>parser</span><span class=o>.</span><span class=n>add_argument</span><span class=p>(</span><span class=s2>&#34;--training_dir&#34;</span><span class=p>,</span> <span class=nb>type</span><span class=o>=</span><span class=nb>str</span><span class=p>,</span> <span class=n>default</span><span class=o>=</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>&#34;SM_CHANNEL_TRAIN&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>parser</span><span class=o>.</span><span class=n>add_argument</span><span class=p>(</span><span class=s2>&#34;--test_dir&#34;</span><span class=p>,</span> <span class=nb>type</span><span class=o>=</span><span class=nb>str</span><span class=p>,</span> <span class=n>default</span><span class=o>=</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>&#34;SM_CHANNEL_TEST&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>train_dataset</span> <span class=o>=</span> <span class=n>load_from_disk</span><span class=p>(</span><span class=n>args</span><span class=o>.</span><span class=n>training_dir</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>test_dataset</span> <span class=o>=</span> <span class=n>load_from_disk</span><span class=p>(</span><span class=n>args</span><span class=o>.</span><span class=n>test_dir</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># storing models</span>
</span></span><span class=line><span class=cl><span class=n>trainer</span><span class=o>.</span><span class=n>save_model</span><span class=p>(</span><span class=n>args</span><span class=o>.</span><span class=n>model_dir</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=run-training-using-hugging-face-estimator>Run training using Hugging Face estimator</h3><p>First we create a Hugging Face estimator which exposes methods similar to other Sagemaker Estimator. Note that the <code>entry_point</code> attribute matches the file name of our training script.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sagemaker.huggingface</span> <span class=kn>import</span> <span class=n>HuggingFace</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># hyperparameters, which are passed into the training job</span>
</span></span><span class=line><span class=cl><span class=n>hyperparameters</span><span class=o>=</span><span class=p>{</span><span class=s1>&#39;epochs&#39;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=s1>&#39;per_device_train_batch_size&#39;</span><span class=p>:</span> <span class=mi>32</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=s1>&#39;model_name_or_path&#39;</span><span class=p>:</span> <span class=s1>&#39;distilbert-base-uncased&#39;</span>
</span></span><span class=line><span class=cl>                 <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># create the Estimator</span>
</span></span><span class=line><span class=cl><span class=n>huggingface_estimator</span> <span class=o>=</span> <span class=n>HuggingFace</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>entry_point</span><span class=o>=</span><span class=s1>&#39;train.py&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>instance_type</span><span class=o>=</span><span class=s1>&#39;ml.p3.2xlarge&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>instance_count</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>role</span><span class=o>=</span><span class=n>role</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>transformers_version</span><span class=o>=</span><span class=s1>&#39;4.4&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>pytorch_version</span><span class=o>=</span><span class=s1>&#39;1.6&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>py_version</span><span class=o>=</span><span class=s1>&#39;py36&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>hyperparameters</span> <span class=o>=</span> <span class=n>hyperparameters</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Training is invoked by calling the <code>fit</code> method on <code>Hugging Face</code> Estimator.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>huggingface_estimator</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=p>{</span><span class=s1>&#39;train&#39;</span><span class=p>:</span> <span class=s1>&#39;s3://sagemaker-us-east-1-558105141721/samples/datasets/imdb/train&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>   <span class=s1>&#39;test&#39;</span><span class=p>:</span> <span class=s1>&#39;s3://sagemaker-us-east-1-558105141721/samples/datasets/imdb/test&#39;</span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>The trained model is a tarball with all the resources needed for inference.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>model.tar.gz/
</span></span><span class=line><span class=cl>|- pytroch_model.bin
</span></span><span class=line><span class=cl>|- vocab.txt
</span></span><span class=line><span class=cl>|- tokenizer_config.json
</span></span><span class=line><span class=cl>|- config.json
</span></span><span class=line><span class=cl>|- special_tokens_map.json
</span></span></code></pre></td></tr></table></div></div><h2 id=deploying-the-model-for-inference>Deploying the model for inference</h2><p>Once the training is completed, we can deploy a <code>Hugging Face</code> model directly from the <code>Estimator</code> object.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># deploy model to SageMaker Inference</span>
</span></span><span class=line><span class=cl><span class=n>predictor</span> <span class=o>=</span> <span class=n>huggingface_estimator</span><span class=o>.</span><span class=n>deploy</span><span class=p>(</span><span class=n>initial_instance_count</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>instance_type</span><span class=o>=</span><span class=s2>&#34;ml.m5.xlarge&#34;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Alternatively, if we already have a completed training job, we can used its output model to deploy a new <code>Hugging Face</code> model and deploy it.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sagemaker.huggingface.model</span> <span class=kn>import</span> <span class=n>HuggingFaceModel</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># create Hugging Face Model Class</span>
</span></span><span class=line><span class=cl><span class=n>huggingface_model</span> <span class=o>=</span> <span class=n>HuggingFaceModel</span><span class=p>(</span>
</span></span><span class=line><span class=cl>   <span class=n>model_data</span><span class=o>=</span><span class=s2>&#34;s3://models/my-bert-model/model.tar.gz&#34;</span><span class=p>,</span>  <span class=c1># path to your trained sagemaker model</span>
</span></span><span class=line><span class=cl>   <span class=n>role</span><span class=o>=</span><span class=n>role</span><span class=p>,</span> <span class=c1># iam role with permissions to create an Endpoint</span>
</span></span><span class=line><span class=cl>   <span class=n>transformers_version</span><span class=o>=</span><span class=s2>&#34;4.6&#34;</span><span class=p>,</span> <span class=c1># transformers version used</span>
</span></span><span class=line><span class=cl>   <span class=n>pytorch_version</span><span class=o>=</span><span class=s2>&#34;1.7&#34;</span><span class=p>,</span> <span class=c1># pytorch version used</span>
</span></span><span class=line><span class=cl>   <span class=n>py_version</span><span class=o>=</span><span class=s1>&#39;py36&#39;</span><span class=p>,</span> <span class=c1># python version used</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># deploy model to SageMaker Inference</span>
</span></span><span class=line><span class=cl><span class=n>predictor</span> <span class=o>=</span> <span class=n>huggingface_model</span><span class=o>.</span><span class=n>deploy</span><span class=p>(</span>
</span></span><span class=line><span class=cl>   <span class=n>initial_instance_count</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>   <span class=n>instance_type</span><span class=o>=</span><span class=s2>&#34;ml.m5.xlarge&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>We can use this deployed model to make predictions on input text. The default inference script in Hugging Face DLC expects a dictionary with <code>inputs</code> as key. For details on default input formats for various tasks refer to <a class=link href=https://huggingface.co/docs/sagemaker/inference#inference-toolkit---api-description target=_blank rel=noopener>this</a>.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># example request. </span>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>   <span class=s2>&#34;inputs&#34;</span><span class=p>:</span> <span class=s2>&#34;Sagemaker SDK is easy to use&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=c1># request</span>
</span></span><span class=line><span class=cl><span class=n>predictor</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>The above method makes use of Sagemaker SDK to invoke the model. Often in a production ML application, invocation is handled by calling <a class=link href=https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_runtime_InvokeEndpoint.html target=_blank rel=noopener>InvokeEndpoint API</a> via boto3 or other SDK. A sample boto3 based invocation would look like below:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>io</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>boto3</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>json</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>csv</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># grab environment variables</span>
</span></span><span class=line><span class=cl><span class=n>ENDPOINT_NAME</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;ENDPOINT_NAME&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>runtime</span><span class=o>=</span> <span class=n>boto3</span><span class=o>.</span><span class=n>client</span><span class=p>(</span><span class=s1>&#39;runtime.sagemaker&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>lambda_handler</span><span class=p>(</span><span class=n>event</span><span class=p>,</span> <span class=n>context</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Received event: &#34;</span> <span class=o>+</span> <span class=n>json</span><span class=o>.</span><span class=n>dumps</span><span class=p>(</span><span class=n>event</span><span class=p>,</span> <span class=n>indent</span><span class=o>=</span><span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>data</span> <span class=o>=</span> <span class=n>json</span><span class=o>.</span><span class=n>loads</span><span class=p>(</span><span class=n>json</span><span class=o>.</span><span class=n>dumps</span><span class=p>(</span><span class=n>event</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>payload</span> <span class=o>=</span> <span class=n>data</span><span class=p>[</span><span class=s1>&#39;data&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>payload</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>   <span class=s2>&#34;inputs&#34;</span><span class=p>:</span> <span class=s2>&#34;Sagemaker SDK is easy to use&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>payload</span> <span class=o>=</span> <span class=n>json</span><span class=o>.</span><span class=n>dumps</span><span class=p>(</span><span class=n>data</span><span class=p>)</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s1>&#39;utf-8&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>runtime</span><span class=o>.</span><span class=n>invoke_endpoint</span><span class=p>(</span><span class=n>EndpointName</span><span class=o>=</span><span class=n>ENDPOINT_NAME</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                    <span class=n>ContentType</span><span class=o>=</span><span class=s1>&#39;application/json&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                    <span class=n>Body</span><span class=o>=</span><span class=n>payload</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Remember to delete the endpoint at the end of experiments.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># delete endpoint</span>
</span></span><span class=line><span class=cl><span class=n>predictor</span><span class=o>.</span><span class=n>delete_endpoint</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=advanced-features-of-the-inference-toolkit>Advanced features of the Inference toolkit</h3><p>We can also pass additional environment variables to the inference model that simplifies deployment.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>hub</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=s1>&#39;HF_MODEL_ID&#39;</span><span class=p>:</span><span class=s1>&#39;distilbert-base-uncased-distilled-squad&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=s1>&#39;HF_TASK&#39;</span><span class=p>:</span><span class=s1>&#39;question-answering&#39;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=c1># create Hugging Face Model Class</span>
</span></span><span class=line><span class=cl><span class=n>huggingface_model</span> <span class=o>=</span> <span class=n>HuggingFaceModel</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>transformers_version</span><span class=o>=</span><span class=s1>&#39;4.6&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>pytorch_version</span><span class=o>=</span><span class=s1>&#39;1.7&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>py_version</span><span class=o>=</span><span class=s1>&#39;py36&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>env</span><span class=o>=</span><span class=n>hub</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    
</span></span></code></pre></td></tr></table></div></div><p>Here, <code>HF_TASK</code> variable defines the task for the Transformers pipeline and <code>HF_MODEL_ID</code> defines the model id to load from <a class=link href=https://huggingface.co/models target=_blank rel=noopener>huggingface.co/models</a>. For the full list of supported environment variables refer to <a class=link href=https://github.com/aws/sagemaker-huggingface-inference-toolkit#%EF%B8%8F-environment-variables target=_blank rel=noopener>here</a>.</p><h3 id=customizing-inference-script>Customizing Inference script</h3><p>When creating an inference model, we can specify use defined code/modules that allows us to customize the inference process.</p><p>For example, here is a barebones inference script which we will call <code>inference.py</code>:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sagemaker_huggingface_inference_toolkit.transformers_utils</span> <span class=kn>import</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>_is_gpu_available</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>get_pipeline</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sagemaker_huggingface_inference_toolkit</span> <span class=kn>import</span> <span class=n>decoder_encoder</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>_is_gpu_available</span><span class=p>():</span>
</span></span><span class=line><span class=cl>  <span class=n>device</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>context</span><span class=o>.</span><span class=n>system_properties</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;gpu_id&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=n>device</span> <span class=o>=</span> <span class=o>-</span><span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>load_fn</span><span class=p>(</span><span class=n>model_dir</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=c1># gets pipeline from task tag</span>
</span></span><span class=line><span class=cl>  <span class=n>hf_pipeline</span> <span class=o>=</span> <span class=n>get_pipeline</span><span class=p>(</span><span class=n>task</span><span class=o>=</span><span class=s1>&#39;sentiment-analysis&#39;</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                             <span class=n>model_dir</span><span class=o>=</span><span class=n>model_dir</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                             <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>hf_pipeline</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>transform_fn</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>input_data</span><span class=p>,</span> <span class=n>content_type</span><span class=p>,</span> <span class=n>accept</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>processed_data</span> <span class=o>=</span> <span class=n>decoder_encoder</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>input_data</span><span class=p>,</span> <span class=n>content_type</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>predictions</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>processed_data</span><span class=p>[</span><span class=s1>&#39;my_custom_input&#39;</span><span class=p>])</span> <span class=c1># Our custom input format</span>
</span></span><span class=line><span class=cl>  <span class=n>response</span> <span class=o>=</span> <span class=n>decoder_encoder</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>predictions</span><span class=p>,</span> <span class=n>accept</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>response</span>
</span></span></code></pre></td></tr></table></div></div><p>To use this script, we need to place it under a source directory along with any additional files required.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>|- source/
</span></span><span class=line><span class=cl>  |- inference.py
</span></span><span class=line><span class=cl>  |- requirements.txt 
</span></span></code></pre></td></tr></table></div></div><p>Next when we create the <code>Hugging FaceModel</code> we need to set the <code>source_dir</code> and <code>entry_point</code> attribute. These attributes are derived from the <a class=link href=https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.Framework target=_blank rel=noopener>Sagemaker Estimator Framework</a> so they are available under all Frameworks.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>huggingface_model</span> <span class=o>=</span> <span class=n>HuggingFaceModel</span><span class=p>(</span>
</span></span><span class=line><span class=cl>   <span class=n>model_data</span><span class=o>=</span><span class=s2>&#34;s3://models/my-bert-model/model.tar.gz&#34;</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>   <span class=n>source_dir</span><span class=o>=</span><span class=s1>&#39;source&#39;</span><span class=p>,</span>  <span class=c1>#relative path to current directory of calling script</span>
</span></span><span class=line><span class=cl>   <span class=n>entry_point</span><span class=o>=</span><span class=s1>&#39;inference.py&#39;</span> <span class=c1>#name of the inference script under the source dir</span>
</span></span><span class=line><span class=cl>   <span class=n>role</span><span class=o>=</span><span class=n>role</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>   <span class=n>transformers_version</span><span class=o>=</span><span class=s2>&#34;4.6&#34;</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>   <span class=n>pytorch_version</span><span class=o>=</span><span class=s2>&#34;1.7&#34;</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>   <span class=n>py_version</span><span class=o>=</span><span class=s1>&#39;py36&#39;</span><span class=p>,</span> 
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>This has the effect of setting the environment variables <code>SAGEMAKER_SUBMIT_DIRECTORY</code> to <code>source</code> and <code>SAGEMAKER_PROGRAM</code> to <code>inference.py</code> on the inference model. The inference model also has the files packaged with the following directory structure:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>model.tar.gz/
</span></span><span class=line><span class=cl>|- pytroch_model.bin
</span></span><span class=line><span class=cl>|- ....
</span></span><span class=line><span class=cl>|- code/
</span></span><span class=line><span class=cl>  |- inference.py
</span></span><span class=line><span class=cl>  |- requirements.txt 
</span></span></code></pre></td></tr></table></div></div><p>Now when we deploy the model, we can pass custom inputs to it.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># deploy model to SageMaker Inference</span>
</span></span><span class=line><span class=cl><span class=n>predictor</span> <span class=o>=</span> <span class=n>huggingface_model</span><span class=o>.</span><span class=n>deploy</span><span class=p>(</span>
</span></span><span class=line><span class=cl>   <span class=n>initial_instance_count</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>   <span class=n>instance_type</span><span class=o>=</span><span class=s2>&#34;ml.m5.xlarge&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># example request. </span>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>   <span class=s2>&#34;my_custom_input&#34;</span><span class=p>:</span> <span class=s2>&#34;The Hugging Face Transformers library is amazing&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># request</span>
</span></span><span class=line><span class=cl><span class=n>predictor</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>For further instructions on how to customize inference, refer to <a class=link href=https://github.com/aws/sagemaker-huggingface-inference-toolkit#%EF%B8%8F-environment-variables target=_blank rel=noopener>this</a></p><h1 id=additional-resources>Additional resources</h1><p>To learn more, you can refer to:</p><ul><li><a class=link href=https://huggingface.co/transformers/philosophy.html target=_blank rel=noopener>Philosophy of Hugging Face transformers library</a></li><li><a class=link href=https://huggingface.co/transformers/notebooks.html target=_blank rel=noopener>Sample Hugging Face Transformers Notebooks</a></li><li><a class=link href=https://huggingface.co/docs/sagemaker/main target=_blank rel=noopener>Hugging Face on Amazon Sagemaker</a></li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/aws/>Aws</a>
<a href=/tags/huggingface/>Huggingface</a>
<a href=/tags/transformers/>Transformers</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/building-natural-language-interfaces-with-llm-function-calling/><div class=article-details><h2 class=article-title>Building Natural Language Interfaces with LLM Function Calling</h2></div></a></article><article><a href=/p/2018-04-28-predicting-the-playing-role-of-a-cricketer-using-machine-learning-part-2/><div class=article-details><h2 class=article-title>Predicting the playing role of a cricketer using Machine Learning (Part 2)</h2></div></a></article><article><a href=/p/2018-04-23-predicting-the-playing-role-of-a-cricketer-using-machine-learning-part-1/><div class=article-details><h2 class=article-title>Predicting the playing role of a cricketer using Machine Learning (Part 1)</h2></div></a></article></div></div></aside><div class=disqus-container></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 Ephemeral Dance Of Electrons</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.29.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>