<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Artificial Intelligence on Ephemeral Dance Of Electrons</title><link>https://example.com/categories/artificial-intelligence/</link><description>Recent content in Artificial Intelligence on Ephemeral Dance Of Electrons</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Wed, 15 Mar 2023 12:00:00 +0000</lastBuildDate><atom:link href="https://example.com/categories/artificial-intelligence/index.xml" rel="self" type="application/rss+xml"/><item><title>GPT-4, Stable Diffusion, and Beyond: How Generative AI Will Shape Human Society</title><link>https://example.com/p/2023-03-15-gpt-4-stable-diffusion-and-beyond/</link><pubDate>Wed, 15 Mar 2023 12:00:00 +0000</pubDate><guid>https://example.com/p/2023-03-15-gpt-4-stable-diffusion-and-beyond/</guid><description>&lt;img src="https://example.com/images/2020-09-26-meta-learning.png" alt="Featured image of post GPT-4, Stable Diffusion, and Beyond: How Generative AI Will Shape Human Society" />&lt;p>In 2020, I &lt;a class="link" href="https://example.com/post/2020-09-20-gpt-3-and-prospects-of-artificial-general-intelligence/" >wrote&lt;/a> about GPT-3 model. Late last year, OpenAI released ChatGPT which was based on GPT-3 but trained using Reinforcement Learning from Human Feedback (RLHF). And now GPT-4 has been released. It has only been out for a few days, but it is already seeing incredible applications such as &lt;a class="link" href="https://youtu.be/S7xTBa93TX8" target="_blank" rel="noopener"
>creating office documents&lt;/a>, turning sketches into &lt;a class="link" href="https://twitter.com/blader/status/1635792905628712960" target="_blank" rel="noopener"
>functional apps&lt;/a>, creating &lt;a class="link" href="https://www.youtube.com/watch?v=yEgHrxvLsz0" target="_blank" rel="noopener"
>personal tutors&lt;/a>, and &lt;a class="link" href="https://twitter.com/rowancheung/status/1636017917136367617" target="_blank" rel="noopener"
>more&lt;/a>.&lt;/p>
&lt;p>And not just GPT-based models, StableDiffusion and Dall-E are also pushing the boundaries of art, creating stunning visuals from mere textual descriptions. Professional ad agencies, too, are exploring how to use AI, as seen in this &lt;a class="link" href="https://youtu.be/951q69P0La" target="_blank" rel="noopener"
>Coca-Cola ad&lt;/a> and this &lt;a class="link" href="https://twitter.com/nonmayorpete/status/163516240862815846" target="_blank" rel="noopener"
>Crocs ad&lt;/a> which apparently took only 28 minutes to create from scratch.&lt;/p>
&lt;figure>
&lt;img src="https://example.com/images/2023-03-15-gpt-4-performance.png"
alt="GPT-4 performance"
width="80%">
&lt;figcaption>&lt;i>The newly released GPT-4 exhibits human-level performance on a variety of common and professional academic exams. Source: &lt;a href="https://cdn.openai.com/papers/gpt-4.pdf">OpenAI GPT-4 Technical Report&lt;/a>&lt;/i>&lt;/figcaption>
&lt;/figure>
&lt;p>Suddenly, the pace of advancements in AI has accelerated from a sluggish seep to a torrent, some might say it is not something we can control anymore. The dread of hyper-intelligence has long been a staple of our fears, for if an AI were to improve itself recursively, it would have no perceivable ceiling to the extent of its intelligence. Yet, such concerns remain speculations, given our limited understanding of AI&amp;rsquo;s internal workings (more on this below). We tend to project our apprehensions and anthropocentric tendencies onto AI and assume that it would act as a sentient being would. Rather than fretting over future possibilities, we should focus on the present: AI capabilities are already advanced enough to disrupt society.&lt;/p>
&lt;h2 id="changing-realities-the-societal-impact-of-generative-ai">Changing Realities: The Societal Impact of Generative AI
&lt;/h2>&lt;p>Human society has always evolved in a dynamic equilibrium with the technology it wields. Every major technological revolution has been accompanied by a sociological inflection point, as society adjusts not just to the novelty of new inventions but revises its implicit assumptions, norms and behaviors.&lt;/p>
&lt;p>The cognitive revolution (language &amp;amp; arts) that laid the foundation for civilization by bringing people together, also gave us myths, hero worship, and kings. It took us millennia for society at large to loosen the shackles of such institutions. Similarly, the invention of the printing press brought about a new era of mass communication, but also led to the spread of propaganda and the manipulation of public opinion. With the rise of radio and television came new forms of propaganda and disinformation. With the internet and social media, we still haven&amp;rsquo;t figured out coping mechanisms for fake news and large-scale manipulation of public opinion.&lt;/p>
&lt;h3 id="few-models-to-rule-them-all">Few models to rule them all
&lt;/h3>&lt;p>Generative AI brings with it all of the above risks, but on a scale and reach never imagined before. Almost all large scale models are derived from very few foundational models, further centralizing the risks and power. When I wrote about GPT-3, three things caught my attention:&lt;/p>
&lt;ul>
&lt;li>Effectiveness of scale&lt;/li>
&lt;li>Languages can be a model of the physical world&lt;/li>
&lt;li>Emergent behaviors leading to Zero to few shot learning&lt;/li>
&lt;/ul>
&lt;p>However, I only thought of GPT-3 as a precursor for things to come. But recent products like ChatGPT that are built on GPT-3 have shown that GPT-3 and similar Large Language Models (LLM) are already good enough.&lt;/p>
&lt;p>And good enough models are easy to use, so their adoption will only continue to grow. As people become more familiar with these tools, they will begin to trust its judgment automatically. In place of a circumspect, potentially biased human, we will have an infallible, impartial arbitrator telling us what is true. But how can we know how it made its decision?&lt;/p>
&lt;blockquote>
&lt;p>&amp;ldquo;In place of a circumspect, potentially biased human, we will have an infallible, impartial arbitrator telling us what is true&amp;rdquo;&lt;/p>
&lt;/blockquote>
&lt;h3 id="epistemology-of-large-language-models-defying-human-comprehension">Epistemology of Large Language Models: Defying Human Comprehension
&lt;/h3>&lt;p>The performance of an LLM comes from their emergent behaviors rather than explicit instruction. For example, when OpenAI scaled GPT-3 to 175 billion parameters from 1.5 billion in GPT-2, they observed few-shot learning, an emergent property that was neither specifically trained for nor anticipated to arise. Even in the past researchers have &lt;a class="link" href="https://arxiv.org/pdf/2108.07258.pdf#subsection.1.3" target="_blank" rel="noopener"
>warned&lt;/a> that &lt;em>&amp;ldquo;despite their deployment into the real world, these models are very much research prototypes that are poorly understood&amp;rdquo;&lt;/em>. It is just now more of the same.&lt;/p>
&lt;p>Furthermore, the resources required for training such large-scale models keep it out of reach for many; commercial incentives lend very few reasons for companies to make their models transparent or to attend to any of these social externalities arising from their obscurity. In essence, our attempts to grasp the nature of these models resemble those of blind men attempting to describe an elephant.&lt;/p>
&lt;h2 id="cultural-norms-redefined">Cultural Norms Redefined
&lt;/h2>&lt;p>As mentioned before, large-scale models are largely homogeneous, derived from very few foundation models; training data is often lopsided and only represents a tiny percentage of languages. The embedded social and political factors in these models lead to the entrenchment of a pre-dominant value system and undermine plurality. The prevalence of homogeneous LLMs in society can lead to cultural conformity where individual expression and differences are lost on several levels ranging from the individual to society.&lt;/p>
&lt;p>In the past, LLMs have also produced inaccurate or harmful outputs. While these have been somewhat mitigated through fine-tuning, the notion that a mere handful of individuals could define the parameters of morality with any confidence is problematic. The obscure and hermetic nature of these large-scale models only amplifies our concerns. Chomsky in his NY Times piece &lt;a class="link" href="https://dnyuz.com/2023/03/08/noam-chomsky-the-false-promise-of-chatgpt" target="_blank" rel="noopener"
>argues&lt;/a> that fine-tuning their ability to be original and opinionated. Thus, engineers &amp;ldquo;sacrificed creativity for a kind of amorality&amp;rdquo;&lt;/p>
&lt;h2 id="the-future-of-work-in-an-ai-dominated-world">The Future of Work in an AI Dominated World
&lt;/h2>&lt;p>There is always a concern that AI systems will replace human workers in a variety of industries, leading to job losses and economic disruption. Fears of job losses due to technology changes are not new. In the early 19th century, textile workers in England formed the Luddite movement to protest against the introduction of new textile technologies that threatened their jobs. Their paranoia turned out to be misguided as new technologies led to industry growth, creating new jobs and increasing productivity.&lt;/p>
&lt;p>The increasing ability of AI to outperform humans in certain tasks has raised concerns about AI replacing human workforce. However, it is more likely that AI will operate in a more symbiotic relationship with the human workforce by enhancing and empowering human expertise rather than replacing them. For example, chess engines have long possessed the capabilities to defeat human players but now they are used to assist Grandmasters in their preparations. However, it is also important to acknowledge that while new technologies can bring in new avenues of employment there will be displacements in existing employment patterns. We as individuals and society, need to be prepared for these changes and adapt accordingly.&lt;/p>
&lt;h2 id="the-intersection-of-ai-and-art">The Intersection Of AI and Art
&lt;/h2>&lt;p>With the advent of DALL-E and Stable Diffusion, AI-generated art has now been thrust into the mainstream, and as such, concerns have arisen over the potential for plagiarism and disrespect for intellectual property rights. This is like the ancient paradox of &amp;ldquo;Ship of Theseus&amp;rdquo; - when does art transcend beyond its origins to become something entirely new? How do you distinguish and attribute ownership to your own contributions.&lt;/p>
&lt;figure>
&lt;img src="https://example.com/images/2023-03-15-surrealist-art-stable-diffusion.png"
witdth="50%"
alt="Surrealist art generated using Stable Diffusion">
&lt;figcaption>&lt;i>Surrealist art generated using Stable Diffusion (MidJourney v4)&lt;/i>&lt;/figcaption>
&lt;/figure>
&lt;p>Irrespective of how we address the plagiarism problem, I don&amp;rsquo;t see AI-generated art diminishing the need or impact of artists. Art in its purest form serves as a counterpoint to the cultural norms of contemporary society. If everyone is producing StableDiffusion art, certain predictable patterns begin to emerge that can feel soulless. The same can be said of ChatGPT or any AI-derived work. Folks have already &lt;a class="link" href="https://twitter.com/bildoperationen/status/1633082030178050048" target="_blank" rel="noopener"
>noted&lt;/a> that in StableDiffusion &amp;ldquo;the default mode of these images is to shine and sparkle, as if illuminated from within&amp;rdquo;. True art will find voice to shine through these mimics. On the other hand, mainstream artists who adopt derivative, orthodox forms of expression may become obsolete and be replaced by a new generation of artists who integrate technology into their art.&lt;/p>
&lt;blockquote>
&lt;p>Art in its purest form serves as a counterpoint to the cultural norms of contemporary society.&lt;/p>
&lt;/blockquote>
&lt;h2 id="balancing-progress-with-needs-of-society-and-safety">Balancing Progress with Needs of Society and Safety
&lt;/h2>&lt;p>In order to tackle the potential hazards of AI, we have to channel our innate ability to adapt. As much as we&amp;rsquo;d like to think we&amp;rsquo;re in charge of the progress of AI, we can&amp;rsquo;t possibly anticipate all the potential consequences and risks that come with AI and expect to control it with a set of rules and regulations. The genie is out of the bottle, and companies large and small will embrace AI in one way or another to keep up with the growing competition. Companies that embrace AI will have a distinct advantage over those that are slow to adopt it. Therefore, it is imperative that we approach the development and integration of AI with humility, acknowledging the complexity and unpredictability of the technology, and actively seeking to examine, understand and manage its potential impact on society and individual lives. Companies should also build AI systems to express doubts and train users regarding the error margins&lt;/p></description></item><item><title>GPT-3 and prospects of Artificial General Intelligence</title><link>https://example.com/p/2020-09-20-gpt-3-and-prospects-of-artificial-general-intelligence/</link><pubDate>Sun, 20 Sep 2020 12:00:00 +0000</pubDate><guid>https://example.com/p/2020-09-20-gpt-3-and-prospects-of-artificial-general-intelligence/</guid><description>&lt;img src="https://example.com/images/2020-09-26-meta-learning.png" alt="Featured image of post GPT-3 and prospects of Artificial General Intelligence" />&lt;p>Last year OpenAI released the Generative Pre-trained Transformer 2 (GPT-2) model. GPT-2 was a language model with 1.5 billion parameters, trained on 8 million web pages. It generated quite a buzz as it could generate coherent text, comprehend paragraphs, answer questions, and summarize text and do all sorts of smart stuff&amp;hellip; all without any task-specific learning. OpenAI even deemed the model too dangerous to release but eventually ended up releasing them.&lt;/p>
&lt;p>In May 2020, OpenAI released their follow-up GPT-3 model which took the game several notches higher. They trained it with 175 billion parameters, using close to half-a-trillion tokens. The model and its weights alone would take up 300GB VRAM. This is a drastic increase in scale and complexity, anyway you look at it. So what can a huge model like this achieve and why has it reinvigorated the talks ?&lt;/p>
&lt;p>&lt;img src="https://example.com/images/2020-09-26-gpt-3-training-size.png"
loading="lazy"
alt="GPT-3 Training Size"
>&lt;/p>
&lt;p>GPT-3 can write poetry, mimic &lt;a class="link" href="https://www.gwern.net/GPT-3#literary-parodies" target="_blank" rel="noopener"
>the writing style of personalities&lt;/a>. It also performs better than an average college applicant in &lt;a class="link" href="https://arxiv.org/pdf/2005.14165.pdf#page=25" target="_blank" rel="noopener"
>SAT analogy problems&lt;/a>, generates &lt;a class="link" href="https://medium.com/@aidungeon/ai-dungeon-dragon-model-upgrade-7e8ea579abfe" target="_blank" rel="noopener"
>cohesive stories&lt;/a>, writes news articles that are &lt;a class="link" href="https://arxiv.org/pdf/2005.14165.pdf#page=27" target="_blank" rel="noopener"
>hard to distinguish&lt;/a> from a human-written article. You can even ask it to do system admin jobs in natural language and it will come up with shell commands and execute them- like a seasoned sysadmin.&lt;/p>
&lt;p>&lt;img src="https://example.com/images/2020-09-26-nlsh.png"
loading="lazy"
alt="Natural Language Shell"
>&lt;/p>
&lt;h2 id="learning-how-to-learn">Learning how to learn
&lt;/h2>&lt;p>One of the amazing aspects of the GPT-3 model is that it doesn&amp;rsquo;t need any task-specific fine-tuning and yet achieves decent results on many of the Natural Language Processing (NLP) benchmarks and tasks. OpenAI&amp;rsquo;s claim is that if an NLP model is sufficiently complex and if it has been trained on a large volume of data, it can learn to do new tasks only by looking at few examples prompts i.e, the model has &lt;a class="link" href="https://www.gwern.net/newsletter/2020/05#meta-learning" target="_blank" rel="noopener"
>learned the ability to learn&lt;/a> new tasks on the go. They call it &amp;ldquo;Few Shot Learning&amp;rdquo;. You can see that in action in the chart below.&lt;/p>
&lt;p>&lt;img src="https://example.com/images/2020-09-26-meta-learning.png"
loading="lazy"
alt="Meta-learning"
>&lt;/p>
&lt;p>In the above chart, you can see when the GPT-3 175B model is given with few examples, the accuracy rate shoots up compared to a poor learner (1.3B or even the 13B parameter model). The ability to learn is one of the defining characteristics of Artificial General Intelligence (AGI) and now you can understand why GPT-3 is generating the buzz.&lt;/p>
&lt;h2 id="is-language-enough-to-model-reality">Is Language enough to model reality?
&lt;/h2>&lt;p>GPT-3 has no direct exposure to reality, except via a large corpus of text. Yet the knowledge imparted from this large volume of text allows GPT-3 to reason (arguably) about the physical world as seen in the &lt;a class="link" href="https://www.lesswrong.com/posts/L5JSMZQvkBAx9MD5A/to-what-extent-is-gpt-3-capable-of-reasoning" target="_blank" rel="noopener"
>example&lt;/a> below&lt;/p>
&lt;p>&lt;img src="https://example.com/images/2020-09-26-reasoning.png"
loading="lazy"
alt="Reasoning about the physical world"
>&lt;/p>
&lt;p>In fact, GPT-3 also demonstrates surprising ability outside the traditional NLP domain. It can do certain non-linguistic tasks like simple arithmetics. This begs the question, whether modeling language alone is sufficient to form an understanding of reality? We use language to describe our world. So a relation between words should correspond to a relation between the real world entities represented by those words. &lt;a class="link" href="https://deponysum.com/2020/01/16/recent-advances-in-natural-language-processing-some-woolly-speculations/" target="_blank" rel="noopener"
>Some&lt;/a> reason that ML models are transitive (i.e, if X models Y and Y models Z, then X models Z) and since language itself is an approximate model of the world, a language model should be an approximate representation of the world.&lt;/p>
&lt;h2 id="the-upper-bound-of-the-effectiveness-of-scale">The upper-bound of the effectiveness of scale
&lt;/h2>&lt;p>Another surprising outcome of GPT-3 is that the performance has not tapered as the model got more complex. When GPT-2 got released most people speculated that we will soon hit the upper bound on the returns from further scaling up the model. But that hasn&amp;rsquo;t happened with GPT-3. With a more complex model, GPT-3 has shown big improvements in its performance and seems to get better at learning new tasks (see the widening gap between zero-shot and few-shot benchmarks).&lt;/p>
&lt;p>As recently as last year, training a model with 175B seemed far off into the future. The Human brain is estimated to contain around 100 trillion neurons, how long before we reach that figure and what would AI look like as it approaches the figure?&lt;/p>
&lt;p>&lt;img src="https://example.com/images/2020-09-26-scaling-hypothesis.png"
loading="lazy"
alt="Scaling hypothesis"
>&lt;/p>
&lt;blockquote>
&lt;p>Now I agree that I am playing to the gallery here and being sensational. Biological neurons and ML neurons are quite different. A CNN model can do object detection (in fact better than us) but it doesn&amp;rsquo;t share the same structure with our visual cortex. This is why CNN outperforms humans in certain tasks like detecting anomalies in medical images, yet they can get &lt;a class="link" href="https://www.iflscience.com/technology/ai-camera-ruins-soccar-game-for-fans-after-mistaking-referees-bald-head-for-ball/" target="_blank" rel="noopener"
>confused between a ball and a referee&amp;rsquo;s head&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;p>Not everyone buys the scaling hypothesis. Detractors like Marcus claim that just because we know how to stack ladders doesn&amp;rsquo;t mean that we can build ladders to the moon. The argument is loaded with an assumption. In the case of ladders to the moon, we know the governing rules and the limits of possibility. But to extend the metaphor to AGI, we don&amp;rsquo;t understand our own intelligence well enough and we cannot say for sure that the chasm between a statistical model and causal reasoning is impervious. Like the expression &lt;a class="link" href="https://www.wikiwand.com/en/Turtles_all_the_way_down" target="_blank" rel="noopener"
>Turtles all the way down&lt;/a>, perhaps our intelligence is also made of statistical models stacked all the way down.&lt;/p>
&lt;h2 id="what-do-the-critics-say">What do the critics say?
&lt;/h2>&lt;h3 id="it-is-not-as-good-as-sota">It is not as good as SOTA
&lt;/h3>&lt;p>To be correct, GPT-3 is not the best model out there. On most NLP tasks &amp;amp; benchmarks, other State-of-the-art (SOTA) algorithms perform better than GPT-3. But those criticisms are missing the point.&lt;/p>
&lt;ol>
&lt;li>Most of the SOTA algorithms are fine-tuned for specific tasks whereas GPT-3 is not. But GPT-3 has still learned to do these tasks by understanding (in a specific sense) the English language, somewhat similar to how humans learn language tasks. And this, not the benchmark performance, is the most talked-about aspect of GPT-3.&lt;/li>
&lt;li>SOTA also performs better than humans on many specific tasks like object recognition, mastering games like Chess and Go. But that doesn&amp;rsquo;t weigh against the ability of our general intelligence. General intelligence, in a sense, always has to underfit to have wider applicability.&lt;/li>
&lt;/ol>
&lt;p>One should also note that, in spite of being a general-purpose model, GPT-3 betters fine-tuned SOTA in some benchmarks (PhysicalQA, LAMBADA, Penn Tree Bank to name a few)&lt;/p>
&lt;h3 id="it-is-just-an-autocomplete">It is just an autocomplete
&lt;/h3>&lt;p>Another common criticism is that GPT-3 is just a sophisticated &lt;a class="link" href="https://www.forbes.com/sites/robtoews/2020/07/19/gpt-3-is-amazingand-overhyped/?sh=5907df881b1c" target="_blank" rel="noopener"
>text prediction engine&lt;/a>. It doesn&amp;rsquo;t understand what those words mean. But this is a hasty criticism based on an &lt;a class="link" href="https://plato.stanford.edu/entries/chinese-room/#IntuRepl" target="_blank" rel="noopener"
>intuitive definition&lt;/a> understanding. Our intuitions about intelligence, understanding, and meaning are not precise enough for such claims.&lt;/p>
&lt;p>Let us say our brain is a sophisticated lookup dictionary populated by past experiences and culturally accumulated knowledge. Suppose a non-human entity possesses a similar dictionary but populated using a different process (by training on text corpus for example). Can it claim to have an understanding? This was the crux of the argument laid down by Searle in his &lt;a class="link" href="https://plato.stanford.edu/entries/chinese-room/#Over" target="_blank" rel="noopener"
>Chinese room thought experiment&lt;/a>.&lt;/p>
&lt;p>Searle claims that looking up a dictionary doesn&amp;rsquo;t represent understanding, but not everyone agrees with that take. On a similar note, if we dismiss GPT-3&amp;rsquo;s output as a mere statistical calculation, there is no telling that our brain&amp;rsquo;s neuro-biological process is any different.&lt;/p>
&lt;h3 id="it-has-already-seen-the-data-it-is-predicting-on">It has already seen the data it is predicting on
&lt;/h3>&lt;p>This is a more serious allegation than the ones above. Since GPT-3 was trained on a huge volume of data, there is a chance that it has already seen the inputs and it is simply recalling them from memory. For example, Yannic Kilcher in his &lt;a class="link" href="https://youtu.be/SY5PvZrJhLE" target="_blank" rel="noopener"
>video&lt;/a> suspects that it can do arithmetic predictions because the model is likely to have seen the same data in the training dataset. OpenAI team claims to have done sufficient deduplication to remove the testing dataset from the training data. But given the volume of data, their deduplication is only &lt;a class="link" href="https://arxiv.org/pdf/2005.14165.pdf#page=25" target="_blank" rel="noopener"
>optimistic&lt;/a>.&lt;/p>
&lt;p>But there is evidence that transformer models can indeed generate answers that it has not seen before. Karpathy trained a &lt;a class="link" href="https://github.com/karpathy/minGPT" target="_blank" rel="noopener"
>mini GPT model&lt;/a> exclusively on synthetic data and it was already able to do 2 digit arithmetic. In this case, there is a clear separation between the training set and the validation set. So to conclude, the results we see from GPT-3 is not impossible and need not necessarily come out of a training data set corruption.&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>All these comparisons with AGI don&amp;rsquo;t mean we should haphazardly claim that GPT-3 is an AGI or that it definitively proves that AGI is possible. However, there are few interesting observations from the results of GPT-3:&lt;/p>
&lt;ol>
&lt;li>It shows that NLP models can be a shortcut to AGI as they have the ability to indirectly model the physical world.&lt;/li>
&lt;li>The performance of the GPT model continues to scale with the model size and hasn&amp;rsquo;t tapered off yet. So there is the promise that bigger models can be more intelligent.&lt;/li>
&lt;li>At higher model complexities, interesting behaviors seem to emerge, such as the ability to learn new tasks, perform non-linguistic tasks, etc.,&lt;/li>
&lt;/ol>
&lt;p>The year 2018 has been called the &lt;a class="link" href="https://thegradient.pub/nlp-imagenet/" target="_blank" rel="noopener"
>ImageNet moment&lt;/a> for NLP and we can see why. The amount of progress made in NLP during the last couple of years is staggering and there is continued interest and investments in the domain. And mastering the NLP domain is closely linked with mastering General Intelligence.&lt;/p>
&lt;p>There are many who believe that intelligence is somewhat innate to biological processes and cannot be reduced to mathematical models. There are others who believe that it is possible. In a way, this is just another incarnation of the nature vs nurture debate raging in the philosophical world for centuries. Except for this time, we are not relying on thought experiments. We get to do real experiments and see the answers in reality. Whichever side of the fence one sits on, the world waits with bated breath as the story unfolds.&lt;/p></description></item></channel></rss>